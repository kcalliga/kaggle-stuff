{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a53fde11",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction & Clustering Playground\n",
    "\n",
    "This is a **hands-on learning notebook** for exploring high-dimensional data using:\n",
    "\n",
    "- **PCA** (Principal Component Analysis)\n",
    "- **t-SNE** (t-distributed Stochastic Neighbor Embedding)\n",
    "- **UMAP** (Uniform Manifold Approximation and Projection)\n",
    "- **Clustering** (KMeans, DBSCAN)\n",
    "\n",
    "Goals:\n",
    "- Build intuition for what PCA, t-SNE, and UMAP are *doing*.\n",
    "- Understand when to use each method.\n",
    "- See how clustering interacts with these embeddings.\n",
    "- Give you a reusable playground for your own datasets (baseball, weather, Kaggle, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3a2b68",
   "metadata": {},
   "source": [
    "## 0. High-Level Workflow\n",
    "\n",
    "1. **Load data**\n",
    "   - Either from a CSV (your dataset) or from a built-in demo dataset.\n",
    "2. **Basic EDA**\n",
    "   - Shape, distributions, correlations.\n",
    "3. **Preprocessing**\n",
    "   - Select numeric features.\n",
    "   - Standardize features.\n",
    "4. **PCA**\n",
    "   - Explained variance (scree plot).\n",
    "   - 2D PCA scatter.\n",
    "5. **t-SNE**\n",
    "   - Nonlinear 2D embedding for visualization.\n",
    "   - Parameters: `perplexity`, `learning_rate`.\n",
    "6. **UMAP** (if available)\n",
    "   - Nonlinear 2D embedding.\n",
    "   - Parameters: `n_neighbors`, `min_dist`.\n",
    "7. **Clustering**\n",
    "   - KMeans on PCA and/or UMAP.\n",
    "   - Elbow & silhouette heuristics.\n",
    "   - DBSCAN for density-based clusters.\n",
    "8. **Use in ML**\n",
    "   - How to fold embeddings & cluster labels back into your ML pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bde969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1. Imports & Config ==========\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.datasets import load_iris, make_blobs, make_circles\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "sns.set(style='whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# UMAP is optional; we check availability.\n",
    "try:\n",
    "    import umap\n",
    "    UMAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    UMAP_AVAILABLE = False\n",
    "    print('UMAP is not installed. Install with `pip install umap-learn` to enable UMAP sections.')\n",
    "\n",
    "# Random state for reproducibility\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b90d081",
   "metadata": {},
   "source": [
    "## 2. Choose a Data Source\n",
    "\n",
    "Two options:\n",
    "\n",
    "1. **Your dataset (CSV)**\n",
    "   - Set `DATA_MODE = 'csv'` and provide path + feature columns.\n",
    "2. **Built-in demo dataset** (easier for quick experiments):\n",
    "   - `iris` – classic 4D flower dataset (3 classes).\n",
    "   - `blobs` – synthetic Gaussian clusters.\n",
    "   - `circles` – concentric circles (nonlinear structure).\n",
    "\n",
    "Advice:\n",
    "- Start with a **demo** dataset to get a feel.\n",
    "- Then plug in your **baseball stats, weather features, or Kaggle data** using CSV mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8ebbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 2.1 Config: pick data mode ==========\n",
    "\n",
    "DATA_MODE = 'iris'  # options: 'iris', 'blobs', 'circles', 'csv'\n",
    "\n",
    "# If using CSV mode, set these:\n",
    "DATA_DIR = Path('../input')\n",
    "CSV_FILE = 'your_data.csv'  # change this\n",
    "\n",
    "# If your CSV has a label column (e.g., position, cluster, etc.), set it here.\n",
    "CSV_LABEL_COL: Optional[str] = None  # e.g. 'position', 'class', or None\n",
    "\n",
    "# If None, we'll use all numeric columns. Otherwise, specify a list.\n",
    "CSV_FEATURE_COLS: Optional[list] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d091cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 2.2 Helper: load data depending on DATA_MODE ==========\n",
    "\n",
    "def load_demo_data(mode: str) -> Tuple[pd.DataFrame, Optional[pd.Series]]:\n",
    "    \"\"\"Return (X_df, y_series or None) for demo datasets.\"\"\"\n",
    "    if mode == 'iris':\n",
    "        iris = load_iris()\n",
    "        X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "        y = pd.Series(iris.target, name='class')\n",
    "        return X, y\n",
    "    elif mode == 'blobs':\n",
    "        X_array, y_array = make_blobs(n_samples=800, centers=4, n_features=5,\n",
    "                                     random_state=RANDOM_STATE, cluster_std=1.2)\n",
    "        X = pd.DataFrame(X_array, columns=[f'feat_{i}' for i in range(X_array.shape[1])])\n",
    "        y = pd.Series(y_array, name='cluster')\n",
    "        return X, y\n",
    "    elif mode == 'circles':\n",
    "        X_array, y_array = make_circles(n_samples=800, factor=0.5, noise=0.05,\n",
    "                                       random_state=RANDOM_STATE)\n",
    "        X = pd.DataFrame(X_array, columns=['x1', 'x2'])\n",
    "        y = pd.Series(y_array, name='circle')\n",
    "        return X, y\n",
    "    else:\n",
    "        raise ValueError(f'Unknown demo mode: {mode}')\n",
    "\n",
    "\n",
    "def load_csv_data(\n",
    "    data_dir: Path = DATA_DIR,\n",
    "    csv_file: str = CSV_FILE,\n",
    "    label_col: Optional[str] = CSV_LABEL_COL,\n",
    "    feature_cols: Optional[list] = CSV_FEATURE_COLS,\n",
    ") -> Tuple[pd.DataFrame, Optional[pd.Series]]:\n",
    "    path = data_dir / csv_file\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f'CSV file not found: {path}')\n",
    "    df = pd.read_csv(path)\n",
    "    print('Loaded CSV shape:', df.shape)\n",
    "    display(df.head())\n",
    "\n",
    "    y = None\n",
    "    if label_col is not None and label_col in df.columns:\n",
    "        y = df[label_col]\n",
    "    \n",
    "    if feature_cols is None:\n",
    "        X = df.select_dtypes(include=[np.number])\n",
    "        if label_col is not None and label_col in X.columns:\n",
    "            X = X.drop(columns=[label_col])\n",
    "    else:\n",
    "        missing = [c for c in feature_cols if c not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f'Feature columns not in CSV: {missing}')\n",
    "        X = df[feature_cols]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# ---- Load data depending on mode ----\n",
    "if DATA_MODE in ['iris', 'blobs', 'circles']:\n",
    "    X, y = load_demo_data(DATA_MODE)\n",
    "    print(f'Using demo dataset: {DATA_MODE}')\n",
    "else:\n",
    "    X, y = load_csv_data()\n",
    "    print('Using CSV dataset.')\n",
    "\n",
    "print('\\nFeature matrix shape:', X.shape)\n",
    "if y is not None:\n",
    "    print('Labels shape:', y.shape, '| n classes (approx):', y.nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09680dc8",
   "metadata": {},
   "source": [
    "## 3. Quick EDA on Features\n",
    "\n",
    "We'll:\n",
    "- Inspect basic statistics.\n",
    "- Look at distributions of a few features.\n",
    "- (Optionally) check pairwise correlations.\n",
    "\n",
    "This step is about **building intuition** about the space we're going to compress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517a6914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 3.1 Basic stats ==========\n",
    "\n",
    "display(X.head())\n",
    "display(X.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5488b785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 3.2 Histograms for a subset of features ==========\n",
    "\n",
    "num_cols = list(X.columns)\n",
    "max_cols = min(6, len(num_cols))\n",
    "\n",
    "fig, axes = plt.subplots(2, max_cols // 2 if max_cols > 2 else max_cols, figsize=(16, 6))\n",
    "axes = np.array(axes).reshape(-1)\n",
    "for ax, col in zip(axes, num_cols[:max_cols]):\n",
    "    ax.hist(X[col], bins=30)\n",
    "    ax.set_title(col)\n",
    "plt.suptitle('Feature histograms (subset)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1e06fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 3.3 Correlation heatmap (if not too many features) ==========\n",
    "\n",
    "if X.shape[1] <= 20:\n",
    "    corr = X.corr()\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(corr, cmap='coolwarm', center=0)\n",
    "    plt.title('Feature correlation heatmap')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('More than 20 features; skipping full correlation heatmap for readability.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777a4bd2",
   "metadata": {},
   "source": [
    "## 4. Standardize Features\n",
    "\n",
    "Most dimensionality reduction and clustering methods assume features are on a comparable scale.\n",
    "- **PCA** is especially sensitive to scale.\n",
    "- **KMeans** uses Euclidean distance.\n",
    "\n",
    "We'll standardize each feature to have roughly:\n",
    "- Mean 0\n",
    "- Standard deviation 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e32660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 4. Standardize features ==========\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print('Scaled feature matrix shape:', X_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de06962",
   "metadata": {},
   "source": [
    "## 5. PCA – Linear Dimensionality Reduction\n",
    "\n",
    "PCA finds **orthogonal directions** (principal components) that explain the maximum variance.\n",
    "\n",
    "Use PCA when:\n",
    "- You want a **fast, linear** dimensionality reduction.\n",
    "- You care about variance explained and interpretability.\n",
    "- You want a **baseline embedding** before t-SNE/UMAP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34064f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 5.1 Fit PCA and inspect explained variance ==========\n",
    "\n",
    "pca = PCA(n_components=min(10, X_scaled.shape[1]))\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "explained = pca.explained_variance_ratio_\n",
    "print('Explained variance ratio by component:')\n",
    "for i, v in enumerate(explained):\n",
    "    print(f'PC{i+1}: {v:.3f}')\n",
    "\n",
    "plt.plot(np.arange(1, len(explained) + 1), np.cumsum(explained), marker='o')\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')\n",
    "plt.title('PCA cumulative explained variance')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a82813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 5.2 2D PCA scatter ==========\n",
    "\n",
    "pc1, pc2 = X_pca[:, 0], X_pca[:, 1]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "if y is not None:\n",
    "    scatter = plt.scatter(pc1, pc2, c=y, cmap='tab10', alpha=0.7)\n",
    "    plt.legend(*scatter.legend_elements(), title='Classes', loc='best')\n",
    "else:\n",
    "    plt.scatter(pc1, pc2, alpha=0.7)\n",
    "\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA (2D)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c1edfc",
   "metadata": {},
   "source": [
    "**Interpretation tips:**\n",
    "\n",
    "- Separated blobs in PC1/PC2 space suggest linear structure you can exploit.\n",
    "- Strong overlap may hint at more complex / nonlinear structure.\n",
    "- For interpretability, you can inspect PCA loadings (how each feature contributes to each component).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d3c4e1",
   "metadata": {},
   "source": [
    "## 6. t-SNE – Nonlinear Local Structure Visualization\n",
    "\n",
    "t-SNE is best used **only for visualization**, not as a feature generator for downstream models.\n",
    "\n",
    "Key properties:\n",
    "- Preserves **local neighborhood** structure.\n",
    "- Can reveal clusters that are not linearly separable.\n",
    "- Sensitive to **hyperparameters** and randomness.\n",
    "\n",
    "Main hyperparameters:\n",
    "- `perplexity` (how many neighbors each point sees; often 5–50).\n",
    "- `learning_rate` (often 50–1000; start with 'auto').\n",
    "- `n_iter` (start around 1000–2000).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3742c2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 6.1 t-SNE 2D embedding ==========\n",
    "\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=30,\n",
    "    learning_rate='auto',\n",
    "    init='pca',\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_iter=1500,\n",
    ")\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "if y is not None:\n",
    "    scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', alpha=0.7)\n",
    "    plt.legend(*scatter.legend_elements(), title='Classes', loc='best')\n",
    "else:\n",
    "    plt.scatter(X_tsne[:, 0], X_tsne[:, 1], alpha=0.7)\n",
    "\n",
    "plt.title('t-SNE (2D)')\n",
    "plt.xlabel('Dim 1')\n",
    "plt.ylabel('Dim 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10da2887",
   "metadata": {},
   "source": [
    "**t-SNE notes:**\n",
    "\n",
    "- Different runs with different seeds/perplexity can look quite different.\n",
    "- Distances between **well-separated clusters** are not very meaningful.\n",
    "- Great for:\n",
    "  - Visualizing embeddings from neural nets (NLP, CV).\n",
    "  - Inspecting cluster structure in high-dimensional tabular data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd54f367",
   "metadata": {},
   "source": [
    "## 7. UMAP – Nonlinear Local + Global Structure\n",
    "\n",
    "UMAP is similar in spirit to t-SNE but often:\n",
    "- Faster and more scalable.\n",
    "- Preserves more of the **global** structure.\n",
    "- Works well as a **preprocessing step** for clustering or downstream models.\n",
    "\n",
    "Key hyperparameters:\n",
    "- `n_neighbors`: balance local vs global (5–50; lower → more local, higher → more global).\n",
    "- `min_dist`: how tightly points are packed (smaller → tighter clusters).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41b4edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 7.1 UMAP 2D embedding (if available) ==========\n",
    "\n",
    "if UMAP_AVAILABLE:\n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=15,\n",
    "        min_dist=0.1,\n",
    "        n_components=2,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "    X_umap = reducer.fit_transform(X_scaled)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    if y is not None:\n",
    "        scatter = plt.scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap='tab10', alpha=0.7)\n",
    "        plt.legend(*scatter.legend_elements(), title='Classes', loc='best')\n",
    "    else:\n",
    "        plt.scatter(X_umap[:, 0], X_umap[:, 1], alpha=0.7)\n",
    "\n",
    "    plt.title('UMAP (2D)')\n",
    "    plt.xlabel('UMAP1')\n",
    "    plt.ylabel('UMAP2')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('UMAP not available; skipping UMAP embedding.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486c426c",
   "metadata": {},
   "source": [
    "**PCA vs t-SNE vs UMAP (mental model):**\n",
    "\n",
    "- PCA: linear, fast, interpretable → **baseline** / feature engineering.\n",
    "- t-SNE: nonlinear, local neighborhood focus → **pretty 2D cluster visualizations**, not for features.\n",
    "- UMAP: nonlinear, can be used as **features or visualization**, better global+local balance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f072f6",
   "metadata": {},
   "source": [
    "## 8. Clustering on Embeddings\n",
    "\n",
    "We’ll explore:\n",
    "\n",
    "- **KMeans**: partitions data into k clusters using distances.\n",
    "  - Works best with spherical-ish clusters.\n",
    "- **DBSCAN**: density-based, can find arbitrarily shaped clusters and label outliers.\n",
    "\n",
    "Common patterns:\n",
    "- Run KMeans on **PCA** or **UMAP** embeddings instead of raw features.\n",
    "- Use **elbow plot** and **silhouette score** to pick k.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c7853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 8.1 Helper: run KMeans for multiple k on PCA ==========\n",
    "\n",
    "k_values = [2, 3, 4, 5, 6]\n",
    "inertias = []\n",
    "silhouettes = []\n",
    "\n",
    "for k in k_values:\n",
    "    km = KMeans(n_clusters=k, random_state=RANDOM_STATE)\n",
    "    labels_k = km.fit_predict(X_pca[:, :2])  # use first 2 PCs for simplicity\n",
    "    inertias.append(km.inertia_)\n",
    "    # silhouette needs at least 2 clusters and more samples than clusters\n",
    "    if len(np.unique(labels_k)) > 1 and X_pca.shape[0] > len(np.unique(labels_k)):\n",
    "        sil = silhouette_score(X_pca[:, :2], labels_k)\n",
    "    else:\n",
    "        sil = np.nan\n",
    "    silhouettes.append(sil)\n",
    "\n",
    "print('k | inertia | silhouette')\n",
    "for k, inn, sil in zip(k_values, inertias, silhouettes):\n",
    "    print(f'{k:2d} | {inn:8.1f} | {sil:9.3f}')\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(k_values, inertias, marker='o')\n",
    "ax1.set_xlabel('k')\n",
    "ax1.set_ylabel('Inertia', color='b')\n",
    "ax1.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(k_values, silhouettes, marker='s')\n",
    "ax2.set_ylabel('Silhouette', color='g')\n",
    "ax2.tick_params(axis='y', labelcolor='g')\n",
    "\n",
    "plt.title('KMeans on PCA(2D): inertia & silhouette vs k')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd36841",
   "metadata": {},
   "source": [
    "**Choosing k:**\n",
    "\n",
    "- Look for an **elbow** in inertia (where improvement slows).\n",
    "- Prefer k where **silhouette** is relatively high.\n",
    "- Also consider domain meaning (e.g., player archetypes, weather regimes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0760f326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 8.2 Fit KMeans with chosen k and visualize clusters ==========\n",
    "\n",
    "BEST_K = 3  # adjust after inspecting previous outputs\n",
    "\n",
    "kmeans_final = KMeans(n_clusters=BEST_K, random_state=RANDOM_STATE)\n",
    "cluster_labels_pca = kmeans_final.fit_predict(X_pca[:, :2])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels_pca, cmap='tab10', alpha=0.7)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title(f'KMeans (k={BEST_K}) clusters in PCA space')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# If t-SNE/UMAP are available, color those by the same clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=cluster_labels_pca, cmap='tab10', alpha=0.7)\n",
    "plt.title('Same KMeans clusters visualized in t-SNE space')\n",
    "plt.xlabel('t-SNE1')\n",
    "plt.ylabel('t-SNE2')\n",
    "plt.show()\n",
    "\n",
    "if UMAP_AVAILABLE:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X_umap[:, 0], X_umap[:, 1], c=cluster_labels_pca, cmap='tab10', alpha=0.7)\n",
    "    plt.title('Same KMeans clusters visualized in UMAP space')\n",
    "    plt.xlabel('UMAP1')\n",
    "    plt.ylabel('UMAP2')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8f16e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 8.3 DBSCAN for density-based clustering ==========\n",
    "\n",
    "# DBSCAN finds clusters of high density and marks sparse points as noise (-1)\n",
    "\n",
    "dbscan = DBSCAN(eps=0.8, min_samples=10)\n",
    "db_labels = dbscan.fit_predict(X_pca[:, :2])  # using PCA(2D) for simplicity\n",
    "\n",
    "unique_labels = np.unique(db_labels)\n",
    "print('DBSCAN unique labels:', unique_labels)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=db_labels, cmap='tab20', alpha=0.7)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('DBSCAN clusters in PCA space (label -1 = noise)')\n",
    "plt.colorbar(scatter, label='DBSCAN label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12bb412",
   "metadata": {},
   "source": [
    "**KMeans vs DBSCAN:**\n",
    "\n",
    "- **KMeans**:\n",
    "  - You choose k.\n",
    "  - Good for roughly spherical clusters of similar size.\n",
    "  - Fast and widely used.\n",
    "\n",
    "- **DBSCAN**:\n",
    "  - Finds \"natural\" clusters based on density.\n",
    "  - Can discover arbitrarily shaped clusters.\n",
    "  - Marks outliers explicitly.\n",
    "  - Sensitive to `eps` and `min_samples`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffa10b5",
   "metadata": {},
   "source": [
    "## 9. Using Embeddings & Clusters in ML Pipelines\n",
    "\n",
    "Once you have PCA/UMAP/t-SNE (mostly PCA/UMAP) and cluster labels, you can:\n",
    "\n",
    "- **Add them as features** to your supervised ML models.\n",
    "  - PCA components: `pc1, pc2, pc3, ...`.\n",
    "  - UMAP components: `umap1, umap2, ...`.\n",
    "  - Cluster labels: one-hot encode `cluster_id`.\n",
    "- Use clusters to:\n",
    "  - Create **player archetypes** (e.g., power hitter vs speedster).\n",
    "  - Segment customers or weather regimes.\n",
    "  - Perform separate models per cluster if behavior is very different.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedaf180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 9.1 Build a combined DataFrame with embeddings & cluster labels ==========\n",
    "\n",
    "df_features = X.copy()\n",
    "\n",
    "df_features['pc1'] = X_pca[:, 0]\n",
    "df_features['pc2'] = X_pca[:, 1]\n",
    "df_features['kmeans_cluster'] = cluster_labels_pca\n",
    "\n",
    "df_features['tsne1'] = X_tsne[:, 0]\n",
    "df_features['tsne2'] = X_tsne[:, 1]\n",
    "\n",
    "if UMAP_AVAILABLE:\n",
    "    df_features['umap1'] = X_umap[:, 0]\n",
    "    df_features['umap2'] = X_umap[:, 1]\n",
    "\n",
    "if y is not None:\n",
    "    df_features['label'] = y.values\n",
    "\n",
    "display(df_features.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd203ac5",
   "metadata": {},
   "source": [
    "At this point you can:\n",
    "\n",
    "- Export `df_features` to CSV and feed it to your **regression/classification templates**.\n",
    "- Use PCA/UMAP components as **compressed representations** of your original features.\n",
    "- Treat `kmeans_cluster` as a categorical feature in downstream models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7e96bb",
   "metadata": {},
   "source": [
    "## 10. Practical Decision Rules (Quick Reference)\n",
    "\n",
    "### 10.1 Which dimensionality reduction method?\n",
    "\n",
    "- **Start with PCA** when:\n",
    "  - You want a quick, linear baseline.\n",
    "  - You care about explained variance and feature contributions.\n",
    "  - You want input to KMeans or other models.\n",
    "\n",
    "- **Use t-SNE** when:\n",
    "  - You specifically want a **2D plot** for exploration.\n",
    "  - You don’t care about using the embedding as input features.\n",
    "  - You’re exploring complex embeddings (e.g., from deep models).\n",
    "\n",
    "- **Use UMAP** when:\n",
    "  - You want a **nonlinear embedding** that you *can* use as features.\n",
    "  - You care about both local and some global structure.\n",
    "  - You need scalability to larger datasets.\n",
    "\n",
    "### 10.2 Which clustering method?\n",
    "\n",
    "- **KMeans**:\n",
    "  - You have a rough idea of the number of clusters.\n",
    "  - Clusters are somewhat spherical/convex in embedding space.\n",
    "  - You want something fast and simple.\n",
    "\n",
    "- **DBSCAN**:\n",
    "  - You expect irregular cluster shapes.\n",
    "  - You want outlier detection built in.\n",
    "  - You don’t know how many clusters to expect.\n",
    "\n",
    "Always cross-check clusters with **domain knowledge**:\n",
    "- Do the discovered clusters make sense for players, customers, or weather regimes?\n",
    "- Are they stable across random seeds and reasonable parameter changes?\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
