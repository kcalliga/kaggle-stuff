{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1e895ef",
   "metadata": {},
   "source": [
    "# Tabular Regression Workflow Template (EDA ‚Üí Skew ‚Üí Outliers ‚Üí Model Choice)\n",
    "\n",
    "This notebook is a **reusable template** for tabular regression problems (e.g. Kaggle competitions).  \n",
    "It includes both **code** and a **granular decision workflow** so you can follow the same thought process every time.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ High-Level Workflow\n",
    "\n",
    "1. **Set config & load data**\n",
    "2. **Understand structure**: dtypes, missingness, basic stats\n",
    "3. **Explore numeric features**: distributions, feature‚Äìtarget relationships, correlations\n",
    "4. **Explore categorical & boolean features**\n",
    "5. **Quantify skewness & kurtosis** and decide on transformations\n",
    "6. **Detect & handle outliers** (winsorize, remove, or flag)\n",
    "7. **Assess relationship shape** (linear vs monotonic vs nonlinear)\n",
    "8. **Choose model family** based on aggregate shape (linear vs trees)\n",
    "9. **(Later) Build preprocessing + baseline model + CV**\n",
    "\n",
    "You can duplicate this notebook for any new regression competition and only change the config (paths, target name, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6fc998",
   "metadata": {},
   "source": [
    "## üß≠ Decision Workflow Cheat Sheet (Granular Rules)\n",
    "\n",
    "Use this as a **mental and practical checklist** every time.\n",
    "\n",
    "### 1Ô∏è‚É£ Data & Structure\n",
    "\n",
    "1. Load train (and test if available).\n",
    "2. Check:\n",
    "   - `shape` (rows, columns)\n",
    "   - dtypes\n",
    "   - missing values\n",
    "   - obvious ID columns\n",
    "3. Identify initial column groups:\n",
    "   - Numeric features\n",
    "   - Categorical features\n",
    "   - Boolean / 0‚Äì1 features\n",
    "   - Target column\n",
    "   - ID column(s)\n",
    "\n",
    "> üìå **Action**: If something looks wrong (e.g. target all zeros, date parsed as object, weird dtypes), fix **before** going further.\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Numeric EDA: Distributions & Relationships\n",
    "\n",
    "For numeric columns (excluding target and IDs):\n",
    "\n",
    "1. Plot histograms for each numeric feature.\n",
    "2. Plot **target distribution** (hist + boxplot).\n",
    "3. Plot **scatter plots** of feature vs target for a subset of numeric features.\n",
    "4. Compute correlations:\n",
    "   - Pearson (linear) \n",
    "   - Spearman (rank / monotonic) for sanity checks later (optional).\n",
    "\n",
    "**Interpretation rules:**\n",
    "\n",
    "- If a feature‚Äôs scatter vs target looks roughly like a **straight band** ‚Üí relationship is approximately **linear**.\n",
    "- If it is curved (U-shape, log curve, exponential, plateauing) ‚Üí **nonlinear**.\n",
    "- If there is no clear pattern ‚Üí likely **weak/no signal** or dominated by noise.\n",
    "\n",
    "> üìå **If most of your strong features look linear:**  \n",
    "> ‚Üí Linear models (Ridge/ElasticNet) are a good first baseline (after transformations).  \n",
    "> üìå **If most look clearly nonlinear/curved/step-like:**  \n",
    "> ‚Üí Start with tree-based models (LightGBM/XGBoost/CatBoost).  \n",
    "> üìå **If it‚Äôs a mix or unclear:**  \n",
    "> ‚Üí Start with a tree model (safe default), then experiment with linear models later.\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ Categorical & Boolean EDA\n",
    "\n",
    "For categorical features:\n",
    "\n",
    "- Look at value counts.\n",
    "- Compute target statistics by category (mean, count, etc.).\n",
    "- Plot **boxplots/violins** of target vs category.\n",
    "- Compute **Cram√©r‚Äôs V** between categoricals to find redundancies.\n",
    "\n",
    "For boolean / 0‚Äì1 features:\n",
    "\n",
    "- Compute **point-biserial correlation** with the target.\n",
    "- Plot boxplots of target vs boolean value.\n",
    "\n",
    "**Interpretation rules:**\n",
    "\n",
    "- Categories with very different target means are **highly informative**.\n",
    "- Categoricals strongly associated with each other (high Cram√©r‚Äôs V) may be redundant.\n",
    "- Boolean features with high |correlation| with target are good candidates to keep; others might be weak.\n",
    "\n",
    "> üìå **Action**:  \n",
    "> - Plan encodings (one-hot, target encoding, CatBoost handling).  \n",
    "> - Consider merging rare categories if cardinality is high.\n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ Skewness & Kurtosis: Shape of Numeric Distributions\n",
    "\n",
    "For each numeric feature (including target):\n",
    "\n",
    "- Compute **skewness** and **kurtosis**.\n",
    "\n",
    "**Skewness rules of thumb:**\n",
    "\n",
    "- `|skew| < 0.5` ‚Üí approximately symmetric\n",
    "- `0.5 ‚â§ |skew| ‚â§ 1.0` ‚Üí moderately skewed\n",
    "- `|skew| > 1.0` ‚Üí highly skewed\n",
    "\n",
    "**Kurtosis (Fisher=False) rules:**\n",
    "\n",
    "- `‚âà 3` ‚Üí roughly normal tails\n",
    "- `> 3` ‚Üí heavy tails (more outliers)\n",
    "- `< 3` ‚Üí light tails\n",
    "\n",
    "**Transformation decisions:**\n",
    "\n",
    "- If `|skew| < 0.5` ‚Üí leave as is (no transform needed for shape).\n",
    "- If `0.5 ‚â§ |skew| ‚â§ 1.0`:\n",
    "  - Consider **log1p** or **sqrt** transform for **right-skewed** (positive skew) features.\n",
    "  - For **left-skewed** features, you can reflect: `x' = max(x) - x + 1`, then log/sqrt.\n",
    "- If `|skew| > 1.0`:\n",
    "  - Strong candidate for transformation:\n",
    "    - `log1p(x)` if x ‚â• 0\n",
    "    - Box-Cox or Yeo‚ÄìJohnson if more flexibility is needed\n",
    "  - Also examine for outliers.\n",
    "\n",
    "> üìå **Model choice impact:**  \n",
    "> - Linear models prefer **low skew + near-normal residuals**.  \n",
    "> - Tree models handle skew fine, but removing extreme skew can still improve generalization.\n",
    "\n",
    "---\n",
    "\n",
    "### 5Ô∏è‚É£ Outlier Detection & Handling\n",
    "\n",
    "Goal: reduce the effect of **unreasonably extreme values** that can distort training, especially for linear models.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. For key numeric features (and/or all of them):\n",
    "   - Use **IQR rule** or **z-score** to flag outliers.\n",
    "   - Optionally use **IsolationForest** for multivariate detection.\n",
    "2. Compare target distribution **with vs without** outliers to see their impact.\n",
    "\n",
    "**Decision rules:**\n",
    "\n",
    "- **If outliers are legitimate signal** (e.g., very rich customers, very large valid sales):  \n",
    "  - Consider **keeping them**, especially if using tree-based models.\n",
    "- **If outliers are likely errors / noise / impossible values**:  \n",
    "  - Remove those rows outright.\n",
    "- **If outliers are extreme but plausible and hurting linear models**:  \n",
    "  - Use **winsorization** (clip to low/high percentiles, e.g. 1% and 99%).  \n",
    "  - Or transform (log) then clip less aggressively.\n",
    "\n",
    "General strategies:\n",
    "\n",
    "- `strategy=\"winsorize\"` ‚Üí good baseline for regression.  \n",
    "- `strategy=\"remove\"` ‚Üí use cautiously; track % of data removed.  \n",
    "- `strategy=\"flag\"` ‚Üí keep original values but add `_outlier` indicator features.\n",
    "\n",
    "> üìå **Best practice:** For contest work, start with winsorizing or flagging rather than deleting.\n",
    "\n",
    "---\n",
    "\n",
    "### 6Ô∏è‚É£ Relationship Shape & Model Family\n",
    "\n",
    "Use scatter plots, Pearson vs Spearman correlations, and your EDA impressions to classify features:\n",
    "\n",
    "- **Linear relationship**: roughly straight trend in scatter, high Pearson & Spearman.\n",
    "- **Nonlinear monotonic**: curved trend but always increasing/decreasing; low Pearson, higher Spearman.\n",
    "- **Nonlinear non-monotonic**: U-shapes, plateaus, or complicated patterns.\n",
    "- **No clear relationship**: cloud with no pattern.\n",
    "\n",
    "**Model choice rules:**\n",
    "\n",
    "- If **most strong features are linear** **and** you‚Äôre comfortable with transformations:  \n",
    "  ‚Üí Try a **linear regression / Ridge / ElasticNet baseline** after fixing skew & outliers.\n",
    "- If **many features are clearly nonlinear or monotonic but curved**:  \n",
    "  ‚Üí Prefer **tree-based gradient boosting** (LightGBM/XGBoost/CatBoost).\n",
    "- If the picture is **mixed** (some linear, some nonlinear) or unclear:  \n",
    "  ‚Üí Start with **LightGBM** (good default for tabular).  \n",
    "  ‚Üí Later, build a **linear baseline** to compare.\n",
    "\n",
    "> ‚ùó You almost never build separate models per feature.  \n",
    "> You **transform features** based on their shapes, then feed them into a single model (or ensemble).\n",
    "\n",
    "---\n",
    "\n",
    "### 7Ô∏è‚É£ Putting It All Together (Execution Flow)\n",
    "\n",
    "When you open a new regression dataset, follow this order:\n",
    "\n",
    "1. **Config & Data Load**\n",
    "   - Set paths, target name, ID column.\n",
    "   - Load `train_df` (and `test_df` if available).\n",
    "\n",
    "2. **Initial Structure Check**\n",
    "   - Run `summarize_dataframe(train_df)`.\n",
    "   - Fix obvious issues (dtypes, weird IDs, broken target).\n",
    "\n",
    "3. **Column Typing**\n",
    "   - Use helpers to get `num_cols`, `cat_cols`, `bool_cols`.\n",
    "\n",
    "4. **Numeric EDA**\n",
    "   - Plot target distribution.  \n",
    "   - Plot numeric feature histograms and a subset of feature-vs-target scatter plots.  \n",
    "   - Check correlation with target (Pearson).\n",
    "\n",
    "5. **Categorical & Boolean EDA**\n",
    "   - Examine value counts.  \n",
    "   - Summarize target by category and plot box/violin.  \n",
    "   - Compute Cram√©r‚Äôs V matrix for categoricals; point-biserial correlations for booleans.\n",
    "\n",
    "6. **Skewness & Kurtosis**\n",
    "   - Compute skew/kurtosis for all numeric features.  \n",
    "   - Decide which features are candidates for log / other transformations.\n",
    "\n",
    "7. **Outliers**\n",
    "   - Use IQR/Z-score/IsolationForest to flag outliers on key columns.  \n",
    "   - Compare target with/without to see impact.  \n",
    "   - Apply chosen strategy: winsorize / remove / flag.\n",
    "\n",
    "8. **Model Strategy Planning**\n",
    "   - Based on shapes and correlations:  \n",
    "     - If mostly linear ‚Üí plan a linear model baseline + engineered features.  \n",
    "     - If mostly nonlinear ‚Üí plan tree-based models.  \n",
    "     - If mixed ‚Üí start with trees, later add linear baseline.\n",
    "\n",
    "9. **(Next Notebook Sections)**\n",
    "   - Implement preprocessing (encoders, scalers, transformers).  \n",
    "   - Build train/validation split (KFold, TimeSeriesSplit, etc.).  \n",
    "   - Train baseline models and compare metrics.  \n",
    "   - Iterate with feature engineering and ensembling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7cebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1. Imports & Config ==========\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import skew, kurtosis, chi2_contingency, pointbiserialr, zscore\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Display & plotting options\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:,.4f}\")\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "# ---- Project-level config (edit per dataset/competition) ----\n",
    "DATA_DIR = Path(\"../input\")      # change to your data path\n",
    "TRAIN_FILE = \"train.csv\"\n",
    "TEST_FILE = \"test.csv\"           # set to None if no test set\n",
    "\n",
    "TARGET_COL = \"target\"            # change to your target column\n",
    "ID_COL = \"id\"                    # change or set to None if no ID\n",
    "\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d43011f",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Load Data\n",
    "\n",
    "Edit the `DATA_DIR`, `TRAIN_FILE`, `TEST_FILE`, and `TARGET_COL` in the config cell above to match your dataset.\n",
    "\n",
    "Then run this cell to load your train (and test, if present) data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c3076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(\n",
    "    data_dir: Path = DATA_DIR,\n",
    "    train_file: str = TRAIN_FILE,\n",
    "    test_file: Optional[str] = TEST_FILE,\n",
    "):\n",
    "    \"\"\"Load train/test DataFrames from CSV.\"\"\"\n",
    "    train_path = data_dir / train_file\n",
    "    if not train_path.exists():\n",
    "        raise FileNotFoundError(f\"Train file not found: {train_path}\")\n",
    "        \n",
    "    train_df = pd.read_csv(train_path)\n",
    "    \n",
    "    test_df = None\n",
    "    if test_file is not None:\n",
    "        test_path = data_dir / test_file\n",
    "        if test_path.exists():\n",
    "            test_df = pd.read_csv(test_path)\n",
    "        else:\n",
    "            print(f\"Test file not found: {test_path} (continuing without test_df)\")\n",
    "    \n",
    "    print(\"Train shape:\", train_df.shape)\n",
    "    if test_df is not None:\n",
    "        print(\"Test shape :\", test_df.shape)\n",
    "    else:\n",
    "        print(\"Test data  : None\")\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "train_df, test_df = load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3983100",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Column Typing & Initial Summary\n",
    "\n",
    "Use these helpers to:\n",
    "\n",
    "- Identify numeric, categorical, and boolean/0‚Äì1 features\n",
    "- Get a quick overview of the data structure, missing values, and basic stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a512222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numeric_features(df: pd.DataFrame, exclude: Optional[List[str]] = None) -> List[str]:\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if exclude:\n",
    "        num_cols = [c for c in num_cols if c not in exclude]\n",
    "    return num_cols\n",
    "\n",
    "\n",
    "def get_categorical_features(df: pd.DataFrame) -> List[str]:\n",
    "    return df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "\n",
    "def get_boolean_features(df: pd.DataFrame) -> List[str]:\n",
    "    bool_cols = df.select_dtypes(include=[\"bool\"]).columns.tolist()\n",
    "    for col in df.select_dtypes(include=[\"int64\", \"int32\", \"int16\"]).columns:\n",
    "        unique_vals = df[col].dropna().unique()\n",
    "        if len(unique_vals) <= 2 and set(unique_vals).issubset({0, 1}):\n",
    "            bool_cols.append(col)\n",
    "    return list(dict.fromkeys(bool_cols))\n",
    "\n",
    "\n",
    "def summarize_dataframe(df: pd.DataFrame, name: str = \"df\"):\n",
    "    print(f\"===== {name} SUMMARY =====\")\n",
    "    print(\"Shape:\", df.shape)\n",
    "    \n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(df.head())\n",
    "\n",
    "    print(\"\\nDtypes:\")\n",
    "    display(df.dtypes)\n",
    "\n",
    "    print(\"\\nMissing values (count):\")\n",
    "    display(df.isna().sum().sort_values(ascending=False))\n",
    "\n",
    "    print(\"\\nBasic describe (numeric):\")\n",
    "    display(df.describe().T)\n",
    "\n",
    "    print(\"\\nPossible categorical columns (heuristic):\")\n",
    "    cat_like = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == \"object\":\n",
    "            cat_like.append(col)\n",
    "        else:\n",
    "            unique_vals = df[col].nunique()\n",
    "            if unique_vals < 20 and str(df[col].dtype).startswith(\"int\"):\n",
    "                cat_like.append(col)\n",
    "    print(cat_like)\n",
    "\n",
    "\n",
    "summarize_dataframe(train_df, name=\"train_df\")\n",
    "\n",
    "\n",
    "num_cols = get_numeric_features(\n",
    "    train_df,\n",
    "    exclude=[TARGET_COL] + ([ID_COL] if ID_COL in train_df.columns else [])\n",
    ")\n",
    "cat_cols = get_categorical_features(train_df)\n",
    "bool_cols = get_boolean_features(train_df)\n",
    "\n",
    "print(\"Numeric features (first 10):\", num_cols[:10], \"...\" if len(num_cols) > 10 else \"\")\n",
    "print(\"Categorical features:\", cat_cols)\n",
    "print(\"Boolean features:\", bool_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da49861",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Numeric EDA: Distributions & Correlations\n",
    "\n",
    "Follow this sequence:\n",
    "\n",
    "1. Inspect target distribution.  \n",
    "2. Inspect numeric feature distributions.  \n",
    "3. Look at scatter plots of feature vs target.  \n",
    "4. Examine correlations with the target.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267f8dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_target_distribution(df: pd.DataFrame, target_col: str = TARGET_COL):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    sns.histplot(df[target_col], kde=True, ax=axes[0])\n",
    "    axes[0].set_title(f\"Distribution of {target_col}\")\n",
    "\n",
    "    sns.boxplot(x=df[target_col], ax=axes[1])\n",
    "    axes[1].set_title(f\"Boxplot of {target_col}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_numeric_distributions(df: pd.DataFrame, max_cols: int = 12):\n",
    "    num_cols_local = get_numeric_features(df, exclude=[TARGET_COL])\n",
    "    num_cols_local = num_cols_local[:max_cols]\n",
    "\n",
    "    n = len(num_cols_local)\n",
    "    if n == 0:\n",
    "        print(\"No numeric features to plot.\")\n",
    "        return\n",
    "\n",
    "    n_cols = 3\n",
    "    n_rows = int(np.ceil(n / n_cols))\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, col in enumerate(num_cols_local):\n",
    "        sns.histplot(df[col], kde=False, ax=axes[i])\n",
    "        axes[i].set_title(col)\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_feature_vs_target(df: pd.DataFrame, target_col: str = TARGET_COL, max_cols: int = 6):\n",
    "    num_cols_local = get_numeric_features(df, exclude=[target_col])\n",
    "    num_cols_local = num_cols_local[:max_cols]\n",
    "\n",
    "    n = len(num_cols_local)\n",
    "    if n == 0:\n",
    "        print(\"No numeric features to plot vs target.\")\n",
    "        return\n",
    "\n",
    "    n_cols = 3\n",
    "    n_rows = int(np.ceil(n / n_cols))\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, col in enumerate(num_cols_local):\n",
    "        sns.scatterplot(x=df[col], y=df[target_col], ax=axes[i], alpha=0.4)\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel(target_col)\n",
    "        axes[i].set_title(f\"{col} vs {target_col}\")\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def correlation_with_target(df: pd.DataFrame, target_col: str = TARGET_COL, top_n: int = 20):\n",
    "    num_cols_local = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if target_col not in num_cols_local:\n",
    "        print(f\"Target {target_col} is not numeric or not in df.\")\n",
    "        return\n",
    "\n",
    "    corr = df[num_cols_local].corr()[target_col].sort_values(ascending=False)\n",
    "    print(\"Top positively correlated with target:\")\n",
    "    display(corr.head(top_n))\n",
    "    print(\"\\nTop negatively correlated with target:\")\n",
    "    display(corr.tail(top_n))\n",
    "\n",
    "\n",
    "def plot_correlation_heatmap(df: pd.DataFrame, target_col: str = TARGET_COL, top_n: int = 20):\n",
    "    num_cols_local = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if target_col not in num_cols_local:\n",
    "        print(f\"Target {target_col} is not numeric or not in df.\")\n",
    "        return\n",
    "\n",
    "    corr_series = df[num_cols_local].corr()[target_col].drop(target_col)\n",
    "    top_features = corr_series.abs().sort_values(ascending=False).head(top_n).index.tolist()\n",
    "    cols_to_plot = top_features + [target_col]\n",
    "\n",
    "    corr_matrix = df[cols_to_plot].corr()\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=False, cmap=\"coolwarm\", center=0)\n",
    "    plt.title(f\"Correlation heatmap (top {top_n} correlated with {target_col})\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Run core numeric EDA\n",
    "plot_target_distribution(train_df, TARGET_COL)\n",
    "plot_numeric_distributions(train_df)\n",
    "plot_feature_vs_target(train_df, TARGET_COL)\n",
    "correlation_with_target(train_df, TARGET_COL)\n",
    "plot_correlation_heatmap(train_df, TARGET_COL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0279283a",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Categorical & Boolean EDA\n",
    "\n",
    "Now examine **categorical** and **boolean** predictors:\n",
    "\n",
    "- How the target varies across categories\n",
    "- How categoricals relate to each other (Cram√©r's V)\n",
    "- How booleans relate to the target (point-biserial correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daba4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(x, y) -> float:\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    chi2, p, dof, expected = chi2_contingency(confusion_matrix)\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    k = min(confusion_matrix.shape) - 1\n",
    "    if k == 0:\n",
    "        return np.nan\n",
    "    return np.sqrt((chi2 / n) / k)\n",
    "\n",
    "\n",
    "def cramers_v_matrix(df: pd.DataFrame, cols: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "    if cols is None:\n",
    "        cols = get_categorical_features(df)\n",
    "\n",
    "    n = len(cols)\n",
    "    result = pd.DataFrame(np.ones((n, n)), index=cols, columns=cols)\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            v = cramers_v(df[cols[i]], df[cols[j]])\n",
    "            result.iloc[i, j] = v\n",
    "            result.iloc[j, i] = v\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def plot_cramers_v_heatmap(df: pd.DataFrame, cols: Optional[List[str]] = None):\n",
    "    cv_mat = cramers_v_matrix(df, cols)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cv_mat, annot=False, cmap=\"coolwarm\", vmin=0, vmax=1)\n",
    "    plt.title(\"Cram√©r's V between categorical features\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def summarize_target_by_category(\n",
    "    df: pd.DataFrame,\n",
    "    cat_col: str,\n",
    "    target_col: str = TARGET_COL,\n",
    "    sort_by: str = \"mean\",\n",
    ") -> pd.DataFrame:\n",
    "    summary = (\n",
    "        df.groupby(cat_col)[target_col]\n",
    "        .agg([\"count\", \"mean\", \"std\", \"min\", \"max\"]\n",
    "        ).sort_values(by=sort_by, ascending=False)\n",
    "    )\n",
    "    display(summary)\n",
    "    return summary\n",
    "\n",
    "\n",
    "def plot_target_by_category(\n",
    "    df: pd.DataFrame,\n",
    "    cat_col: str,\n",
    "    target_col: str = TARGET_COL,\n",
    "    max_categories: int = 20,\n",
    "    kind: str = \"box\",\n",
    "):\n",
    "    if df[cat_col].nunique() > max_categories:\n",
    "        top_cats = df[cat_col].value_counts().head(max_categories).index\n",
    "        data = df[df[cat_col].isin(top_cats)].copy()\n",
    "    else:\n",
    "        data = df\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    if kind == \"box\":\n",
    "        sns.boxplot(x=cat_col, y=target_col, data=data)\n",
    "    elif kind == \"violin\":\n",
    "        sns.violinplot(x=cat_col, y=target_col, data=data, cut=0)\n",
    "    else:\n",
    "        raise ValueError(\"kind must be 'box' or 'violin'\")\n",
    "\n",
    "    plt.title(f\"{target_col} by {cat_col}\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def pointbiserial_correlations_with_target(\n",
    "    df: pd.DataFrame,\n",
    "    bool_cols: Optional[List[str]] = None,\n",
    "    target_col: str = TARGET_COL,\n",
    ") -> pd.DataFrame:\n",
    "    if bool_cols is None:\n",
    "        bool_cols = get_boolean_features(df)\n",
    "\n",
    "    results = []\n",
    "    for col in bool_cols:\n",
    "        series = df[col]\n",
    "        if series.dtype == \"bool\":\n",
    "            series = series.astype(int)\n",
    "\n",
    "        mask = series.notna() & df[target_col].notna()\n",
    "        if mask.sum() == 0:\n",
    "            corr = np.nan\n",
    "            pval = np.nan\n",
    "        else:\n",
    "            corr, pval = pointbiserialr(series[mask], df[target_col][mask])\n",
    "        results.append({\"feature\": col, \"corr\": corr, \"p_value\": pval})\n",
    "\n",
    "    res_df = pd.DataFrame(results).sort_values(\"corr\", key=lambda x: x.abs(), ascending=False)\n",
    "    display(res_df)\n",
    "    return res_df\n",
    "\n",
    "\n",
    "def plot_target_by_boolean(df: pd.DataFrame, bool_col: str, target_col: str = TARGET_COL):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxplot(x=df[bool_col].astype(str), y=df[target_col])\n",
    "    plt.title(f\"{target_col} by {bool_col} (bool)\")\n",
    "    plt.xlabel(bool_col)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Run basic categorical/boolean EDA if there are such columns\n",
    "if len(cat_cols) > 0:\n",
    "    print(\"\\nCram√©r's V heatmap for categorical features:\")\n",
    "    if len(cat_cols) > 1:\n",
    "        plot_cramers_v_heatmap(train_df, cat_cols)\n",
    "    for col in cat_cols[:5]:\n",
    "        print(f\"\\n=== {col} vs {TARGET_COL} ===\")\n",
    "        summarize_target_by_category(train_df, col, TARGET_COL)\n",
    "        plot_target_by_category(train_df, col, TARGET_COL, kind=\"box\")\n",
    "\n",
    "\n",
    "if len(bool_cols) > 0:\n",
    "    print(\"\\nPoint-biserial correlations with target for boolean features:\")\n",
    "    pointbiserial_correlations_with_target(train_df, bool_cols, TARGET_COL)\n",
    "    for col in bool_cols:\n",
    "        plot_target_by_boolean(train_df, col, TARGET_COL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fe0de4",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Skewness & Kurtosis: Shape & Transform Suggestions\n",
    "\n",
    "Use skewness and kurtosis to decide **which numeric features need transformation**.\n",
    "\n",
    "Rules (built into your workflow):\n",
    "\n",
    "- `|skew| < 0.5` ‚Üí leave as is.\n",
    "- `0.5 ‚â§ |skew| ‚â§ 1.0` ‚Üí consider log/sqrt transform.\n",
    "- `|skew| > 1.0` ‚Üí strong candidate for transformation (log1p, Box-Cox, Yeo‚ÄìJohnson).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653ab882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skew_kurtosis_summary(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    num_cols_local = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    summary = []\n",
    "\n",
    "    for col in num_cols_local:\n",
    "        col_data = df[col].dropna()\n",
    "        if len(col_data) == 0:\n",
    "            continue\n",
    "        summary.append({\n",
    "            \"feature\": col,\n",
    "            \"skewness\": skew(col_data),\n",
    "            \"kurtosis\": kurtosis(col_data, fisher=False)\n",
    "        })\n",
    "\n",
    "    result = pd.DataFrame(summary).set_index(\"feature\")\n",
    "    display(result.sort_values(\"skewness\", key=lambda x: x.abs(), ascending=False))\n",
    "    return result\n",
    "\n",
    "\n",
    "def suggest_log_transform(df: pd.DataFrame, skew_threshold: float = 1.0) -> List[str]:\n",
    "    num_cols_local = df.select_dtypes(include=[np.number]).columns\n",
    "    candidates = []\n",
    "\n",
    "    for col in num_cols_local:\n",
    "        col_data = df[col].dropna()\n",
    "        if len(col_data) == 0:\n",
    "            continue\n",
    "        s = skew(col_data)\n",
    "        if abs(s) > skew_threshold and col_data.min() >= 0:\n",
    "            candidates.append((col, s))\n",
    "\n",
    "    print(f\"Log-transform candidates (|skew| > {skew_threshold} and min>=0):\")\n",
    "    display(pd.DataFrame(candidates, columns=[\"feature\", \"skewness\"]))\n",
    "    return [c[0] for c in candidates]\n",
    "\n",
    "\n",
    "skew_kurtosis_df = skew_kurtosis_summary(train_df)\n",
    "log_candidates = suggest_log_transform(train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3f3450",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Outlier Detection & Handling\n",
    "\n",
    "Now use simple, consistent rules to detect and handle outliers.\n",
    "\n",
    "Recommended flow:\n",
    "\n",
    "1. Start with **IQR or z-score** on key numeric columns.  \n",
    "2. Check how many points are flagged and how much they shift target stats.  \n",
    "3. Decide whether to **keep**, **winsorize**, **remove**, or **flag**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6202af2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(df: pd.DataFrame, col: str, multiplier: float = 1.5):\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - multiplier * IQR\n",
    "    upper = Q3 + multiplier * IQR\n",
    "    mask = (df[col] < lower) | (df[col] > upper)\n",
    "    return mask, lower, upper\n",
    "\n",
    "\n",
    "def detect_outliers_z(df: pd.DataFrame, col: str, threshold: float = 3.0):\n",
    "    col_data = df[col]\n",
    "    col_z = zscore(col_data.dropna())\n",
    "    mask_raw = np.abs(col_z) > threshold\n",
    "    mask = pd.Series(False, index=df.index)\n",
    "    mask[col_data.dropna().index] = mask_raw\n",
    "    return mask\n",
    "\n",
    "\n",
    "def detect_outliers_isoforest(df: pd.DataFrame, num_cols: Optional[List[str]] = None, contamination: float = 0.01):\n",
    "    if num_cols is None:\n",
    "        num_cols = get_numeric_features(df, exclude=[TARGET_COL])\n",
    "\n",
    "    iso = IsolationForest(contamination=contamination, random_state=RANDOM_STATE)\n",
    "    preds = iso.fit_predict(df[num_cols])\n",
    "    mask = preds == -1\n",
    "    return pd.Series(mask, index=df.index)\n",
    "\n",
    "\n",
    "def compare_target_with_without_outliers(df: pd.DataFrame, mask: pd.Series, target_col: str = TARGET_COL):\n",
    "    full_mean = df[target_col].mean()\n",
    "    clean_mean = df[~mask][target_col].mean()\n",
    "    print(f\"Full target mean:      {full_mean:.4f}\")\n",
    "    print(f\"Without outliers mean: {clean_mean:.4f}\")\n",
    "    print(f\"Difference:            {clean_mean - full_mean:.4f}\")\n",
    "    print(f\"Outlier count:         {mask.sum()} / {len(df)}\")\n",
    "\n",
    "\n",
    "def winsorize_series(s: pd.Series, lower_q: float = 0.01, upper_q: float = 0.99) -> pd.Series:\n",
    "    lower = s.quantile(lower_q)\n",
    "    upper = s.quantile(upper_q)\n",
    "    return s.clip(lower, upper)\n",
    "\n",
    "\n",
    "def process_outliers(\n",
    "    df: pd.DataFrame,\n",
    "    num_cols: Optional[List[str]] = None,\n",
    "    strategy: str = \"winsorize\",\n",
    "    z_thresh: float = 3.0,\n",
    "    lower_q: float = 0.01,\n",
    "    upper_q: float = 0.99,\n",
    ") -> pd.DataFrame:\n",
    "    if num_cols is None:\n",
    "        num_cols = get_numeric_features(df, exclude=[TARGET_COL])\n",
    "\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    for col in num_cols:\n",
    "        if strategy == \"remove\":\n",
    "            mask = detect_outliers_z(df_processed, col, threshold=z_thresh)\n",
    "            df_processed = df_processed[~mask]\n",
    "        elif strategy == \"winsorize\":\n",
    "            df_processed[col] = winsorize_series(df_processed[col], lower_q, upper_q)\n",
    "        elif strategy == \"flag\":\n",
    "            mask = detect_outliers_z(df_processed, col, threshold=z_thresh)\n",
    "            df_processed[f\"{col}_outlier\"] = mask.astype(int)\n",
    "        else:\n",
    "            raise ValueError(\"strategy must be one of: 'remove', 'winsorize', 'flag'\")\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "\n",
    "# Example: examine outliers for a single numeric feature, if any exist\n",
    "example_col = num_cols[0] if len(num_cols) > 0 else None\n",
    "if example_col:\n",
    "    mask_iqr, low, high = detect_outliers_iqr(train_df, example_col)\n",
    "    print(f\"Example IQR outlier bounds for {example_col}: [{low:.4f}, {high:.4f}]\")\n",
    "    compare_target_with_without_outliers(train_df, mask_iqr, TARGET_COL)\n",
    "\n",
    "# Example: create a winsorized version of the training data\n",
    "train_df_processed = process_outliers(\n",
    "    train_df,\n",
    "    num_cols=get_numeric_features(train_df, exclude=[TARGET_COL]),\n",
    "    strategy=\"winsorize\",\n",
    "    lower_q=0.01,\n",
    "    upper_q=0.99,\n",
    ")\n",
    "print(\"Original shape:\", train_df.shape)\n",
    "print(\"Processed shape:\", train_df_processed.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfcc68c",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Relationship Shape & Model Family Choice (Conceptual)\n",
    "\n",
    "At this point, you have:\n",
    "\n",
    "- Cleaned understanding of the data\n",
    "- Insight into numeric and categorical behaviors\n",
    "- Skew and outliers under control\n",
    "\n",
    "Now you decide **what kind of model to try first**.\n",
    "\n",
    "### A. Quick Relationship Diagnostics (Recommended)\n",
    "\n",
    "For each numeric feature:\n",
    "\n",
    "1. Look at the scatter vs target plots.\n",
    "2. Optionally compute:\n",
    "   - Pearson correlation (linear)\n",
    "   - Spearman correlation (monotonic)\n",
    "\n",
    "**Heuristics:**\n",
    "\n",
    "- If **Pearson and Spearman are both high** and scatter looks straight-ish ‚Üí **linear** relationship.\n",
    "- If **Spearman > Pearson** and the scatter is curved but monotonic ‚Üí **nonlinear monotonic**.\n",
    "- If both correlations are low and scatter is a cloud ‚Üí likely **no strong relationship**.\n",
    "\n",
    "### B. Aggregate Decisions\n",
    "\n",
    "- If **most strong features are linear** and you‚Äôre willing to transform skewed ones:  \n",
    "  ‚Üí Start with a **linear baseline** (Ridge/ElasticNet).  \n",
    "  - Standardize features.  \n",
    "  - Consider adding polynomial or interaction terms for obvious curves.\n",
    "\n",
    "- If **many features show nonlinear / monotonic patterns**:  \n",
    "  ‚Üí Start with **tree-based gradient boosting** (LightGBM/XGBoost/CatBoost).  \n",
    "  - Encodes nonlinearity and interactions automatically.  \n",
    "  - Handles skew and outliers better by default.\n",
    "\n",
    "- If the dataset is **mixed** (some linear, some nonlinear, some categorical-heavy):  \n",
    "  ‚Üí Start with **LightGBM or CatBoost** as a robust baseline, then later:\n",
    "  - Build a linear model for interpretability.\n",
    "  - Compare performance and use both if needed (stacking/ensembling).\n",
    "\n",
    "> ‚úÖ You **do not** build separate models per feature.  \n",
    "> You **transform the features** based on their shapes and feed them into a single model (or ensemble).\n",
    "\n",
    "In a future section of this notebook (you can add later), you‚Äôll:\n",
    "\n",
    "- Build preprocessing pipelines (encoders, scalers, transformers)\n",
    "- Define train/validation splits\n",
    "- Train baseline models (LGBM / XGB / ElasticNet)\n",
    "- Evaluate and iterate\n",
    "\n",
    "For now, this notebook serves as your **guided EDA + data-shape analysis foundation** for any regression task.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
