{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1e895ef",
   "metadata": {},
   "source": [
    "# Tabular Regression Workflow Template (EDA ‚Üí Skew ‚Üí Outliers ‚Üí Model Choice)\n",
    "\n",
    "This notebook is a **reusable template** for tabular regression problems (e.g. Kaggle competitions).  \n",
    "It includes both **code** and a **granular decision workflow** so you can follow the same thought process every time.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ High-Level Workflow\n",
    "\n",
    "1. **Set config & load data**\n",
    "2. **Understand structure**: dtypes, missingness, basic stats\n",
    "3. **Explore numeric features**: distributions, feature‚Äìtarget relationships, correlations\n",
    "4. **Explore categorical & boolean features**\n",
    "5. **Quantify skewness & kurtosis** and decide on transformations\n",
    "6. **Detect & handle outliers** (winsorize, remove, or flag)\n",
    "7. **Assess relationship shape** (linear vs monotonic vs nonlinear)\n",
    "8. **Choose model family** based on aggregate shape (linear vs trees)\n",
    "9. **(Later) Build preprocessing + baseline model + CV**\n",
    "\n",
    "You can duplicate this notebook for any new regression competition and only change the config (paths, target name, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6fc998",
   "metadata": {},
   "source": [
    "## üß≠ Decision Workflow Cheat Sheet (Granular Rules)\n",
    "\n",
    "Use this as a **mental and practical checklist** every time.\n",
    "\n",
    "### 1Ô∏è‚É£ Data & Structure\n",
    "\n",
    "1. Load train (and test if available).\n",
    "2. Check:\n",
    "   - `shape` (rows, columns)\n",
    "   - dtypes\n",
    "   - missing values\n",
    "   - obvious ID columns\n",
    "3. Identify initial column groups:\n",
    "   - Numeric features\n",
    "   - Categorical features\n",
    "   - Boolean / 0‚Äì1 features\n",
    "   - Target column\n",
    "   - ID column(s)\n",
    "\n",
    "> üìå **Action**: If something looks wrong (e.g. target all zeros, date parsed as object, weird dtypes), fix **before** going further.\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Numeric EDA: Distributions & Relationships\n",
    "\n",
    "For numeric columns (excluding target and IDs):\n",
    "\n",
    "1. Plot histograms for each numeric feature.\n",
    "2. Plot **target distribution** (hist + boxplot).\n",
    "3. Plot **scatter plots** of feature vs target for a subset of numeric features.\n",
    "4. Compute correlations:\n",
    "   - Pearson (linear) \n",
    "   - Spearman (rank / monotonic) for sanity checks later (optional).\n",
    "\n",
    "**Interpretation rules:**\n",
    "\n",
    "- If a feature‚Äôs scatter vs target looks roughly like a **straight band** ‚Üí relationship is approximately **linear**.\n",
    "- If it is curved (U-shape, log curve, exponential, plateauing) ‚Üí **nonlinear**.\n",
    "- If there is no clear pattern ‚Üí likely **weak/no signal** or dominated by noise.\n",
    "\n",
    "> üìå **If most of your strong features look linear:**  \n",
    "> ‚Üí Linear models (Ridge/ElasticNet) are a good first baseline (after transformations).  \n",
    "> üìå **If most look clearly nonlinear/curved/step-like:**  \n",
    "> ‚Üí Start with tree-based models (LightGBM/XGBoost/CatBoost).  \n",
    "> üìå **If it‚Äôs a mix or unclear:**  \n",
    "> ‚Üí Start with a tree model (safe default), then experiment with linear models later.\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ Categorical & Boolean EDA\n",
    "\n",
    "For categorical features:\n",
    "\n",
    "- Look at value counts.\n",
    "- Compute target statistics by category (mean, count, etc.).\n",
    "- Plot **boxplots/violins** of target vs category.\n",
    "- Compute **Cram√©r‚Äôs V** between categoricals to find redundancies.\n",
    "\n",
    "For boolean / 0‚Äì1 features:\n",
    "\n",
    "- Compute **point-biserial correlation** with the target.\n",
    "- Plot boxplots of target vs boolean value.\n",
    "\n",
    "**Interpretation rules:**\n",
    "\n",
    "- Categories with very different target means are **highly informative**.\n",
    "- Categoricals strongly associated with each other (high Cram√©r‚Äôs V) may be redundant.\n",
    "- Boolean features with high |correlation| with target are good candidates to keep; others might be weak.\n",
    "\n",
    "> üìå **Action**:  \n",
    "> - Plan encodings (one-hot, target encoding, CatBoost handling).  \n",
    "> - Consider merging rare categories if cardinality is high.\n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ Skewness & Kurtosis: Shape of Numeric Distributions\n",
    "\n",
    "For each numeric feature (including target):\n",
    "\n",
    "- Compute **skewness** and **kurtosis**.\n",
    "\n",
    "**Skewness rules of thumb:**\n",
    "\n",
    "- `|skew| < 0.5` ‚Üí approximately symmetric\n",
    "- `0.5 ‚â§ |skew| ‚â§ 1.0` ‚Üí moderately skewed\n",
    "- `|skew| > 1.0` ‚Üí highly skewed\n",
    "\n",
    "**Kurtosis (Fisher=False) rules:**\n",
    "\n",
    "- `‚âà 3` ‚Üí roughly normal tails\n",
    "- `> 3` ‚Üí heavy tails (more outliers)\n",
    "- `< 3` ‚Üí light tails\n",
    "\n",
    "**Transformation decisions:**\n",
    "\n",
    "- If `|skew| < 0.5` ‚Üí leave as is (no transform needed for shape).\n",
    "- If `0.5 ‚â§ |skew| ‚â§ 1.0`:\n",
    "  - Consider **log1p** or **sqrt** transform for **right-skewed** (positive skew) features.\n",
    "  - For **left-skewed** features, you can reflect: `x' = max(x) - x + 1`, then log/sqrt.\n",
    "- If `|skew| > 1.0`:\n",
    "  - Strong candidate for transformation:\n",
    "    - `log1p(x)` if x ‚â• 0\n",
    "    - Box-Cox or Yeo‚ÄìJohnson if more flexibility is needed\n",
    "  - Also examine for outliers.\n",
    "\n",
    "> üìå **Model choice impact:**  \n",
    "> - Linear models prefer **low skew + near-normal residuals**.  \n",
    "> - Tree models handle skew fine, but removing extreme skew can still improve generalization.\n",
    "\n",
    "---\n",
    "\n",
    "### 5Ô∏è‚É£ Outlier Detection & Handling\n",
    "\n",
    "Goal: reduce the effect of **unreasonably extreme values** that can distort training, especially for linear models.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. For key numeric features (and/or all of them):\n",
    "   - Use **IQR rule** or **z-score** to flag outliers.\n",
    "   - Optionally use **IsolationForest** for multivariate detection.\n",
    "2. Compare target distribution **with vs without** outliers to see their impact.\n",
    "\n",
    "**Decision rules:**\n",
    "\n",
    "- **If outliers are legitimate signal** (e.g., very rich customers, very large valid sales):  \n",
    "  - Consider **keeping them**, especially if using tree-based models.\n",
    "- **If outliers are likely errors / noise / impossible values**:  \n",
    "  - Remove those rows outright.\n",
    "- **If outliers are extreme but plausible and hurting linear models**:  \n",
    "  - Use **winsorization** (clip to low/high percentiles, e.g. 1% and 99%).  \n",
    "  - Or transform (log) then clip less aggressively.\n",
    "\n",
    "General strategies:\n",
    "\n",
    "- `strategy=\"winsorize\"` ‚Üí good baseline for regression.  \n",
    "- `strategy=\"remove\"` ‚Üí use cautiously; track % of data removed.  \n",
    "- `strategy=\"flag\"` ‚Üí keep original values but add `_outlier` indicator features.\n",
    "\n",
    "> üìå **Best practice:** For contest work, start with winsorizing or flagging rather than deleting.\n",
    "\n",
    "---\n",
    "\n",
    "### 6Ô∏è‚É£ Relationship Shape & Model Family\n",
    "\n",
    "Use scatter plots, Pearson vs Spearman correlations, and your EDA impressions to classify features:\n",
    "\n",
    "- **Linear relationship**: roughly straight trend in scatter, high Pearson & Spearman.\n",
    "- **Nonlinear monotonic**: curved trend but always increasing/decreasing; low Pearson, higher Spearman.\n",
    "- **Nonlinear non-monotonic**: U-shapes, plateaus, or complicated patterns.\n",
    "- **No clear relationship**: cloud with no pattern.\n",
    "\n",
    "**Model choice rules:**\n",
    "\n",
    "- If **most strong features are linear** **and** you‚Äôre comfortable with transformations:  \n",
    "  ‚Üí Try a **linear regression / Ridge / ElasticNet baseline** after fixing skew & outliers.\n",
    "- If **many features are clearly nonlinear or monotonic but curved**:  \n",
    "  ‚Üí Prefer **tree-based gradient boosting** (LightGBM/XGBoost/CatBoost).\n",
    "- If the picture is **mixed** (some linear, some nonlinear) or unclear:  \n",
    "  ‚Üí Start with **LightGBM** (good default for tabular).  \n",
    "  ‚Üí Later, build a **linear baseline** to compare.\n",
    "\n",
    "> ‚ùó You almost never build separate models per feature.  \n",
    "> You **transform features** based on their shapes, then feed them into a single model (or ensemble).\n",
    "\n",
    "---\n",
    "\n",
    "### 7Ô∏è‚É£ Putting It All Together (Execution Flow)\n",
    "\n",
    "When you open a new regression dataset, follow this order:\n",
    "\n",
    "1. **Config & Data Load**\n",
    "   - Set paths, target name, ID column.\n",
    "   - Load `train_df` (and `test_df` if available).\n",
    "\n",
    "2. **Initial Structure Check**\n",
    "   - Run `summarize_dataframe(train_df)`.\n",
    "   - Fix obvious issues (dtypes, weird IDs, broken target).\n",
    "\n",
    "3. **Column Typing**\n",
    "   - Use helpers to get `num_cols`, `cat_cols`, `bool_cols`.\n",
    "\n",
    "4. **Numeric EDA**\n",
    "   - Plot target distribution.  \n",
    "   - Plot numeric feature histograms and a subset of feature-vs-target scatter plots.  \n",
    "   - Check correlation with target (Pearson).\n",
    "\n",
    "5. **Categorical & Boolean EDA**\n",
    "   - Examine value counts.  \n",
    "   - Summarize target by category and plot box/violin.  \n",
    "   - Compute Cram√©r‚Äôs V matrix for categoricals; point-biserial correlations for booleans.\n",
    "\n",
    "6. **Skewness & Kurtosis**\n",
    "   - Compute skew/kurtosis for all numeric features.  \n",
    "   - Decide which features are candidates for log / other transformations.\n",
    "\n",
    "7. **Outliers**\n",
    "   - Use IQR/Z-score/IsolationForest to flag outliers on key columns.  \n",
    "   - Compare target with/without to see impact.  \n",
    "   - Apply chosen strategy: winsorize / remove / flag.\n",
    "\n",
    "8. **Model Strategy Planning**\n",
    "   - Based on shapes and correlations:  \n",
    "     - If mostly linear ‚Üí plan a linear model baseline + engineered features.  \n",
    "     - If mostly nonlinear ‚Üí plan tree-based models.  \n",
    "     - If mixed ‚Üí start with trees, later add linear baseline.\n",
    "\n",
    "9. **(Next Notebook Sections)**\n",
    "   - Implement preprocessing (encoders, scalers, transformers).  \n",
    "   - Build train/validation split (KFold, TimeSeriesSplit, etc.).  \n",
    "   - Train baseline models and compare metrics.  \n",
    "   - Iterate with feature engineering and ensembling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7cebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1. Imports & Config ==========\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import skew, kurtosis, chi2_contingency, pointbiserialr, zscore\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Display & plotting options\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:,.4f}\")\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "# ---- Project-level config (edit per dataset/competition) ----\n",
    "DATA_DIR = Path(\"../input\")      # change to your data path\n",
    "TRAIN_FILE = \"train.csv\"\n",
    "TEST_FILE = \"test.csv\"           # set to None if no test set\n",
    "\n",
    "TARGET_COL = \"target\"            # change to your target column\n",
    "ID_COL = \"id\"                    # change or set to None if no ID\n",
    "\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d43011f",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Load Data\n",
    "\n",
    "Edit the `DATA_DIR`, `TRAIN_FILE`, `TEST_FILE`, and `TARGET_COL` in the config cell above to match your dataset.\n",
    "\n",
    "Then run this cell to load your train (and test, if present) data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c3076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(\n",
    "    data_dir: Path = DATA_DIR,\n",
    "    train_file: str = TRAIN_FILE,\n",
    "    test_file: Optional[str] = TEST_FILE,\n",
    "):\n",
    "    \"\"\"Load train/test DataFrames from CSV.\"\"\"\n",
    "    train_path = data_dir / train_file\n",
    "    if not train_path.exists():\n",
    "        raise FileNotFoundError(f\"Train file not found: {train_path}\")\n",
    "        \n",
    "    train_df = pd.read_csv(train_path)\n",
    "    \n",
    "    test_df = None\n",
    "    if test_file is not None:\n",
    "        test_path = data_dir / test_file\n",
    "        if test_path.exists():\n",
    "            test_df = pd.read_csv(test_path)\n",
    "        else:\n",
    "            print(f\"Test file not found: {test_path} (continuing without test_df)\")\n",
    "    \n",
    "    print(\"Train shape:\", train_df.shape)\n",
    "    if test_df is not None:\n",
    "        print(\"Test shape :\", test_df.shape)\n",
    "    else:\n",
    "        print(\"Test data  : None\")\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "train_df, test_df = load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3983100",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Column Typing & Initial Summary\n",
    "\n",
    "Use these helpers to:\n",
    "\n",
    "- Identify numeric, categorical, and boolean/0‚Äì1 features\n",
    "- Get a quick overview of the data structure, missing values, and basic stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a512222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numeric_features(df: pd.DataFrame, exclude: Optional[List[str]] = None) -> List[str]:\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if exclude:\n",
    "        num_cols = [c for c in num_cols if c not in exclude]\n",
    "    return num_cols\n",
    "\n",
    "\n",
    "def get_categorical_features(df: pd.DataFrame) -> List[str]:\n",
    "    return df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "\n",
    "def get_boolean_features(df: pd.DataFrame) -> List[str]:\n",
    "    bool_cols = df.select_dtypes(include=[\"bool\"]).columns.tolist()\n",
    "    for col in df.select_dtypes(include=[\"int64\", \"int32\", \"int16\"]).columns:\n",
    "        unique_vals = df[col].dropna().unique()\n",
    "        if len(unique_vals) <= 2 and set(unique_vals).issubset({0, 1}):\n",
    "            bool_cols.append(col)\n",
    "    return list(dict.fromkeys(bool_cols))\n",
    "\n",
    "\n",
    "def summarize_dataframe(df: pd.DataFrame, name: str = \"df\"):\n",
    "    print(f\"===== {name} SUMMARY =====\")\n",
    "    print(\"Shape:\", df.shape)\n",
    "    \n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(df.head())\n",
    "\n",
    "    print(\"\\nDtypes:\")\n",
    "    display(df.dtypes)\n",
    "\n",
    "    print(\"\\nMissing values (count):\")\n",
    "    display(df.isna().sum().sort_values(ascending=False))\n",
    "\n",
    "    print(\"\\nBasic describe (numeric):\")\n",
    "    display(df.describe().T)\n",
    "\n",
    "    print(\"\\nPossible categorical columns (heuristic):\")\n",
    "    cat_like = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == \"object\":\n",
    "            cat_like.append(col)\n",
    "        else:\n",
    "            unique_vals = df[col].nunique()\n",
    "            if unique_vals < 20 and str(df[col].dtype).startswith(\"int\"):\n",
    "                cat_like.append(col)\n",
    "    print(cat_like)\n",
    "\n",
    "\n",
    "summarize_dataframe(train_df, name=\"train_df\")\n",
    "\n",
    "\n",
    "num_cols = get_numeric_features(\n",
    "    train_df,\n",
    "    exclude=[TARGET_COL] + ([ID_COL] if ID_COL in train_df.columns else [])\n",
    ")\n",
    "cat_cols = get_categorical_features(train_df)\n",
    "bool_cols = get_boolean_features(train_df)\n",
    "\n",
    "print(\"Numeric features (first 10):\", num_cols[:10], \"...\" if len(num_cols) > 10 else \"\")\n",
    "print(\"Categorical features:\", cat_cols)\n",
    "print(\"Boolean features:\", bool_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da49861",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Numeric EDA: Distributions & Correlations\n",
    "\n",
    "Follow this sequence:\n",
    "\n",
    "1. Inspect target distribution.  \n",
    "2. Inspect numeric feature distributions.  \n",
    "3. Look at scatter plots of feature vs target.  \n",
    "4. Examine correlations with the target.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267f8dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_target_distribution(df: pd.DataFrame, target_col: str = TARGET_COL):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    sns.histplot(df[target_col], kde=True, ax=axes[0])\n",
    "    axes[0].set_title(f\"Distribution of {target_col}\")\n",
    "\n",
    "    sns.boxplot(x=df[target_col], ax=axes[1])\n",
    "    axes[1].set_title(f\"Boxplot of {target_col}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_numeric_distributions(df: pd.DataFrame, max_cols: int = 12):\n",
    "    num_cols_local = get_numeric_features(df, exclude=[TARGET_COL])\n",
    "    num_cols_local = num_cols_local[:max_cols]\n",
    "\n",
    "    n = len(num_cols_local)\n",
    "    if n == 0:\n",
    "        print(\"No numeric features to plot.\")\n",
    "        return\n",
    "\n",
    "    n_cols = 3\n",
    "    n_rows = int(np.ceil(n / n_cols))\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, col in enumerate(num_cols_local):\n",
    "        sns.histplot(df[col], kde=False, ax=axes[i])\n",
    "        axes[i].set_title(col)\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_feature_vs_target(df: pd.DataFrame, target_col: str = TARGET_COL, max_cols: int = 6):\n",
    "    num_cols_local = get_numeric_features(df, exclude=[target_col])\n",
    "    num_cols_local = num_cols_local[:max_cols]\n",
    "\n",
    "    n = len(num_cols_local)\n",
    "    if n == 0:\n",
    "        print(\"No numeric features to plot vs target.\")\n",
    "        return\n",
    "\n",
    "    n_cols = 3\n",
    "    n_rows = int(np.ceil(n / n_cols))\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, col in enumerate(num_cols_local):\n",
    "        sns.scatterplot(x=df[col], y=df[target_col], ax=axes[i], alpha=0.4)\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel(target_col)\n",
    "        axes[i].set_title(f\"{col} vs {target_col}\")\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def correlation_with_target(df: pd.DataFrame, target_col: str = TARGET_COL, top_n: int = 20):\n",
    "    num_cols_local = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if target_col not in num_cols_local:\n",
    "        print(f\"Target {target_col} is not numeric or not in df.\")\n",
    "        return\n",
    "\n",
    "    corr = df[num_cols_local].corr()[target_col].sort_values(ascending=False)\n",
    "    print(\"Top positively correlated with target:\")\n",
    "    display(corr.head(top_n))\n",
    "    print(\"\\nTop negatively correlated with target:\")\n",
    "    display(corr.tail(top_n))\n",
    "\n",
    "\n",
    "def plot_correlation_heatmap(df: pd.DataFrame, target_col: str = TARGET_COL, top_n: int = 20):\n",
    "    num_cols_local = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if target_col not in num_cols_local:\n",
    "        print(f\"Target {target_col} is not numeric or not in df.\")\n",
    "        return\n",
    "\n",
    "    corr_series = df[num_cols_local].corr()[target_col].drop(target_col)\n",
    "    top_features = corr_series.abs().sort_values(ascending=False).head(top_n).index.tolist()\n",
    "    cols_to_plot = top_features + [target_col]\n",
    "\n",
    "    corr_matrix = df[cols_to_plot].corr()\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=False, cmap=\"coolwarm\", center=0)\n",
    "    plt.title(f\"Correlation heatmap (top {top_n} correlated with {target_col})\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Run core numeric EDA\n",
    "plot_target_distribution(train_df, TARGET_COL)\n",
    "plot_numeric_distributions(train_df)\n",
    "plot_feature_vs_target(train_df, TARGET_COL)\n",
    "correlation_with_target(train_df, TARGET_COL)\n",
    "plot_correlation_heatmap(train_df, TARGET_COL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0279283a",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Categorical & Boolean EDA\n",
    "\n",
    "Now examine **categorical** and **boolean** predictors:\n",
    "\n",
    "- How the target varies across categories\n",
    "- How categoricals relate to each other (Cram√©r's V)\n",
    "- How booleans relate to the target (point-biserial correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daba4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(x, y) -> float:\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    chi2, p, dof, expected = chi2_contingency(confusion_matrix)\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    k = min(confusion_matrix.shape) - 1\n",
    "    if k == 0:\n",
    "        return np.nan\n",
    "    return np.sqrt((chi2 / n) / k)\n",
    "\n",
    "\n",
    "def cramers_v_matrix(df: pd.DataFrame, cols: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "    if cols is None:\n",
    "        cols = get_categorical_features(df)\n",
    "\n",
    "    n = len(cols)\n",
    "    result = pd.DataFrame(np.ones((n, n)), index=cols, columns=cols)\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            v = cramers_v(df[cols[i]], df[cols[j]])\n",
    "            result.iloc[i, j] = v\n",
    "            result.iloc[j, i] = v\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def plot_cramers_v_heatmap(df: pd.DataFrame, cols: Optional[List[str]] = None):\n",
    "    cv_mat = cramers_v_matrix(df, cols)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cv_mat, annot=False, cmap=\"coolwarm\", vmin=0, vmax=1)\n",
    "    plt.title(\"Cram√©r's V between categorical features\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def summarize_target_by_category(\n",
    "    df: pd.DataFrame,\n",
    "    cat_col: str,\n",
    "    target_col: str = TARGET_COL,\n",
    "    sort_by: str = \"mean\",\n",
    ") -> pd.DataFrame:\n",
    "    summary = (\n",
    "        df.groupby(cat_col)[target_col]\n",
    "        .agg([\"count\", \"mean\", \"std\", \"min\", \"max\"]\n",
    "        ).sort_values(by=sort_by, ascending=False)\n",
    "    )\n",
    "    display(summary)\n",
    "    return summary\n",
    "\n",
    "\n",
    "def plot_target_by_category(\n",
    "    df: pd.DataFrame,\n",
    "    cat_col: str,\n",
    "    target_col: str = TARGET_COL,\n",
    "    max_categories: int = 20,\n",
    "    kind: str = \"box\",\n",
    "):\n",
    "    if df[cat_col].nunique() > max_categories:\n",
    "        top_cats = df[cat_col].value_counts().head(max_categories).index\n",
    "        data = df[df[cat_col].isin(top_cats)].copy()\n",
    "    else:\n",
    "        data = df\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    if kind == \"box\":\n",
    "        sns.boxplot(x=cat_col, y=target_col, data=data)\n",
    "    elif kind == \"violin\":\n",
    "        sns.violinplot(x=cat_col, y=target_col, data=data, cut=0)\n",
    "    else:\n",
    "        raise ValueError(\"kind must be 'box' or 'violin'\")\n",
    "\n",
    "    plt.title(f\"{target_col} by {cat_col}\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def pointbiserial_correlations_with_target(\n",
    "    df: pd.DataFrame,\n",
    "    bool_cols: Optional[List[str]] = None,\n",
    "    target_col: str = TARGET_COL,\n",
    ") -> pd.DataFrame:\n",
    "    if bool_cols is None:\n",
    "        bool_cols = get_boolean_features(df)\n",
    "\n",
    "    results = []\n",
    "    for col in bool_cols:\n",
    "        series = df[col]\n",
    "        if series.dtype == \"bool\":\n",
    "            series = series.astype(int)\n",
    "\n",
    "        mask = series.notna() & df[target_col].notna()\n",
    "        if mask.sum() == 0:\n",
    "            corr = np.nan\n",
    "            pval = np.nan\n",
    "        else:\n",
    "            corr, pval = pointbiserialr(series[mask], df[target_col][mask])\n",
    "        results.append({\"feature\": col, \"corr\": corr, \"p_value\": pval})\n",
    "\n",
    "    res_df = pd.DataFrame(results).sort_values(\"corr\", key=lambda x: x.abs(), ascending=False)\n",
    "    display(res_df)\n",
    "    return res_df\n",
    "\n",
    "\n",
    "def plot_target_by_boolean(df: pd.DataFrame, bool_col: str, target_col: str = TARGET_COL):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxplot(x=df[bool_col].astype(str), y=df[target_col])\n",
    "    plt.title(f\"{target_col} by {bool_col} (bool)\")\n",
    "    plt.xlabel(bool_col)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Run basic categorical/boolean EDA if there are such columns\n",
    "if len(cat_cols) > 0:\n",
    "    print(\"\\nCram√©r's V heatmap for categorical features:\")\n",
    "    if len(cat_cols) > 1:\n",
    "        plot_cramers_v_heatmap(train_df, cat_cols)\n",
    "    for col in cat_cols[:5]:\n",
    "        print(f\"\\n=== {col} vs {TARGET_COL} ===\")\n",
    "        summarize_target_by_category(train_df, col, TARGET_COL)\n",
    "        plot_target_by_category(train_df, col, TARGET_COL, kind=\"box\")\n",
    "\n",
    "\n",
    "if len(bool_cols) > 0:\n",
    "    print(\"\\nPoint-biserial correlations with target for boolean features:\")\n",
    "    pointbiserial_correlations_with_target(train_df, bool_cols, TARGET_COL)\n",
    "    for col in bool_cols:\n",
    "        plot_target_by_boolean(train_df, col, TARGET_COL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fe0de4",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Skewness & Kurtosis: Shape & Transform Suggestions\n",
    "\n",
    "Use skewness and kurtosis to decide **which numeric features need transformation**.\n",
    "\n",
    "Rules (built into your workflow):\n",
    "\n",
    "- `|skew| < 0.5` ‚Üí leave as is.\n",
    "- `0.5 ‚â§ |skew| ‚â§ 1.0` ‚Üí consider log/sqrt transform.\n",
    "- `|skew| > 1.0` ‚Üí strong candidate for transformation (log1p, Box-Cox, Yeo‚ÄìJohnson).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653ab882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skew_kurtosis_summary(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    num_cols_local = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    summary = []\n",
    "\n",
    "    for col in num_cols_local:\n",
    "        col_data = df[col].dropna()\n",
    "        if len(col_data) == 0:\n",
    "            continue\n",
    "        summary.append({\n",
    "            \"feature\": col,\n",
    "            \"skewness\": skew(col_data),\n",
    "            \"kurtosis\": kurtosis(col_data, fisher=False)\n",
    "        })\n",
    "\n",
    "    result = pd.DataFrame(summary).set_index(\"feature\")\n",
    "    display(result.sort_values(\"skewness\", key=lambda x: x.abs(), ascending=False))\n",
    "    return result\n",
    "\n",
    "\n",
    "def suggest_log_transform(df: pd.DataFrame, skew_threshold: float = 1.0) -> List[str]:\n",
    "    num_cols_local = df.select_dtypes(include=[np.number]).columns\n",
    "    candidates = []\n",
    "\n",
    "    for col in num_cols_local:\n",
    "        col_data = df[col].dropna()\n",
    "        if len(col_data) == 0:\n",
    "            continue\n",
    "        s = skew(col_data)\n",
    "        if abs(s) > skew_threshold and col_data.min() >= 0:\n",
    "            candidates.append((col, s))\n",
    "\n",
    "    print(f\"Log-transform candidates (|skew| > {skew_threshold} and min>=0):\")\n",
    "    display(pd.DataFrame(candidates, columns=[\"feature\", \"skewness\"]))\n",
    "    return [c[0] for c in candidates]\n",
    "\n",
    "\n",
    "skew_kurtosis_df = skew_kurtosis_summary(train_df)\n",
    "log_candidates = suggest_log_transform(train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3f3450",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Outlier Detection & Handling\n",
    "\n",
    "Now use simple, consistent rules to detect and handle outliers.\n",
    "\n",
    "Recommended flow:\n",
    "\n",
    "1. Start with **IQR or z-score** on key numeric columns.  \n",
    "2. Check how many points are flagged and how much they shift target stats.  \n",
    "3. Decide whether to **keep**, **winsorize**, **remove**, or **flag**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6202af2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(df: pd.DataFrame, col: str, multiplier: float = 1.5):\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - multiplier * IQR\n",
    "    upper = Q3 + multiplier * IQR\n",
    "    mask = (df[col] < lower) | (df[col] > upper)\n",
    "    return mask, lower, upper\n",
    "\n",
    "\n",
    "def detect_outliers_z(df: pd.DataFrame, col: str, threshold: float = 3.0):\n",
    "    col_data = df[col]\n",
    "    col_z = zscore(col_data.dropna())\n",
    "    mask_raw = np.abs(col_z) > threshold\n",
    "    mask = pd.Series(False, index=df.index)\n",
    "    mask[col_data.dropna().index] = mask_raw\n",
    "    return mask\n",
    "\n",
    "\n",
    "def detect_outliers_isoforest(df: pd.DataFrame, num_cols: Optional[List[str]] = None, contamination: float = 0.01):\n",
    "    if num_cols is None:\n",
    "        num_cols = get_numeric_features(df, exclude=[TARGET_COL])\n",
    "\n",
    "    iso = IsolationForest(contamination=contamination, random_state=RANDOM_STATE)\n",
    "    preds = iso.fit_predict(df[num_cols])\n",
    "    mask = preds == -1\n",
    "    return pd.Series(mask, index=df.index)\n",
    "\n",
    "\n",
    "def compare_target_with_without_outliers(df: pd.DataFrame, mask: pd.Series, target_col: str = TARGET_COL):\n",
    "    full_mean = df[target_col].mean()\n",
    "    clean_mean = df[~mask][target_col].mean()\n",
    "    print(f\"Full target mean:      {full_mean:.4f}\")\n",
    "    print(f\"Without outliers mean: {clean_mean:.4f}\")\n",
    "    print(f\"Difference:            {clean_mean - full_mean:.4f}\")\n",
    "    print(f\"Outlier count:         {mask.sum()} / {len(df)}\")\n",
    "\n",
    "\n",
    "def winsorize_series(s: pd.Series, lower_q: float = 0.01, upper_q: float = 0.99) -> pd.Series:\n",
    "    lower = s.quantile(lower_q)\n",
    "    upper = s.quantile(upper_q)\n",
    "    return s.clip(lower, upper)\n",
    "\n",
    "\n",
    "def process_outliers(\n",
    "    df: pd.DataFrame,\n",
    "    num_cols: Optional[List[str]] = None,\n",
    "    strategy: str = \"winsorize\",\n",
    "    z_thresh: float = 3.0,\n",
    "    lower_q: float = 0.01,\n",
    "    upper_q: float = 0.99,\n",
    ") -> pd.DataFrame:\n",
    "    if num_cols is None:\n",
    "        num_cols = get_numeric_features(df, exclude=[TARGET_COL])\n",
    "\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    for col in num_cols:\n",
    "        if strategy == \"remove\":\n",
    "            mask = detect_outliers_z(df_processed, col, threshold=z_thresh)\n",
    "            df_processed = df_processed[~mask]\n",
    "        elif strategy == \"winsorize\":\n",
    "            df_processed[col] = winsorize_series(df_processed[col], lower_q, upper_q)\n",
    "        elif strategy == \"flag\":\n",
    "            mask = detect_outliers_z(df_processed, col, threshold=z_thresh)\n",
    "            df_processed[f\"{col}_outlier\"] = mask.astype(int)\n",
    "        else:\n",
    "            raise ValueError(\"strategy must be one of: 'remove', 'winsorize', 'flag'\")\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "\n",
    "# Example: examine outliers for a single numeric feature, if any exist\n",
    "example_col = num_cols[0] if len(num_cols) > 0 else None\n",
    "if example_col:\n",
    "    mask_iqr, low, high = detect_outliers_iqr(train_df, example_col)\n",
    "    print(f\"Example IQR outlier bounds for {example_col}: [{low:.4f}, {high:.4f}]\")\n",
    "    compare_target_with_without_outliers(train_df, mask_iqr, TARGET_COL)\n",
    "\n",
    "# Example: create a winsorized version of the training data\n",
    "train_df_processed = process_outliers(\n",
    "    train_df,\n",
    "    num_cols=get_numeric_features(train_df, exclude=[TARGET_COL]),\n",
    "    strategy=\"winsorize\",\n",
    "    lower_q=0.01,\n",
    "    upper_q=0.99,\n",
    ")\n",
    "print(\"Original shape:\", train_df.shape)\n",
    "print(\"Processed shape:\", train_df_processed.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfcc68c",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Relationship Shape & Model Family Choice (Conceptual)\n",
    "\n",
    "At this point, you have:\n",
    "\n",
    "- Cleaned understanding of the data\n",
    "- Insight into numeric and categorical behaviors\n",
    "- Skew and outliers under control\n",
    "\n",
    "Now you decide **what kind of model to try first**.\n",
    "\n",
    "### A. Quick Relationship Diagnostics (Recommended)\n",
    "\n",
    "For each numeric feature:\n",
    "\n",
    "1. Look at the scatter vs target plots.\n",
    "2. Optionally compute:\n",
    "   - Pearson correlation (linear)\n",
    "   - Spearman correlation (monotonic)\n",
    "\n",
    "**Heuristics:**\n",
    "\n",
    "- If **Pearson and Spearman are both high** and scatter looks straight-ish ‚Üí **linear** relationship.\n",
    "- If **Spearman > Pearson** and the scatter is curved but monotonic ‚Üí **nonlinear monotonic**.\n",
    "- If both correlations are low and scatter is a cloud ‚Üí likely **no strong relationship**.\n",
    "\n",
    "### B. Aggregate Decisions\n",
    "\n",
    "- If **most strong features are linear** and you‚Äôre willing to transform skewed ones:  \n",
    "  ‚Üí Start with a **linear baseline** (Ridge/ElasticNet).  \n",
    "  - Standardize features.  \n",
    "  - Consider adding polynomial or interaction terms for obvious curves.\n",
    "\n",
    "- If **many features show nonlinear / monotonic patterns**:  \n",
    "  ‚Üí Start with **tree-based gradient boosting** (LightGBM/XGBoost/CatBoost).  \n",
    "  - Encodes nonlinearity and interactions automatically.  \n",
    "  - Handles skew and outliers better by default.\n",
    "\n",
    "- If the dataset is **mixed** (some linear, some nonlinear, some categorical-heavy):  \n",
    "  ‚Üí Start with **LightGBM or CatBoost** as a robust baseline, then later:\n",
    "  - Build a linear model for interpretability.\n",
    "  - Compare performance and use both if needed (stacking/ensembling).\n",
    "\n",
    "> ‚úÖ You **do not** build separate models per feature.  \n",
    "> You **transform the features** based on their shapes and feed them into a single model (or ensemble).\n",
    "\n",
    "In a future section of this notebook (you can add later), you‚Äôll:\n",
    "\n",
    "- Build preprocessing pipelines (encoders, scalers, transformers)\n",
    "- Define train/validation splits\n",
    "- Train baseline models (LGBM / XGB / ElasticNet)\n",
    "- Evaluate and iterate\n",
    "\n",
    "For now, this notebook serves as your **guided EDA + data-shape analysis foundation** for any regression task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35da4308-f5ee-4d79-bc3e-1419274cc64f",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Feature Engineering Strategy (From Raw Columns to Useful Signals)\n",
    "\n",
    "This section is about **creating better features**, not just cleaning them.\n",
    "\n",
    "So far we have focused on:\n",
    "\n",
    "- Understanding distributions (EDA)\n",
    "- Handling skewness\n",
    "- Handling outliers\n",
    "- Understanding numeric vs categorical relationships\n",
    "\n",
    "Now we answer:\n",
    "\n",
    "> ‚ÄúHow can I **transform** these raw columns into features that make the model‚Äôs job easier?‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### üß© What Feature Engineering Is (vs Encoding)\n",
    "\n",
    "- **Encoding** = turning categorical values into numbers so the model can accept them  \n",
    "  (e.g., one-hot, target encoding)\n",
    "\n",
    "- **Feature Engineering** = transforming/expanding columns to expose useful structure  \n",
    "  (e.g., extracting year/month from date, splitting `\"Ford-F150-XL\"` into three features)\n",
    "\n",
    "**Order of operations (big picture):**\n",
    "\n",
    "1. Handle missing values  \n",
    "2. Handle skew and outliers  \n",
    "3. **Feature engineering (this section)**  \n",
    "4. Encode categoricals  \n",
    "5. Scale numeric features (if needed for linear models)  \n",
    "6. Model training\n",
    "\n",
    "---\n",
    "\n",
    "### üß≠ Types of Feature Engineering You‚Äôll Do\n",
    "\n",
    "Common buckets:\n",
    "\n",
    "1. **Date / time decomposition**\n",
    "   - From a single datetime column ‚Üí `year`, `month`, `day`, `weekday`, `hour`, etc.\n",
    "   - Good when seasonal, monthly, weekly patterns matter.\n",
    "\n",
    "2. **String / code splitting**\n",
    "   - Split structured text like `\"Ford-F150-XL\"` or `\"A12-B3\"` into multiple meaningful parts.\n",
    "   - Example: `Brand`, `Model`, `Trim`.\n",
    "\n",
    "3. **Ratios / rates / per-unit features**\n",
    "   - E.g., `income_per_person`, `sales_per_store`, `goals_per_minute`.\n",
    "   - Often more informative than raw counts.\n",
    "\n",
    "4. **Bucketization / binning**\n",
    "   - Turn a continuous numeric feature into a small number of ordered categories.\n",
    "   - E.g., `age` ‚Üí `age_bin` of `[0‚Äì18, 19‚Äì35, 36‚Äì60, 60+]`.\n",
    "\n",
    "5. **Polynomial / interaction terms**\n",
    "   - `x¬≤`, `x*y`, etc., when you suspect curved or interacting relationships for **linear models**.\n",
    "   - Tree models often learn these interactions on their own.\n",
    "\n",
    "6. **Counts / frequency features**\n",
    "   - Replace a category with its frequency in the data, or add a parallel \"count\" feature.\n",
    "   - E.g., number of times a customer ID appears.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Model-Type-Aware Feature Engineering\n",
    "\n",
    "The model family changes how aggressive you need to be:\n",
    "\n",
    "- For **tree-based models** (LightGBM, XGBoost, CatBoost):\n",
    "  - They handle nonlinearity and interactions automatically.\n",
    "  - Feature engineering helps, but you can keep it **simple and safe**:\n",
    "    - Ratios\n",
    "    - Date parts\n",
    "    - Easy binning\n",
    "    - Count features\n",
    "\n",
    "- For **linear models** (Ridge, Lasso, ElasticNet):\n",
    "  - You must **manually provide nonlinearity**:\n",
    "    - Log transforms\n",
    "    - Polynomial terms (squares, interactions)\n",
    "    - Carefully chosen bins\n",
    "\n",
    "> ‚úÖ Rule of thumb:  \n",
    "> Start with simple, interpretable feature engineering that you can justify.  \n",
    "> Add more complex transforms only if you see consistent gains in CV.\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Practical Workflow for Feature Engineering\n",
    "\n",
    "When approaching a new dataset:\n",
    "\n",
    "1. **Look at EDA findings**:\n",
    "   - Columns that correlate with target?\n",
    "   - Any code-like strings? Dates? Ratios that make sense?\n",
    "\n",
    "2. **Decide which transforms are relevant**:\n",
    "   - If you see dates ‚Üí add date parts.\n",
    "   - If you see codes like `\"A-B-C\"` ‚Üí split into sub-features.\n",
    "   - If you see ‚Äúcounts over time or groups‚Äù ‚Üí create ratios or per-unit measures.\n",
    "   - If relationships are clearly curved in scatter plots ‚Üí consider polynomials or bins.\n",
    "\n",
    "3. **Apply transformations to both `train_df` and `test_df`** consistently:\n",
    "   - Same functions, same parameters, same mappings.\n",
    "\n",
    "4. **Keep track of what you added**:\n",
    "   - Maintain a list of new feature names.\n",
    "   - Optionally tag engineered features (e.g., name them with suffixes like `_fe`, `_ratio`, `_bin`).\n",
    "\n",
    "We‚Äôll now add **reusable helper functions** for common feature engineering patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9cab7b-8ed1-40a0-b6d3-34bcd789a9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 9. Feature Engineering Helpers ==========\n",
    "\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "def add_datetime_parts(\n",
    "    df: pd.DataFrame,\n",
    "    col: str,\n",
    "    prefix: str | None = None,\n",
    "    drop_original: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert a column to datetime (if not already) and add common date parts:\n",
    "    - year, month, day, dayofweek\n",
    "    Optionally use a prefix and optionally drop the original column.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    if prefix is None:\n",
    "        prefix = col\n",
    "\n",
    "    if not np.issubdtype(df[col].dtype, np.datetime64):\n",
    "        df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "    df[f\"{prefix}_year\"] = df[col].dt.year\n",
    "    df[f\"{prefix}_month\"] = df[col].dt.month\n",
    "    df[f\"{prefix}_day\"] = df[col].dt.day\n",
    "    df[f\"{prefix}_dow\"] = df[col].dt.dayofweek  # 0 = Monday\n",
    "\n",
    "    if drop_original:\n",
    "        df = df.drop(columns=[col])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_column(\n",
    "    df: pd.DataFrame,\n",
    "    col: str,\n",
    "    sep: str,\n",
    "    new_cols: list[str],\n",
    "    drop_original: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Split a string column on a delimiter into multiple new columns.\n",
    "    Example:\n",
    "      col = \"CarModel\", value = \"Ford-F150-XL\"\n",
    "      sep = \"-\"\n",
    "      new_cols = [\"Brand\", \"Model\", \"Trim\"]\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    split_data = df[col].astype(str).str.split(sep, expand=True)\n",
    "\n",
    "    if split_data.shape[1] < len(new_cols):\n",
    "        print(f\"Warning: Not enough split parts in {col} for all new_cols\")\n",
    "\n",
    "    for i, new_col in enumerate(new_cols):\n",
    "        if i < split_data.shape[1]:\n",
    "            df[new_col] = split_data[i]\n",
    "        else:\n",
    "            df[new_col] = np.nan\n",
    "\n",
    "    if drop_original:\n",
    "        df = df.drop(columns=[col])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_ratio_feature(\n",
    "    df: pd.DataFrame,\n",
    "    num_col: str,\n",
    "    denom_col: str,\n",
    "    new_col: str | None = None,\n",
    "    epsilon: float = 1e-6\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a ratio feature: new_col = num_col / (denom_col + epsilon).\n",
    "    Useful for per-unit or per-something rates.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    if new_col is None:\n",
    "        new_col = f\"{num_col}_per_{denom_col}\"\n",
    "\n",
    "    df[new_col] = df[num_col] / (df[denom_col] + epsilon)\n",
    "    return df\n",
    "\n",
    "\n",
    "def bin_numeric_feature(\n",
    "    df: pd.DataFrame,\n",
    "    col: str,\n",
    "    bins: int | list[float],\n",
    "    labels: list[str] | None = None,\n",
    "    new_col: str | None = None,\n",
    "    strategy: str = \"quantile\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Bin a numeric feature into categories.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bins:\n",
    "        - If strategy=\"uniform\": number of equal-width bins or explicit bin edges.\n",
    "        - If strategy=\"quantile\": number of quantile-based bins.\n",
    "    strategy:\n",
    "        - \"uniform\": use pd.cut\n",
    "        - \"quantile\": use pd.qcut\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    if new_col is None:\n",
    "        new_col = f\"{col}_bin\"\n",
    "\n",
    "    if strategy == \"uniform\":\n",
    "        df[new_col] = pd.cut(df[col], bins=bins, labels=labels, include_lowest=True)\n",
    "    elif strategy == \"quantile\":\n",
    "        df[new_col] = pd.qcut(df[col], q=bins, labels=labels, duplicates=\"drop\")\n",
    "    else:\n",
    "        raise ValueError(\"strategy must be 'uniform' or 'quantile'\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_polynomial_features(\n",
    "    df: pd.DataFrame,\n",
    "    cols: list[str],\n",
    "    degree: int = 2,\n",
    "    interaction_only: bool = False,\n",
    "    prefix: str = \"poly\"\n",
    ") -> Tuple[pd.DataFrame, list[str]]:\n",
    "    \"\"\"\n",
    "    Add polynomial (and optionally interaction) features for a set of numeric columns.\n",
    "    - For degree=2 and interaction_only=False, you get squares and pairwise products.\n",
    "    - This is more useful for linear models; tree models often don‚Äôt need this.\n",
    "\n",
    "    Returns:\n",
    "      (df_with_poly, new_feature_names)\n",
    "    \"\"\"\n",
    "    from itertools import combinations_with_replacement, combinations\n",
    "\n",
    "    df = df.copy()\n",
    "    new_features = []\n",
    "\n",
    "    if degree < 2:\n",
    "        return df, new_features\n",
    "\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "\n",
    "    if interaction_only:\n",
    "        # Only pairwise products (no squares)\n",
    "        for c1, c2 in combinations(cols, 2):\n",
    "            new_col = f\"{prefix}_{c1}_x_{c2}\"\n",
    "            df[new_col] = df[c1] * df[c2]\n",
    "            new_features.append(new_col)\n",
    "    else:\n",
    "        # Squares + pairwise products\n",
    "        # Squares\n",
    "        for c in cols:\n",
    "            new_col = f\"{prefix}_{c}_sq\"\n",
    "            df[new_col] = df[c] ** 2\n",
    "            new_features.append(new_col)\n",
    "        # Interactions\n",
    "        for c1, c2 in combinations(cols, 2):\n",
    "            new_col = f\"{prefix}_{c1}_x_{c2}\"\n",
    "            df[new_col] = df[c1] * df[c2]\n",
    "            new_features.append(new_col)\n",
    "\n",
    "    return df, new_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644082cb-a9db-4e3e-a299-d5a492685d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 9.1 Example Feature Engineering Usage (Customize per dataset) ==========\n",
    "\n",
    "# Make copies so you don't overwrite original until you're happy\n",
    "fe_train = train_df_processed.copy() if \"train_df_processed\" in globals() else train_df.copy()\n",
    "fe_test = test_df.copy() if test_df is not None else None\n",
    "\n",
    "# 1) Date / time decomposition\n",
    "# If you have date columns, add them here:\n",
    "date_columns = []  # e.g. [\"date\", \"order_date\"]\n",
    "for col in date_columns:\n",
    "    fe_train = add_datetime_parts(fe_train, col, prefix=col, drop_original=False)\n",
    "    if fe_test is not None and col in fe_test.columns:\n",
    "        fe_test = add_datetime_parts(fe_test, col, prefix=col, drop_original=False)\n",
    "\n",
    "# 2) String / code splitting\n",
    "# Example: if you have something like \"Ford-F150-XL\" in a column:\n",
    "code_columns_config: Dict[str, Dict] = {\n",
    "    # \"CarModel\": {\"sep\": \"-\", \"new_cols\": [\"Brand\", \"Model\", \"Trim\"], \"drop_original\": False},\n",
    "}\n",
    "for col, cfg in code_columns_config.items():\n",
    "    fe_train = split_column(\n",
    "        fe_train,\n",
    "        col=col,\n",
    "        sep=cfg[\"sep\"],\n",
    "        new_cols=cfg[\"new_cols\"],\n",
    "        drop_original=cfg.get(\"drop_original\", False),\n",
    "    )\n",
    "    if fe_test is not None and col in fe_test.columns:\n",
    "        fe_test = split_column(\n",
    "            fe_test,\n",
    "            col=col,\n",
    "            sep=cfg[\"sep\"],\n",
    "            new_cols=cfg[\"new_cols\"],\n",
    "            drop_original=cfg.get(\"drop_original\", False),\n",
    "        )\n",
    "\n",
    "# 3) Ratio features\n",
    "# Example: income_per_person = income / household_size\n",
    "ratio_pairs = [\n",
    "    # (\"income\", \"household_size\", \"income_per_person\"),\n",
    "    # (\"sales\", \"num_stores\", None),  # None -> auto name\n",
    "]\n",
    "for num_col, denom_col, new_col in ratio_pairs:\n",
    "    if num_col in fe_train.columns and denom_col in fe_train.columns:\n",
    "        fe_train = add_ratio_feature(fe_train, num_col, denom_col, new_col=new_col)\n",
    "        if fe_test is not None and num_col in fe_test.columns and denom_col in fe_test.columns:\n",
    "            fe_test = add_ratio_feature(fe_test, num_col, denom_col, new_col=new_col)\n",
    "\n",
    "# 4) Binning / bucketization\n",
    "# Example: create age bins\n",
    "bin_config = [\n",
    "    # {\"col\": \"age\", \"bins\": [0, 18, 35, 60, 120], \"labels\": [\"0-18\", \"19-35\", \"36-60\", \"60+\"], \"strategy\": \"uniform\"},\n",
    "]\n",
    "for cfg in bin_config:\n",
    "    col = cfg[\"col\"]\n",
    "    if col in fe_train.columns:\n",
    "        fe_train = bin_numeric_feature(\n",
    "            fe_train,\n",
    "            col=col,\n",
    "            bins=cfg[\"bins\"],\n",
    "            labels=cfg.get(\"labels\"),\n",
    "            new_col=cfg.get(\"new_col\", f\"{col}_bin\"),\n",
    "            strategy=cfg.get(\"strategy\", \"uniform\"),\n",
    "        )\n",
    "        if fe_test is not None and col in fe_test.columns:\n",
    "            fe_test = bin_numeric_feature(\n",
    "                fe_test,\n",
    "                col=col,\n",
    "                bins=cfg[\"bins\"],\n",
    "                labels=cfg.get(\"labels\"),\n",
    "                new_col=cfg.get(\"new_col\", f\"{col}_bin\"),\n",
    "                strategy=cfg.get(\"strategy\", \"uniform\"),\n",
    "            )\n",
    "\n",
    "# 5) Polynomial / interaction features (mainly for linear models)\n",
    "# Choose a small subset of important numeric features to avoid explosion\n",
    "poly_base_cols = []  # e.g. [\"feature1\", \"feature2\"]\n",
    "if poly_base_cols:\n",
    "    fe_train, poly_new_features = add_polynomial_features(\n",
    "        fe_train,\n",
    "        cols=poly_base_cols,\n",
    "        degree=2,\n",
    "        interaction_only=False,\n",
    "        prefix=\"poly\",\n",
    "    )\n",
    "    if fe_test is not None:\n",
    "        # Ensure all base cols exist in fe_test\n",
    "        missing_cols = [c for c in poly_base_cols if c not in fe_test.columns]\n",
    "        if missing_cols:\n",
    "            print(\"Warning: some polynomial base cols missing in fe_test:\", missing_cols)\n",
    "        else:\n",
    "            fe_test, _ = add_polynomial_features(\n",
    "                fe_test,\n",
    "                cols=poly_base_cols,\n",
    "                degree=2,\n",
    "                interaction_only=False,\n",
    "                prefix=\"poly\",\n",
    "            )\n",
    "\n",
    "print(\"Feature-engineered train shape:\", fe_train.shape)\n",
    "if fe_test is not None:\n",
    "    print(\"Feature-engineered test shape:\", fe_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb6faba-519d-43d5-8f24-725c05e73afd",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Missing Values & Imputation Strategy\n",
    "\n",
    "Before encoding or modeling, we must resolve missing values. Most ML algorithms **cannot** handle NaNs directly, and missingness can distort distance-based metrics, scaling, and encoding.\n",
    "\n",
    "### üéØ Goals of Imputation\n",
    "- Replace missing values with **reasonable estimates**\n",
    "- Avoid introducing **bias** or **information leakage**\n",
    "- Preserve or enhance predictive signal\n",
    "- Ensure train/test consistency\n",
    "\n",
    "---\n",
    "\n",
    "### üß≠ How to Choose an Imputation Strategy\n",
    "\n",
    "Use the following logic:\n",
    "\n",
    "#### **1. Understand the Missingness Type**\n",
    "\n",
    "There are three missingness patterns:\n",
    "\n",
    "| Missingness Type | Meaning | Model Impact |\n",
    "|------------------|---------|-------------|\n",
    "| MCAR | Missing completely at random | Safe to impute with simple stats |\n",
    "| MAR | Missing depends on other columns | Might need more careful imputation |\n",
    "| MNAR | Missing depends on the value itself (e.g., salary missing for high earners) | Missingness itself is informative ‚Üí add a missing flag |\n",
    "\n",
    "**Rule:**  \n",
    "If you believe missingness conveys information, **create a `*_missing` flag** before imputing.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Decide Based on Feature Type**\n",
    "\n",
    "| Feature Type | SIMPLE Strategy (Baseline) | ADVANCED Strategy |\n",
    "|--------------|---------------------------|------------------|\n",
    "| Numeric | Median | IterativeImputer / KNNImputer |\n",
    "| Categorical | Most frequent value | Target encoding with `Unknown` bucket |\n",
    "| Boolean | Mode or treat as categorical | Rarely requires more |\n",
    "| Datetime | No median ‚Üí forward/backfill or extract date parts first | Model-based time-series fills |\n",
    "\n",
    "**Why median for numeric?**  \n",
    "- Median handles skew and outliers better than mean.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. When to Use Simple vs Advanced**\n",
    "\n",
    "**Use SIMPLE imputation when:**\n",
    "- Dataset size is medium/large\n",
    "- Data has low to moderate missingness (< 20%)\n",
    "- Model is tree-based (LightGBM/XGBoost/CatBoost)\n",
    "- You are in early prototyping\n",
    "\n",
    "**Use ADVANCED imputation when:**\n",
    "- Missingness is high or patterned\n",
    "- Linear models are planned\n",
    "- The feature is critical and sensitive\n",
    "- Imputation affects model stability\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ General Workflow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3e721d-b672-4beb-ac09-81d8896fbe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 9. Simple Imputation ==========\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def summarize_missing(df: pd.DataFrame):\n",
    "    \"\"\"Display missing percentages for each column.\"\"\"\n",
    "    missing = df.isna().mean() * 100\n",
    "    display(missing[missing > 0].sort_values(ascending=False))\n",
    "    return missing\n",
    "\n",
    "print(\"Missing values in training data:\")\n",
    "missing_report = summarize_missing(train_df)\n",
    "\n",
    "# ---- SIMPLE IMPUTERS ----\n",
    "\n",
    "numeric_imputer = SimpleImputer(strategy=\"median\")\n",
    "categorical_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "\n",
    "def apply_simple_imputation(train_df: pd.DataFrame, test_df: pd.DataFrame | None = None):\n",
    "    \"\"\"\n",
    "    Applies simple type-aware imputation to numeric and categorical features.\n",
    "    Does NOT encode or scale features.\n",
    "    \"\"\"\n",
    "    train = train_df.copy()\n",
    "    test = test_df.copy() if test_df is not None else None\n",
    "\n",
    "    num_cols = get_numeric_features(train, exclude=[TARGET_COL])\n",
    "    cat_cols = get_categorical_features(train)\n",
    "\n",
    "    # Numeric imputation\n",
    "    if len(num_cols) > 0:\n",
    "        train[num_cols] = numeric_imputer.fit_transform(train[num_cols])\n",
    "        if test is not None:\n",
    "            test[num_cols] = numeric_imputer.transform(test[num_cols])\n",
    "\n",
    "    # Categorical imputation\n",
    "    if len(cat_cols) > 0:\n",
    "        train[cat_cols] = categorical_imputer.fit_transform(train[cat_cols])\n",
    "        if test is not None:\n",
    "            test[cat_cols] = categorical_imputer.transform(test[cat_cols])\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "train_imputed, test_imputed = apply_simple_imputation(fe_train, fe_test if \"fe_test\" in globals() else None)\n",
    "print(\"After simple imputation:\", train_imputed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d944adc-a53c-41f8-aae4-c1d31254d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_flags(df: pd.DataFrame, threshold: float = 0.0):\n",
    "    \"\"\"\n",
    "    Add binary missing flags for columns with missing values above threshold.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().mean() > threshold:\n",
    "            df[col + \"_missing\"] = df[col].isna().astype(int)\n",
    "    return df\n",
    "\n",
    "train_imputed = add_missing_flags(train_imputed)\n",
    "if test_imputed is not None:\n",
    "    test_imputed = add_missing_flags(test_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f4b1fb-cf99-4519-b2b0-a2aeb72c0775",
   "metadata": {},
   "source": [
    "### üîÅ Advanced Imputation Strategies (Use Selectively)\n",
    "\n",
    "Upgrade from simple imputation when:\n",
    "\n",
    "- Missingness is correlated with important features\n",
    "- Linear models behave poorly\n",
    "- You need smoother estimates than median\n",
    "- Data has strong local neighborhoods\n",
    "\n",
    "#### Available Advanced Approaches:\n",
    "\n",
    "| Method | When to Use | Pros | Cons |\n",
    "|-------|-------------|------|------|\n",
    "| `KNNImputer` | Numeric features, local similarity | Uses nearest rows | Slow on large data |\n",
    "| `IterativeImputer` | Complex datasets, MAR patterns | Learns relationships | Risk of leakage |\n",
    "| `SoftImpute` | Matrix-like data | Captures latent structure | Heavy assumptions |\n",
    "| Group-based imputation | Business entities, customers | Logical/ domain-aware | Requires domain sense |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb65c30-8d78-43e4-ae67-d231d1fa5ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer, IterativeImputer\n",
    "\n",
    "# KNN Imputer (better for local numeric structure)\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Iterative Imputer (model-based estimation)\n",
    "iterative_imputer = IterativeImputer(random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "def apply_advanced_imputation(train_df, test_df=None, strategy=\"knn\"):\n",
    "    df_train = train_df.copy()\n",
    "    df_test = test_df.copy() if test_df is not None else None\n",
    "\n",
    "    num_cols = get_numeric_features(df_train, exclude=[TARGET_COL])\n",
    "\n",
    "    if strategy == \"knn\":\n",
    "        df_train[num_cols] = knn_imputer.fit_transform(df_train[num_cols])\n",
    "        if df_test is not None:\n",
    "            df_test[num_cols] = knn_imputer.transform(df_test[num_cols])\n",
    "\n",
    "    elif strategy == \"iterative\":\n",
    "        df_train[num_cols] = iterative_imputer.fit_transform(df_train[num_cols])\n",
    "        if df_test is not None:\n",
    "            df_test[num_cols] = iterative_imputer.transform(df_test[num_cols])\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"strategy must be 'knn' or 'iterative'\")\n",
    "\n",
    "    return df_train, df_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e0f1bf-2997-48d7-9489-435877c525e4",
   "metadata": {},
   "source": [
    "### üèÅ Summary: Imputation Decision Rules\n",
    "\n",
    "Use **Simple Imputation** when:\n",
    "- You are using tree models\n",
    "- Missingness is low-to-moderate\n",
    "- Speed and stability matter\n",
    "\n",
    "Use **Advanced Imputation** when:\n",
    "- Missingness is patterned or high\n",
    "- Linear models behave poorly\n",
    "- Numeric relationships are strong\n",
    "- You need smoother, model-aware values\n",
    "\n",
    "Add **Missingness Flags** when:\n",
    "- Missingness is likely informative\n",
    "- Data is customer/entity-based\n",
    "- You suspect MNAR\n",
    "\n",
    "> üöÄ Start simple. Only escalate when evidence tells you to.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e76e72-a167-4630-a07d-66ba8c6cef99",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Baseline Modeling & Model Comparison\n",
    "\n",
    "At this point, the data has passed through:\n",
    "\n",
    "1. EDA ‚Üí understanding distributions and relationships  \n",
    "2. Skewness & outlier handling  \n",
    "3. Feature engineering (date parts, ratios, bins, etc.)  \n",
    "4. Missingness and imputation\n",
    "\n",
    "We now:\n",
    "\n",
    "- Build **modular pipelines** for different model families\n",
    "- Use the **same preprocessed dataset** (`train_imputed`) for all of them\n",
    "- Compare performance (RMSE, R¬≤) to see which family fits this problem best\n",
    "\n",
    "> üß† Rule: only change **`model_type` strings** to switch between linear, tree-based, and deep learning models.  \n",
    "> Preprocessing is chosen automatically based on model config (scaling, encoding, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c264bc-94e4-4aa3-a12a-70d92a3f3c27",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Model Families & Modular Selection\n",
    "\n",
    "Up to this point, the notebook prepares the data:\n",
    "\n",
    "- EDA ‚Üí Skew ‚Üí Outliers ‚Üí Feature Engineering ‚Üí Missingness\n",
    "- (Next) Encoding & Scaling\n",
    "\n",
    "Now we design a **modular way to pick models**, including:\n",
    "\n",
    "- Tree-based models (LightGBM, XGBoost, RandomForest)\n",
    "- Linear models (Ridge, ElasticNet)\n",
    "- Neural nets / deep learning models (e.g., Keras MLP using tensors)\n",
    "\n",
    "Instead of hard-coding everything per model, we create:\n",
    "\n",
    "1. A **model registry**: a dictionary that describes each model type‚Äôs needs.\n",
    "2. A **ModelConfig** object: captures what preprocessing is required.\n",
    "3. A **factory function** that:\n",
    "   - looks at `model_type` (e.g., `\"lightgbm\"`, `\"elasticnet\"`, `\"keras_mlp\"`)\n",
    "   - builds a consistent pipeline:\n",
    "     - numeric preprocessing (imputation, scaling)\n",
    "     - categorical preprocessing (encoding)\n",
    "     - model itself\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Why abstract like this?\n",
    "\n",
    "Different families have different needs:\n",
    "\n",
    "| Model Family | Needs scaling? | Handles nonlinearity? | Handles categoricals directly? |\n",
    "|--------------|----------------|------------------------|---------------------------------|\n",
    "| Linear (Ridge, ElasticNet) | ‚úÖ Yes | ‚ùå Only if we engineer | ‚ùå Needs encoding |\n",
    "| Tree-based (RF, LGBM, XGB) | ‚ùå Not needed | ‚úÖ Yes | ‚ùå Usually need encoding* |\n",
    "| CatBoost | ‚ùå | ‚úÖ | ‚úÖ Yes (native cats) |\n",
    "| Neural nets (Keras MLP) | ‚úÖ Strongly recommended | ‚úÖ (via layers) | ‚ùå Needs encoding & scaling |\n",
    "\n",
    "\\* some libraries have native cat support, but standard sklearn trees do not.\n",
    "\n",
    "So we want our code to **understand** those requirements and set up preprocessing accordingly.\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Design\n",
    "\n",
    "We‚Äôll define:\n",
    "\n",
    "- A `ModelConfig` that stores:\n",
    "  - `name`\n",
    "  - `needs_scaling`\n",
    "  - `handles_categoricals_natively`\n",
    "  - `model_family` (e.g., `\"tree\"`, `\"linear\"`, `\"neural_net\"`)\n",
    "- A `MODEL_REGISTRY` dict mapping model names to `ModelConfig`.\n",
    "- A function `get_model_config(model_type)` that returns the config.\n",
    "- A function `build_model_pipeline(model_type, numeric_features, categorical_features)` that:\n",
    "  - builds a `ColumnTransformer` for numeric/categorical parts\n",
    "  - attaches the appropriate model\n",
    "  - returns a sklearn-style pipeline\n",
    "\n",
    "For deep learning / tensors:\n",
    "\n",
    "- We‚Äôll add a **Keras MLP** option where the model is built via a small `build_keras_regressor` function.\n",
    "- This gives you a path to use TensorFlow/tensors in the same framework.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa3c146-0410-4cf2-a998-e110221dd0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 11. Model Family Config & Pipeline Factory ==========\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Literal, Dict\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import ElasticNet, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Optional: comment out if packages aren't installed\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "except ImportError:\n",
    "    XGBRegressor = None\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "except ImportError:\n",
    "    LGBMRegressor = None\n",
    "\n",
    "# Optional: TensorFlow / Keras for deep learning models\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "except ImportError:\n",
    "    tf = None\n",
    "    keras = None\n",
    "\n",
    "\n",
    "ModelFamily = Literal[\"linear\", \"tree\", \"neural_net\"]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    name: str\n",
    "    family: ModelFamily\n",
    "    needs_scaling: bool\n",
    "    handles_categoricals_natively: bool = False   # e.g., CatBoost\n",
    "    notes: str = \"\"\n",
    "\n",
    "\n",
    "MODEL_REGISTRY: Dict[str, ModelConfig] = {\n",
    "    # Linear models\n",
    "    \"ridge\": ModelConfig(\n",
    "        name=\"ridge\",\n",
    "        family=\"linear\",\n",
    "        needs_scaling=True,\n",
    "        notes=\"Good baseline linear model with L2 regularization.\"\n",
    "    ),\n",
    "    \"elasticnet\": ModelConfig(\n",
    "        name=\"elasticnet\",\n",
    "        family=\"linear\",\n",
    "        needs_scaling=True,\n",
    "        notes=\"Mix of L1/L2 regularization, can do feature selection.\"\n",
    "    ),\n",
    "\n",
    "    # Tree-based models\n",
    "    \"random_forest\": ModelConfig(\n",
    "        name=\"random_forest\",\n",
    "        family=\"tree\",\n",
    "        needs_scaling=False,\n",
    "        notes=\"Bagged trees, robust but can be slower.\"\n",
    "    ),\n",
    "    \"xgboost\": ModelConfig(\n",
    "        name=\"xgboost\",\n",
    "        family=\"tree\",\n",
    "        needs_scaling=False,\n",
    "        notes=\"Gradient boosting trees, strong for tabular.\"\n",
    "    ),\n",
    "    \"lightgbm\": ModelConfig(\n",
    "        name=\"lightgbm\",\n",
    "        family=\"tree\",\n",
    "        needs_scaling=False,\n",
    "        notes=\"Fast gradient boosting, great for large tabular.\"\n",
    "    ),\n",
    "    # You could add CatBoost separately with handles_categoricals_natively=True\n",
    "\n",
    "    # Neural nets / deep learning\n",
    "    \"keras_mlp\": ModelConfig(\n",
    "        name=\"keras_mlp\",\n",
    "        family=\"neural_net\",\n",
    "        needs_scaling=True,\n",
    "        notes=\"Dense neural net via Keras; expects scaled numeric + encoded cats.\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "def get_model_config(model_type: str) -> ModelConfig:\n",
    "    if model_type not in MODEL_REGISTRY:\n",
    "        raise ValueError(f\"Unknown model_type '{model_type}'. Available: {list(MODEL_REGISTRY.keys())}\")\n",
    "    return MODEL_REGISTRY[model_type]\n",
    "\n",
    "\n",
    "# --- Builders for actual estimator objects ---\n",
    "\n",
    "def build_sklearn_regressor(model_type: str):\n",
    "    \"\"\"Return an instantiated sklearn-compatible regressor given a model_type.\"\"\"\n",
    "    cfg = get_model_config(model_type)\n",
    "\n",
    "    if cfg.family == \"linear\":\n",
    "        if model_type == \"ridge\":\n",
    "            return Ridge(alpha=1.0, random_state=RANDOM_STATE)\n",
    "        elif model_type == \"elasticnet\":\n",
    "            return ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=RANDOM_STATE)\n",
    "    \n",
    "    if cfg.family == \"tree\":\n",
    "        if model_type == \"random_forest\":\n",
    "            return RandomForestRegressor(\n",
    "                n_estimators=300,\n",
    "                max_depth=None,\n",
    "                n_jobs=-1,\n",
    "                random_state=RANDOM_STATE,\n",
    "            )\n",
    "        elif model_type == \"xgboost\":\n",
    "            if XGBRegressor is None:\n",
    "                raise ImportError(\"XGBRegressor not available. Install xgboost.\")\n",
    "            return XGBRegressor(\n",
    "                n_estimators=500,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=6,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=RANDOM_STATE,\n",
    "                tree_method=\"hist\",\n",
    "            )\n",
    "        elif model_type == \"lightgbm\":\n",
    "            if LGBMRegressor is None:\n",
    "                raise ImportError(\"LGBMRegressor not available. Install lightgbm.\")\n",
    "            return LGBMRegressor(\n",
    "                n_estimators=500,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=-1,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=RANDOM_STATE,\n",
    "            )\n",
    "\n",
    "    if cfg.family == \"neural_net\" and model_type == \"keras_mlp\":\n",
    "        if keras is None:\n",
    "            raise ImportError(\"TensorFlow/Keras not available. Install tensorflow.\")\n",
    "        # We'll build this via a wrapper below\n",
    "        return None  # placeholder; pipeline builder will handle\n",
    "\n",
    "    raise ValueError(f\"Model builder not implemented for model_type={model_type}\")\n",
    "\n",
    "\n",
    "# --- Keras MLP builder (for deep learning / tensors) ---\n",
    "\n",
    "def build_keras_mlp_regressor(input_dim: int) -> keras.Model:\n",
    "    \"\"\"\n",
    "    Simple dense neural net for regression.\n",
    "    Assumes inputs are:\n",
    "      - fully numeric\n",
    "      - scaled\n",
    "      - all categorical already encoded\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Input(shape=(input_dim,)),\n",
    "        keras.layers.Dense(128, activation=\"relu\"),\n",
    "        keras.layers.Dense(64, activation=\"relu\"),\n",
    "        keras.layers.Dense(1)  # regression output\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8b2293-c9a0-43e0-ac2f-cbbfcdebca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import numpy as np\n",
    "\n",
    "# Optional: a simple sklearn-compatible wrapper for Keras\n",
    "class KerasRegressorWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, build_fn, epochs=20, batch_size=32, verbose=0):\n",
    "        self.build_fn = build_fn\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.model_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X, dtype=\"float32\")\n",
    "        y = np.asarray(y, dtype=\"float32\")\n",
    "        self.model_ = self.build_fn(input_dim=X.shape[1])\n",
    "        self.model_.fit(\n",
    "            X,\n",
    "            y,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            verbose=self.verbose,\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X, dtype=\"float32\")\n",
    "        preds = self.model_.predict(X, verbose=0)\n",
    "        return preds.ravel()\n",
    "\n",
    "\n",
    "def build_model_pipeline(\n",
    "    model_type: str,\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str = TARGET_COL\n",
    ") -> Pipeline:\n",
    "    \"\"\"\n",
    "    Build a unified sklearn pipeline:\n",
    "      - ColumnTransformer for numeric + categorical features\n",
    "      - Optional scaling (depending on model family)\n",
    "      - Chosen model estimator (sklearn or Keras wrapper)\n",
    "    \"\"\"\n",
    "    cfg = get_model_config(model_type)\n",
    "\n",
    "    # Identify feature sets\n",
    "    num_cols = get_numeric_features(df, exclude=[target_col])\n",
    "    cat_cols = get_categorical_features(df)\n",
    "\n",
    "    numeric_transformers = []\n",
    "    categorical_transformers = []\n",
    "\n",
    "    # We assume imputation has already been done and we are now at encoding + scaling.\n",
    "    # If you want imputation inside the pipeline instead, you can add SimpleImputer here.\n",
    "\n",
    "    # Scaling: only if the model needs it (linear models & neural nets)\n",
    "    if cfg.needs_scaling and len(num_cols) > 0:\n",
    "        numeric_transformers.append((\"scaler\", StandardScaler()))\n",
    "    # If no scaling needed, we just pass-through numeric features.\n",
    "\n",
    "    # One-hot encode categoricals by default (for models that don't handle cats natively)\n",
    "    if len(cat_cols) > 0 and not cfg.handles_categoricals_natively:\n",
    "        categorical_transformers.append(\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=True))\n",
    "        )\n",
    "\n",
    "    # Build ColumnTransformer\n",
    "    transformers = []\n",
    "    if len(num_cols) > 0:\n",
    "        transformers.append((\"num\", Pipeline(numeric_transformers) if numeric_transformers else \"passthrough\", num_cols))\n",
    "    if len(cat_cols) > 0 and not cfg.handles_categoricals_natively:\n",
    "        transformers.append((\"cat\", Pipeline(categorical_transformers), cat_cols))\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=transformers,\n",
    "        remainder=\"drop\",  # drop any columns not explicitly listed\n",
    "    )\n",
    "\n",
    "    # Build estimator\n",
    "    if cfg.family == \"neural_net\" and model_type == \"keras_mlp\":\n",
    "        if keras is None:\n",
    "            raise ImportError(\"TensorFlow/Keras not installed, cannot build keras_mlp.\")\n",
    "        # We'll use the wrapper; input_dim will be inferred at fit time\n",
    "        estimator = KerasRegressorWrapper(\n",
    "            build_fn=build_keras_mlp_regressor,\n",
    "            epochs=30,\n",
    "            batch_size=64,\n",
    "            verbose=0,\n",
    "        )\n",
    "    else:\n",
    "        estimator = build_sklearn_regressor(model_type)\n",
    "\n",
    "    model_pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"model\", estimator),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81a5003-4b54-4430-b559-9ac4a930acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Choose which preprocessed data to use:\n",
    "# e.g., train_imputed from earlier steps (after FE + imputation)\n",
    "data_for_model = train_imputed  # or fe_train / fe_train_imputed, etc.\n",
    "\n",
    "X = data_for_model.drop(columns=[TARGET_COL])\n",
    "y = data_for_model[TARGET_COL]\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "for model_type in [\"lightgbm\", \"xgboost\", \"random_forest\", \"elasticnet\", \"keras_mlp\"]:\n",
    "    try:\n",
    "        print(f\"\\n=== Training model: {model_type} ===\")\n",
    "        cfg = get_model_config(model_type)\n",
    "        print(\"Config:\", cfg)\n",
    "\n",
    "        pipeline = build_model_pipeline(model_type, data_for_model, target_col=TARGET_COL)\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = pipeline.predict(X_valid)\n",
    "        rmse = mean_squared_error(y_valid, y_pred, squared=False)\n",
    "        r2 = r2_score(y_valid, y_pred)\n",
    "        print(f\"RMSE: {rmse:.4f} | R¬≤: {r2:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {model_type} due to error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af26bc3c-b3dc-405f-a5c3-6ee511627ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 12. Baseline Modeling & Model Comparison ==========\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "def evaluate_models(\n",
    "    data: pd.DataFrame,\n",
    "    target_col: str = TARGET_COL,\n",
    "    id_col: str | None = ID_COL,\n",
    "    model_types: list[str] | None = None,\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = RANDOM_STATE,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Train several model types on a simple train/validation split\n",
    "    and return a comparison table with RMSE and R¬≤.\n",
    "    \"\"\"\n",
    "    if model_types is None:\n",
    "        model_types = [\"lightgbm\", \"xgboost\", \"random_forest\", \"elasticnet\", \"keras_mlp\"]\n",
    "\n",
    "    df = data.copy()\n",
    "\n",
    "    # Drop ID column if present\n",
    "    drop_cols = [target_col]\n",
    "    if id_col is not None and id_col in df.columns:\n",
    "        drop_cols.append(id_col)\n",
    "\n",
    "    X = df.drop(columns=drop_cols)\n",
    "    y = df[target_col]\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model_type in model_types:\n",
    "        print(f\"\\n=== Training model: {model_type} ===\")\n",
    "        try:\n",
    "            cfg = get_model_config(model_type)\n",
    "            print(\"Config:\", cfg)\n",
    "\n",
    "            pipeline = build_model_pipeline(model_type, df, target_col=target_col)\n",
    "            pipeline.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = pipeline.predict(X_valid)\n",
    "            rmse = mean_squared_error(y_valid, y_pred, squared=False)\n",
    "            r2 = r2_score(y_valid, y_pred)\n",
    "            print(f\"RMSE: {rmse:.4f} | R¬≤: {r2:.4f}\")\n",
    "\n",
    "            results.append({\n",
    "                \"model_type\": model_type,\n",
    "                \"family\": cfg.family,\n",
    "                \"needs_scaling\": cfg.needs_scaling,\n",
    "                \"handles_categoricals_natively\": cfg.handles_categoricals_natively,\n",
    "                \"rmse\": rmse,\n",
    "                \"r2\": r2,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {model_type} due to error: {e}\")\n",
    "            results.append({\n",
    "                \"model_type\": model_type,\n",
    "                \"family\": None,\n",
    "                \"needs_scaling\": None,\n",
    "                \"handles_categoricals_natively\": None,\n",
    "                \"rmse\": np.nan,\n",
    "                \"r2\": np.nan,\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(results).sort_values(\"rmse\")\n",
    "    display(results_df)\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17510aa7-7576-493b-b5a0-3b76b8c885e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which dataset to model on:\n",
    "# After all steps, this should usually be train_imputed\n",
    "data_for_model = train_imputed  # or fe_train/train_df_processed if you're experimenting\n",
    "\n",
    "# Pick which models to compare; remove any you don't have installed\n",
    "models_to_try = [\n",
    "    \"lightgbm\",      # if lightgbm installed\n",
    "    \"xgboost\",       # if xgboost installed\n",
    "    \"random_forest\",\n",
    "    \"elasticnet\",\n",
    "    \"keras_mlp\",     # if tensorflow/keras installed\n",
    "]\n",
    "\n",
    "model_results = evaluate_models(\n",
    "    data=data_for_model,\n",
    "    target_col=TARGET_COL,\n",
    "    id_col=ID_COL,\n",
    "    model_types=models_to_try,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addacbcd-2ae1-44b1-92ec-5809a254a9ef",
   "metadata": {},
   "source": [
    "## üîÆ Next Steps Roadmap (For Future Refinement)\n",
    "\n",
    "The current notebook covers:\n",
    "\n",
    "1. EDA: distributions, correlations, relationships  \n",
    "2. Skew & outliers  \n",
    "3. Feature engineering (date parts, ratios, bins, polynomial options)  \n",
    "4. Missingness & imputation (simple + advanced)  \n",
    "5. Modular modeling: linear, tree-based, and neural net (Keras MLP)\n",
    "\n",
    "To go further in a competition setting, consider adding these **later**, in this order:\n",
    "\n",
    "---\n",
    "\n",
    "### 1Ô∏è‚É£ Encoding Variants (Beyond One-Hot)\n",
    "\n",
    "**When:**\n",
    "- You have high-cardinality categoricals (many unique values)\n",
    "- One-hot encoding explodes feature count\n",
    "- You want more signal from rare categories\n",
    "\n",
    "**What to add:**\n",
    "- Target encoding for high-cardinality features (with CV to avoid leakage)\n",
    "- Frequency / count encoding\n",
    "- Leave-one-out encoding\n",
    "\n",
    "**Where in notebook:**\n",
    "- Replace or extend the categorical branch inside `build_model_pipeline`:\n",
    "  - Instead of `OneHotEncoder`, use a target encoder (e.g. category_encoders library) for selected columns.\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Cross-Validation Instead of Single Train/Valid Split\n",
    "\n",
    "**When:**\n",
    "- You need more stable and reliable scores\n",
    "- LB is noisy, and you want robust local validation\n",
    "\n",
    "**What to add:**\n",
    "- KFold / StratifiedKFold (for general regression/classification)\n",
    "- GroupKFold (if grouped entities like customers, stores)\n",
    "- TimeSeriesSplit (if time-ordered data)\n",
    "\n",
    "**Where in notebook:**\n",
    "- Replace the `train_test_split` inside `evaluate_models` with a CV loop:\n",
    "  - For each fold: fit pipeline ‚Üí predict ‚Üí collect metrics ‚Üí average.\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ Model Ensembling / Blending\n",
    "\n",
    "**When:**\n",
    "- You have several strong but different models (e.g., LGBM, XGB, ElasticNet)\n",
    "- Their errors are uncorrelated\n",
    "- You want the last bit of LB improvement\n",
    "\n",
    "**What to add:**\n",
    "- Simple average of predictions (mean of model outputs)\n",
    "- Weighted average based on validation RMSE\n",
    "- Stacking model that takes multiple model predictions as input features\n",
    "\n",
    "**Where in notebook:**\n",
    "- After `evaluate_models`, add:\n",
    "  - A cell that trains multiple models fully on the whole training data\n",
    "  - Combines their predictions on validation or test data\n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ Moving Imputation/Encoding/Scaling Fully Inside Pipelines\n",
    "\n",
    "**When:**\n",
    "- You are ready for cleaner, production-like code\n",
    "- You want to avoid mistakes between train/test transformations\n",
    "\n",
    "**What to add:**\n",
    "- Move SimpleImputer and/or advanced imputers inside the `ColumnTransformer`\n",
    "- Ensure all transforms are fit only on training data\n",
    "\n",
    "**Where in notebook:**\n",
    "- Modify `build_model_pipeline`:\n",
    "  - Add `SimpleImputer` to numeric and categorical transformers\n",
    "  - Remove external imputation steps (so you no longer need `train_imputed` and can feed `fe_train` directly).\n",
    "\n",
    "---\n",
    "\n",
    "üìå **For now:**  \n",
    "You have a complete, end-to-end, logically ordered notebook that:\n",
    "\n",
    "- Teaches you the process\n",
    "- Runs with simple, robust defaults\n",
    "- Lets you plug in trees, linear models, and a Keras MLP\n",
    "\n",
    "When you‚Äôre comfortable with this, pick **one** of the roadmap items above and implement it incrementally instead of all at once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b92cf02-dc9c-425e-90ab-68348c1c8580",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
