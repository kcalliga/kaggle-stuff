{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae019b02",
   "metadata": {},
   "source": [
    "# Anomaly / Outlier Detection Template ‚Äì Unsupervised & Semi-Supervised\n",
    "\n",
    "This notebook is a reusable template for **anomaly detection** problems, where the goal is to find *unusual* points:\n",
    "\n",
    "- Fraudulent transactions  \n",
    "- Abnormal system logs  \n",
    "- Strange player performances  \n",
    "- Sensor failures / spikes  \n",
    "\n",
    "It focuses on **tabular data** and gives you a decision process for choosing:\n",
    "\n",
    "- **Unsupervised methods** (no labels): IsolationForest, LocalOutlierFactor, One-Class SVM  \n",
    "- **Semi-supervised setups** (small labeled anomalies)  \n",
    "- How to turn anomaly scores into practical flags.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ High-Level Workflow (Anomaly Detection)\n",
    "\n",
    "1. Imports & config  \n",
    "2. Load data & basic EDA (without assuming labels)  \n",
    "3. Decide **problem framing**: unsupervised vs semi-supervised  \n",
    "4. Feature selection & scaling  \n",
    "5. Train one or more anomaly detectors  \n",
    "6. Compare anomaly scores / visualize  \n",
    "7. Set thresholds & generate flags  \n",
    "8. Export anomaly labels for downstream analysis or supervised models\n",
    "\n",
    "> Many production anomaly setups end up being **two-stage**:  \n",
    "> 1) unsupervised scoring ‚Üí 2) supervised model using those scores as features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2118f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1. Imports & Config (Anomaly Detection) ==========\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "# ---- Config ----\n",
    "DATA_DIR = Path(\"../input\")\n",
    "DATA_FILE = \"data.csv\"    # change to your file\n",
    "\n",
    "ID_COL = \"id\"             # optional\n",
    "LABEL_COL = None          # set to e.g. \"is_anomaly\" if you have labels (0/1)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# If None, we'll auto-select all numeric features (except ID / label)\n",
    "ANOMALY_FEATURES: Optional[List[str]] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24fe8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 2. Load Data & Helpers ==========\n",
    "\n",
    "def load_data(data_dir: Path = DATA_DIR, data_file: str = DATA_FILE) -> pd.DataFrame:\n",
    "    path = data_dir / data_file\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Data file not found: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    print(\"Data shape:\", df.shape)\n",
    "    display(df.head())\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_numeric_features(df: pd.DataFrame, exclude: Optional[List[str]] = None) -> List[str]:\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if exclude:\n",
    "        num_cols = [c for c in num_cols if c not in exclude]\n",
    "    return num_cols\n",
    "\n",
    "\n",
    "def summarize_dataframe(df: pd.DataFrame, name: str = \"df\"):\n",
    "    print(f\"===== {name} summary =====\")\n",
    "    print(\"Shape:\", df.shape)\n",
    "    display(df.head())\n",
    "    print(\"\\nDtypes:\")\n",
    "    display(df.dtypes)\n",
    "    print(\"\\nMissing (%):\")\n",
    "    display((df.isna().mean() * 100).sort_values(ascending=False))\n",
    "\n",
    "\n",
    "df = load_data()\n",
    "summarize_dataframe(df, \"df\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c98e40",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Decide: Unsupervised vs Semi-Supervised\n",
    "\n",
    "**Question ‚Äì Do you have labels for anomalies?**\n",
    "\n",
    "- **No labels** ‚Üí pure **unsupervised** anomaly detection  \n",
    "  You will rely on scores & domain inspection.\n",
    "\n",
    "- **Some labels** (e.g., known fraud cases) ‚Üí **semi-supervised**  \n",
    "  You can:\n",
    "  - Use anomaly detectors to generate scores  \n",
    "  - Train a supervised classifier on those scores + original features\n",
    "\n",
    "This template assumes **unsupervised first**, then optionally plugs in labels if present.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d02616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 4. Feature Selection & Scaling ==========\n",
    "\n",
    "exclude_cols = []\n",
    "if ID_COL is not None and ID_COL in df.columns:\n",
    "    exclude_cols.append(ID_COL)\n",
    "if LABEL_COL is not None and LABEL_COL in df.columns:\n",
    "    exclude_cols.append(LABEL_COL)\n",
    "\n",
    "if ANOMALY_FEATURES is None:\n",
    "    feature_cols = get_numeric_features(df, exclude=exclude_cols)\n",
    "    print(\"Auto-selected numeric features:\", feature_cols)\n",
    "else:\n",
    "    feature_cols = [c for c in ANOMALY_FEATURES if c in df.columns]\n",
    "    print(\"Using configured features:\", feature_cols)\n",
    "\n",
    "X_raw = df[feature_cols].copy()\n",
    "\n",
    "# Basic scaling ‚Äì many anomaly methods are distance-based\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_raw)\n",
    "print(\"Scaled feature matrix shape:\", X_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a040a4b",
   "metadata": {},
   "source": [
    "### 5Ô∏è‚É£ Choose Anomaly Detection Method\n",
    "\n",
    "Common options:\n",
    "\n",
    "| Method             | Type            | Pros                                  | Cons                            |\n",
    "|--------------------|-----------------|---------------------------------------|----------------------------------|\n",
    "| IsolationForest    | Unsupervised    | Fast, handles high-dim tabular well   | Randomness, some tuning needed   |\n",
    "| LocalOutlierFactor | Unsupervised    | Local density, good for manifolds     | Slower, no separate `.predict`   |\n",
    "| One-Class SVM      | Unsupervised    | Kernel-based, flexible                | Very slow on large datasets      |\n",
    "\n",
    "**Decision Guide**\n",
    "\n",
    "- Start with **IsolationForest** for large tabular problems.  \n",
    "- Try **LOF** if you suspect local clusters or manifold structure.  \n",
    "- Use **One-Class SVM** only on small/medium datasets (and when you can afford tuning).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bf6869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 6. Train Multiple Anomaly Detectors ==========\n",
    "\n",
    "# IsolationForest\n",
    "iso = IsolationForest(\n",
    "    n_estimators=300,\n",
    "    contamination=\"auto\",   # or specify expected outlier fraction, e.g., 0.01\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "iso_labels = iso.fit_predict(X_scaled)     # +1 normal, -1 outlier\n",
    "iso_scores = -iso_labels                   # simple flipped label score\n",
    "iso_decision = iso.decision_function(X_scaled)  # lower = more abnormal\n",
    "\n",
    "# LocalOutlierFactor (fit_predict only)\n",
    "lof = LocalOutlierFactor(\n",
    "    n_neighbors=20,\n",
    "    contamination=\"auto\",\n",
    "    novelty=False,\n",
    ")\n",
    "lof_labels = lof.fit_predict(X_scaled)     # +1 normal, -1 outlier\n",
    "lof_scores = -lof.negative_outlier_factor_  # higher = more abnormal\n",
    "\n",
    "# One-Class SVM (optional, can be expensive)\n",
    "RUN_OCSVM = False\n",
    "ocsvm_scores = None\n",
    "if RUN_OCSVM:\n",
    "    oc = OneClassSVM(kernel=\"rbf\", gamma=\"scale\", nu=0.05)\n",
    "    oc.fit(X_scaled)\n",
    "    oc_labels = oc.predict(X_scaled)       # +1 normal, -1 outlier\n",
    "    oc_decision = -oc.decision_function(X_scaled)  # higher = more abnormal\n",
    "    ocsvm_scores = oc_decision\n",
    "\n",
    "print(\"Finished training anomaly detectors.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1784ed90",
   "metadata": {},
   "source": [
    "### 7Ô∏è‚É£ Turn Scores into Flags ‚Äì Thresholding\n",
    "\n",
    "Anomaly detectors give you **scores**, not hard labels. You need to:\n",
    "\n",
    "1. Decide on a **budget** for anomalies (e.g., 1% of points).  \n",
    "2. Choose a method (IsolationForest, LOF, etc.).  \n",
    "3. Pick a threshold such that top X% (largest scores) are flagged.\n",
    "\n",
    "If you have labels (`LABEL_COL`), you can grid-search thresholds and maximize F1 or recall at fixed precision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d32449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_top_fraction(scores, fraction=0.01):\n",
    "    # Flag top 'fraction' of points as anomalies (1), rest as 0.\n",
    "    n = len(scores)\n",
    "    k = max(1, int(n * fraction))\n",
    "    thresh = np.partition(scores, -k)[-k]\n",
    "    return (scores >= thresh).astype(int), thresh\n",
    "\n",
    "\n",
    "# Example: use LOF scores and flag top 1% as anomalies\n",
    "frac = 0.01\n",
    "lof_flags, lof_thresh = flag_top_fraction(lof_scores, fraction=frac)\n",
    "print(f\"LOF threshold={lof_thresh:.4f}, fraction={frac}\")\n",
    "\n",
    "df_anom = df.copy()\n",
    "df_anom[\"anomaly_lof\"] = lof_flags\n",
    "df_anom[\"lof_score\"] = lof_scores\n",
    "\n",
    "print(\"Anomaly counts (LOF):\")\n",
    "print(df_anom[\"anomaly_lof\"].value_counts())\n",
    "\n",
    "display(df_anom.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae414d3e",
   "metadata": {},
   "source": [
    "### 8Ô∏è‚É£ If You Have Labels: Evaluate & Calibrate Thresholds\n",
    "\n",
    "If you set `LABEL_COL` to actual anomaly labels (0 = normal, 1 = anomaly):\n",
    "\n",
    "- You can compute:\n",
    "  - Precision, recall, F1  \n",
    "  - Confusion matrix  \n",
    "- You can tune the fraction / threshold to match recall/precision targets.\n",
    "\n",
    "With heavy imbalance, prioritize **recall@fixed-precision** or **PR curves**, not just accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60344339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "if LABEL_COL is not None and LABEL_COL in df_anom.columns:\n",
    "    y_true = df_anom[LABEL_COL].values\n",
    "    y_pred = df_anom[\"anomaly_lof\"].values\n",
    "\n",
    "    print(\"Classification report (LOF-based anomalies):\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb420640",
   "metadata": {},
   "source": [
    "### 9Ô∏è‚É£ What to Do Next?\n",
    "\n",
    "- **Manual inspection**: sort by anomaly score, inspect top N rows.  \n",
    "- **Cluster anomalies**: run clustering *only on flagged anomalies* to see subtypes.  \n",
    "- **Supervised follow-up**:\n",
    "  - Use anomaly flags / scores as features in a supervised model.  \n",
    "  - Train e.g., XGBoost to predict `LABEL_COL` using original features + `lof_score`, `iso_decision`, etc.  \n",
    "- **Iterate**: adjust fraction, experiment with IsolationForest vs LOF, add domain features.\n",
    "\n",
    "You can save the dataset with anomaly scores for downstream use:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b18bc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = Path(\"./anomaly_outputs\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "df_anom.to_csv(OUTPUT_DIR / \"data_with_anomaly_scores.csv\", index=False)\n",
    "print(\"Saved data_with_anomaly_scores.csv\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
