{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12096f5b",
   "metadata": {},
   "source": [
    "# ML Template Library Overview\n",
    "\n",
    "This notebook is a **map** of your reusable ML templates.\n",
    "\n",
    "Each template is a starting point with:\n",
    "\n",
    "- Verbose markdown (when to use, what to do next)\n",
    "- Working code skeletons\n",
    "- Clear config blocks at the top\n",
    "\n",
    "You can copy any of these into a new Kaggle / project folder and edit:\n",
    "- `DATA_DIR`\n",
    "- File names (train/test)\n",
    "- Column names (ID, target, time, text, etc.)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bfc5c9",
   "metadata": {},
   "source": [
    "## 1. Core Supervised Templates (Tabular)\n",
    "\n",
    "### 1.1 Regression (single target)\n",
    "**File:** `tabular_regression_template.ipynb` (your main regression template)\n",
    "\n",
    "**Use when:**\n",
    "- Target is continuous (e.g., price, points, rating-as-number)\n",
    "- You care about RMSE/MAE/R²\n",
    "\n",
    "**Workflow:**\n",
    "1. Basic EDA\n",
    "2. Feature typing (numeric/categorical)\n",
    "3. Imputation + scaling\n",
    "4. Linear baseline + tree/boosting models\n",
    "5. Cross-validation and model comparison\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Multi-Target Regression\n",
    "**File:** `multi_target_regression_template.ipynb`\n",
    "\n",
    "**Use when:**\n",
    "- You predict **multiple numeric targets** at once (e.g., x,y,z coordinates; multiple stats per player)\n",
    "\n",
    "**Key ideas:**\n",
    "- Wrap base regressor into `MultiOutputRegressor`\n",
    "- Evaluate per-target and overall metrics\n",
    "- Optionally correlate targets and consider dimension reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8b1c02",
   "metadata": {},
   "source": [
    "### 1.3 Classification (binary / multiclass)\n",
    "**File:** `tabular_classification_template.ipynb`\n",
    "\n",
    "**Use when:**\n",
    "- Target is a **discrete label** (0/1, or multiple classes)\n",
    "- Metrics of interest: accuracy, F1, ROC-AUC, etc.\n",
    "\n",
    "**Workflow:**\n",
    "1. Explore class balance\n",
    "2. Handle missing values & encoding\n",
    "3. Try baseline models: LogisticRegression, RandomForest, XGBoost/LightGBM hook\n",
    "4. Use stratified train/validation split\n",
    "5. Inspect confusion matrix & per-class metrics\n",
    "\n",
    "**Decision guide:**\n",
    "- Strong linear separability → Logistic/Linear SVM\n",
    "- Complex interactions → Tree/boosting methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9c9a31",
   "metadata": {},
   "source": [
    "## 2. Time & Temporal Structure\n",
    "\n",
    "### 2.1 Time-Series Regression (forecasting via tabular)\n",
    "**File:** `time_series_template.ipynb`\n",
    "\n",
    "**Use when:**\n",
    "- You have `TIME_COL` (+ optional `ID_COL` for panel data)\n",
    "- You want to forecast a numeric target using lags/rolling stats\n",
    "\n",
    "**Workflow:**\n",
    "1. Sort by time, respect temporal order\n",
    "2. Create lag features, rolling means/std\n",
    "3. Time-based train/validation split (no leakage)\n",
    "4. Tree-based model on lagged features as a baseline\n",
    "5. Optionally add calendar features (day-of-week, month, etc.)\n",
    "\n",
    "**Decision guide:**\n",
    "- Start with lagged-feature + tree model\n",
    "- Move to pure time-series models (Prophet, ARIMA, DeepTS) only when baseline is exhausted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea157a8",
   "metadata": {},
   "source": [
    "### 2.2 Survival / Time-to-Event\n",
    "**File:** `survival_time_to_event_template.ipynb`\n",
    "\n",
    "**Use when:**\n",
    "- You have **time until event** (churn, injury, failure)\n",
    "- Some rows are **censored** (event not observed yet)\n",
    "\n",
    "**Core columns:**\n",
    "- `DURATION_COL`: how long each subject was observed\n",
    "- `EVENT_COL`: 1 if event occurred, 0 if censored\n",
    "\n",
    "**Workflow:**\n",
    "1. Exploratory Kaplan–Meier curves\n",
    "2. Cox proportional hazards model with `lifelines`\n",
    "3. Extract `cox_risk_score`\n",
    "4. Evaluate with concordance index\n",
    "5. Optionally create risk groups (low/med/high)\n",
    "\n",
    "**Decision guide:**\n",
    "- Start with Cox\n",
    "- If PH assumption fails or relationships are complex, explore nonlinear models (GBMs, neural nets) later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d2e98f",
   "metadata": {},
   "source": [
    "### 2.3 Sequence Classification\n",
    "**File:** `sequence_classification_template.ipynb`\n",
    "\n",
    "**Use when:**\n",
    "- Each sample is a **fixed-length sequence** (e.g., `seq_0 ... seq_T-1`)\n",
    "- You need a label for the whole sequence (normal/anomaly, class type, etc.)\n",
    "\n",
    "**Two paths inside:**\n",
    "1. Feature-based: mean/std/min/max/slope → RandomForest\n",
    "2. Optional 1D CNN (if Keras is available)\n",
    "\n",
    "**Decision guide:**\n",
    "- Start with feature-based baseline\n",
    "- Move to CNN/RNN when you suspect richer temporal patterns and have enough data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84e0340",
   "metadata": {},
   "source": [
    "## 3. Unsupervised & Semi-Supervised\n",
    "\n",
    "### 3.1 Clustering & Dimensionality Reduction\n",
    "**File:** `clustering_dimred_template.ipynb`\n",
    "\n",
    "**Use when:**\n",
    "- No labels, want to discover **structure** in tabular data\n",
    "- Need segments / cohorts / player archetypes\n",
    "- Want 2D embeddings for visualization\n",
    "\n",
    "**Workflow:**\n",
    "1. Choose features (numeric or encoded)\n",
    "2. Standardize → PCA\n",
    "3. Inspect explained variance and 2D PCA plot\n",
    "4. KMeans on PCA components; elbow + silhouette to choose k\n",
    "5. Cluster profiling (per-cluster feature means)\n",
    "6. Optional t-SNE, hierarchical, DBSCAN\n",
    "\n",
    "**Outputs:**\n",
    "- Original data + `cluster` column\n",
    "- PCA embeddings for plotting or downstream models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12e74df",
   "metadata": {},
   "source": [
    "### 3.2 Anomaly / Outlier Detection\n",
    "**File:** `anomaly_detection_template.ipynb`\n",
    "\n",
    "**Use when:**\n",
    "- You want to find **unusual** points:\n",
    "  - Fraud, weird games, ops logs anomalies, sensor glitches\n",
    "\n",
    "**Methods inside:**\n",
    "- IsolationForest (recommended baseline)\n",
    "- LocalOutlierFactor (local density)\n",
    "- Optional One-Class SVM\n",
    "\n",
    "**Workflow:**\n",
    "1. Decide unsupervised vs semi-supervised (`LABEL_COL`)\n",
    "2. Feature selection + scaling\n",
    "3. Train anomaly detectors\n",
    "4. Convert scores → flags by top X% thresholding\n",
    "5. If labels exist, tune threshold using precision/recall/F1\n",
    "\n",
    "**Next steps:**\n",
    "- Inspect top anomalies manually\n",
    "- Cluster anomalies themselves\n",
    "- Feed anomaly scores/flags into supervised models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a097b6a",
   "metadata": {},
   "source": [
    "## 4. Text & NLP\n",
    "\n",
    "### 4.1 NLP Text Classification\n",
    "**File:** `nlp_text_classification_template.ipynb`\n",
    "\n",
    "**Use when:**\n",
    "- You have text + label (e.g., sentiment, topic, toxicity)\n",
    "\n",
    "**Workflow:**\n",
    "1. Inspect label distribution and text lengths\n",
    "2. Split train/validation (stratified)\n",
    "3. TF-IDF vectorization (ngrams, max_features, min_df)\n",
    "4. Baselines:\n",
    "   - LogisticRegression\n",
    "   - RandomForest\n",
    "\n",
    "**Decision guide:**\n",
    "- Tune TF-IDF first (n-grams, min_df)\n",
    "- Try other linear models (LinearSVC, SGDClassifier)\n",
    "- Once saturated, move to transformer embeddings or end-to-end transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d499228e",
   "metadata": {},
   "source": [
    "## 5. Structured Variants\n",
    "\n",
    "### 5.1 Ordinal Regression\n",
    "**File:** `ordinal_regression_template.ipynb`\n",
    "\n",
    "**Use when:**\n",
    "- Labels are **ordered categories** (1–5, grades, severity levels)\n",
    "\n",
    "**Approaches inside:**\n",
    "1. Treat as numeric regression → round\n",
    "2. Treat as multiclass classification\n",
    "3. Simple cumulative ordinal model (K-1 logistic regressions)\n",
    "\n",
    "**Decision guide:**\n",
    "- Many ordered levels, nearly continuous → regression baseline\n",
    "- Small number of levels, care about \"distance\" between errors → ordinal cumulative model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f92a29",
   "metadata": {},
   "source": [
    "## 6. Graph-Structured Problems\n",
    "\n",
    "### 6.1 Graph ML (Classical)\n",
    "**File:** `graph_ml_template.ipynb`\n",
    "\n",
    "**Use when:**\n",
    "- You have nodes + edges (players, users, items; and their relationships)\n",
    "- Want node-level predictions or link signals without full GNN stack\n",
    "\n",
    "**Workflow:**\n",
    "1. Load `nodes.csv` and `edges.csv`\n",
    "2. Build NetworkX graph\n",
    "3. Compute node features:\n",
    "   - Degree, clustering coefficient, PageRank\n",
    "4. Node classification (if labels exist) using RandomForest\n",
    "5. Simple link prediction sketch (common neighbors)\n",
    "\n",
    "**Next-level path:**\n",
    "- Move to PyTorch Geometric / DGL for full GNNs when this baseline is exhausted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b30aa3e",
   "metadata": {},
   "source": [
    "## 7. Shared Utilities: `utils_tabular_ml.py`\n",
    "\n",
    "You also have a small **shared utilities module**:\n",
    "\n",
    "**File:** `utils_tabular_ml.py`\n",
    "\n",
    "Contains helpers for:\n",
    "\n",
    "- Loading CSVs (`load_csv`)\n",
    "- Quick summaries (`summarize_dataframe`)\n",
    "- Feature type detection:\n",
    "  - `get_numeric_features`\n",
    "  - `get_categorical_features`\n",
    "- Simple train/validation split helper (`basic_train_valid_split`)\n",
    "- EDA plotting helpers:\n",
    "  - `plot_numeric_distributions`\n",
    "  - `plot_correlation_heatmap`\n",
    "  - `plot_target_distribution`\n",
    "\n",
    "Example usage:\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "from utils_tabular_ml import (\n",
    "    load_csv,\n",
    "    summarize_dataframe,\n",
    "    get_numeric_features,\n",
    "    get_categorical_features,\n",
    "    plot_numeric_distributions,\n",
    ")\n",
    "\n",
    "DATA_DIR = Path(\"../input\")\n",
    "df = load_csv(DATA_DIR / \"train.csv\")\n",
    "summarize_dataframe(df, \"train\")\n",
    "num_cols = get_numeric_features(df, exclude=[\"id\", \"target\"])\n",
    "plot_numeric_distributions(df, cols=num_cols[:8], title=\"Numeric feature histograms\")\n",
    "```\n",
    "\n",
    "You don’t need to retrofit old notebooks right away—just start using these helpers\n",
    "for new projects or when you decide to refactor.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
