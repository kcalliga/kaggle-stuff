{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a31ec268",
   "metadata": {},
   "source": [
    "# ML Statistics Primer – Intuition-Focused Notebook\n",
    "\n",
    "This notebook is meant as a **learning notebook** packed with explanations, not just code.\n",
    "\n",
    "It covers the core statistics ideas that show up again and again in ML:\n",
    "\n",
    "- Distributions, central tendency, and spread (mean, median, variance, standard deviation)\n",
    "- Correlation and covariance\n",
    "- Sampling, the Law of Large Numbers (LLN), and the Central Limit Theorem (CLT) intuition\n",
    "- Simple linear regression and residuals\n",
    "- Bias–variance and overfitting\n",
    "\n",
    "Use this notebook to:\n",
    "- Refresh how statistics concepts connect to ML\n",
    "- Play with small simulations and visualizations\n",
    "- Build intuition you can reuse in Kaggle work and real projects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7147499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1. Imports & Basic Setup ==========\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "plt.rcParams['figure.figsize'] = (8, 5)\n",
    "plt.rcParams['figure.dpi'] = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10cd3bd",
   "metadata": {},
   "source": [
    "## 2. Distributions, Mean, Median, Variance, Standard Deviation\n",
    "\n",
    "In ML, your **features** and **target** variables all have underlying distributions.\n",
    "Understanding their shape helps you choose:\n",
    "- Transformations (log, Box–Cox, etc.)\n",
    "- Models (linear vs tree-based)\n",
    "- Whether outliers are a problem.\n",
    "\n",
    "Key quantities:\n",
    "- **Mean**: average value – sensitive to outliers.\n",
    "- **Median**: middle value – robust to outliers.\n",
    "- **Variance / standard deviation**: how spread out the values are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addb03b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 2.1 Simulate a skewed distribution ==========\n",
    "\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "data = np.random.exponential(scale=1.0, size=n)  # right-skewed\n",
    "\n",
    "mean_val = data.mean()\n",
    "median_val = np.median(data)\n",
    "var_val = data.var(ddof=1)\n",
    "std_val = data.std(ddof=1)\n",
    "\n",
    "print(f'Mean:   {mean_val:.3f}')\n",
    "print(f'Median: {median_val:.3f}')\n",
    "print(f'Var:    {var_val:.3f}')\n",
    "print(f'Std:    {std_val:.3f}')\n",
    "\n",
    "plt.hist(data, bins=40)\n",
    "plt.title('Right-skewed distribution (Exponential)')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8f3e5a",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "\n",
    "- For a right-skewed distribution, the **mean > median**.\n",
    "- The standard deviation tells you how wide the histogram is.\n",
    "\n",
    "In ML:\n",
    "- Highly skewed features can hurt some models (especially linear/regr),\n",
    "  and may benefit from log transforms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee655deb",
   "metadata": {},
   "source": [
    "## 3. Covariance and Correlation\n",
    "\n",
    "In ML, we care about how features move together and how they relate to the target.\n",
    "\n",
    "- **Covariance** measures how two variables vary together but is scale-dependent.\n",
    "- **Correlation** is a standardized covariance in [-1, 1].\n",
    "  - +1: perfect positive linear relationship\n",
    "  - 0: no linear relationship\n",
    "  - -1: perfect negative linear relationship\n",
    "\n",
    "Correlation is critical for:\n",
    "- Feature selection and redundancy checks\n",
    "- Diagnosing multicollinearity in linear models\n",
    "- Understanding which features are promising for predicting the target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ee5d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 3.1 Simulate correlated features ==========\n",
    "\n",
    "np.random.seed(0)\n",
    "n = 500\n",
    "x = np.random.normal(loc=0, scale=1, size=n)\n",
    "noise = np.random.normal(loc=0, scale=0.5, size=n)\n",
    "y = 2 * x + noise  # strongly linearly related\n",
    "\n",
    "df_corr = pd.DataFrame({'x': x, 'y': y})\n",
    "display(df_corr.head())\n",
    "\n",
    "print('Covariance matrix:')\n",
    "display(df_corr.cov())\n",
    "print('Correlation matrix:')\n",
    "display(df_corr.corr())\n",
    "\n",
    "plt.scatter(df_corr['x'], df_corr['y'], alpha=0.4)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Scatter plot of x vs y (correlated)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883a92d6",
   "metadata": {},
   "source": [
    "**Takeaways:**\n",
    "\n",
    "- Correlation helps you see **linear** relationships quickly.\n",
    "- A strong correlation with the target often means a feature will be useful in simple models.\n",
    "- For non-linear models (trees, neural nets), correlation is still helpful but not the only story.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fafc2b8",
   "metadata": {},
   "source": [
    "## 4. Sampling, LLN, and CLT Intuition\n",
    "\n",
    "ML almost always works with **samples** from some larger population (e.g., all future games, all future customers).\n",
    "\n",
    "Two key ideas:\n",
    "\n",
    "1. **Law of Large Numbers (LLN)**:\n",
    "   - Sample mean \\( \\bar{X} \\) converges to true mean \\( \\mu \\) as sample size grows.\n",
    "   - More data → more stable estimates.\n",
    "\n",
    "2. **Central Limit Theorem (CLT)**:\n",
    "   - The distribution of sample means is approximately normal,\n",
    "     even if the original data are not, for large enough sample size.\n",
    "\n",
    "These justify many ML practices:\n",
    "- Using validation metrics from random splits.\n",
    "- Using confidence intervals on performance metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1980a330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 4.1 Demonstrate LLN: sample means converge ==========\n",
    "\n",
    "np.random.seed(123)\n",
    "population = np.random.exponential(scale=1.0, size=100000)\n",
    "true_mean = population.mean()\n",
    "\n",
    "sample_sizes = [10, 50, 100, 500, 1000, 5000]\n",
    "approx_means = []\n",
    "\n",
    "for n in sample_sizes:\n",
    "    sample = np.random.choice(population, size=n, replace=False)\n",
    "    approx_means.append(sample.mean())\n",
    "\n",
    "print('True population mean:', round(true_mean, 3))\n",
    "for n, m in zip(sample_sizes, approx_means):\n",
    "    print(f'Sample size {n:4d} -> sample mean {m:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1666f8e3",
   "metadata": {},
   "source": [
    "As sample size grows, the sample mean gets closer to the true population mean.\n",
    "This is exactly why **more data usually helps** – not just for training, but also for\n",
    "more reliable estimates of performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05850d9",
   "metadata": {},
   "source": [
    "## 5. Simple Linear Regression & Residuals\n",
    "\n",
    "Linear regression assumes:\n",
    "- \\( y \\approx \\beta_0 + \\beta_1 x + \\epsilon \\)\n",
    "- Residuals \\( \\epsilon \\) are (ideally) zero-mean, roughly constant variance.\n",
    "\n",
    "In ML terms:\n",
    "- We fit a line to predict a continuous target.\n",
    "- The **residuals** (prediction errors) tell us:\n",
    "  - How well the model captures the pattern\n",
    "  - If there is systematic nonlinearity or heteroskedasticity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bae6e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 5.1 Fit a simple linear regression and inspect residuals ==========\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.random.seed(0)\n",
    "n = 200\n",
    "x = np.random.uniform(-3, 3, size=n)\n",
    "noise = np.random.normal(0, 1, size=n)\n",
    "y = 1.5 * x + 2 + noise\n",
    "\n",
    "X = x.reshape(-1, 1)\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "y_pred = linreg.predict(X)\n",
    "residuals = y - y_pred\n",
    "\n",
    "print('Estimated intercept (beta0):', linreg.intercept_)\n",
    "print('Estimated slope (beta1):   ', linreg.coef_[0])\n",
    "print('MSE:', mean_squared_error(y, y_pred))\n",
    "\n",
    "plt.scatter(x, y, alpha=0.4, label='data')\n",
    "xs = np.linspace(x.min(), x.max(), 100)\n",
    "ys = linreg.predict(xs.reshape(-1, 1))\n",
    "plt.plot(xs, ys)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear regression fit')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(y_pred, residuals, alpha=0.4)\n",
    "plt.axhline(0)\n",
    "plt.xlabel('Predicted y')\n",
    "plt.ylabel('Residual (y - y_pred)')\n",
    "plt.title('Residual plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eea9f6",
   "metadata": {},
   "source": [
    "In a good linear regression fit:\n",
    "\n",
    "- Residuals should be roughly centered around 0 with no strong patterns.\n",
    "- If you see **curvature** in residuals vs predictions, that suggests nonlinearity.\n",
    "- If residual spread grows with predicted values, that suggests non-constant variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe9fba6",
   "metadata": {},
   "source": [
    "## 6. Bias–Variance and Overfitting (Very Intuitive)\n",
    "\n",
    "In ML, we balance:\n",
    "\n",
    "- **Bias**: error from making the model too simple (underfitting).\n",
    "- **Variance**: error from the model being too flexible and fitting noise (overfitting).\n",
    "\n",
    "Typical pattern when increasing model complexity:\n",
    "\n",
    "- Training error ↓ (gets smaller and smaller)\n",
    "- Validation error ↓ then ↑ (U-shaped curve)\n",
    "\n",
    "Your goal: choose complexity where **validation error is minimized**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c925b1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 6.1 Demonstrate under/overfitting with polynomial degree ==========\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(1)\n",
    "n = 200\n",
    "X = np.linspace(-3, 3, n).reshape(-1, 1)\n",
    "y_true = np.sin(X).ravel()\n",
    "noise = np.random.normal(scale=0.3, size=n)\n",
    "y = y_true + noise\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "degrees = [1, 3, 5, 9]\n",
    "train_errors = []\n",
    "valid_errors = []\n",
    "\n",
    "for d in degrees:\n",
    "    poly = PolynomialFeatures(degree=d, include_bias=False)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_valid_poly = poly.transform(X_valid)\n",
    "\n",
    "    model = Ridge(alpha=1.0)\n",
    "    model.fit(X_train_poly, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train_poly)\n",
    "    y_valid_pred = model.predict(X_valid_poly)\n",
    "\n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    valid_mse = mean_squared_error(y_valid, y_valid_pred)\n",
    "\n",
    "    train_errors.append(train_mse)\n",
    "    valid_errors.append(valid_mse)\n",
    "\n",
    "print('Degree | Train MSE | Valid MSE')\n",
    "for d, tr, va in zip(degrees, train_errors, valid_errors):\n",
    "    print(f'{d:6d} | {tr:9.4f} | {va:9.4f}')\n",
    "\n",
    "plt.plot(degrees, train_errors, marker='o', label='Train MSE')\n",
    "plt.plot(degrees, valid_errors, marker='o', label='Valid MSE')\n",
    "plt.xlabel('Polynomial degree')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Underfitting vs Overfitting demo')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d130d56",
   "metadata": {},
   "source": [
    "**What you should see:**\n",
    "\n",
    "- Very low degree (1) underfits → both train and valid error relatively high.\n",
    "- Intermediate degree (3 or 5) often best on validation.\n",
    "- Very high degree (9) may overfit → train error tiny, validation error worse.\n",
    "\n",
    "This is the core idea behind:\n",
    "- Choosing tree depth / number of leaves\n",
    "- Choosing polynomial degree / neural network size\n",
    "- Using regularization (Ridge, Lasso, weight decay).\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
