{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b767b091",
   "metadata": {},
   "source": [
    "# Tabular Classification Template ‚Äì EDA ‚Üí Imbalance ‚Üí Modeling\n",
    "\n",
    "This notebook is a reusable template for **tabular classification problems** (binary or multiclass).  \n",
    "It mirrors the structure of the regression workflow, but with **classification-specific twists**:\n",
    "\n",
    "- Class balance & target leakage checks\n",
    "- Metrics and evaluation suited to classification (accuracy, ROC-AUC, F1, etc.)\n",
    "- Handling label imbalance (class weights, resampling)\n",
    "- Same modular model factory (trees, linear, neural nets)\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ High-Level Workflow (Classification)\n",
    "\n",
    "1. **Imports & config**\n",
    "2. **Load data**\n",
    "3. **Structure & column typing**\n",
    "4. **EDA**\n",
    "   - target distribution (class balance)\n",
    "   - numeric & categorical feature analysis\n",
    "5. **Skewness & outliers (on numeric predictors)**\n",
    "6. **Feature engineering** (date parts, ratios, bins, etc.)\n",
    "7. **Missingness & imputation**\n",
    "8. **Class imbalance strategy**\n",
    "9. **Model family selection & baseline training**\n",
    "10. **Evaluation & error analysis (confusion matrix, ROC, PR curves)**\n",
    "\n",
    "You can duplicate this notebook for any new **classification** competition and adjust only the config.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89986b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1. Imports & Config (Classification) ==========\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Optional: gradient boosting libs\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "except ImportError:\n",
    "    XGBClassifier = None\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "except ImportError:\n",
    "    LGBMClassifier = None\n",
    "\n",
    "# Optional: Keras/TensorFlow\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "except ImportError:\n",
    "    tf = None\n",
    "    keras = None\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "# ---- Config (edit per dataset) ----\n",
    "DATA_DIR = Path(\"../input\")\n",
    "TRAIN_FILE = \"train.csv\"\n",
    "TEST_FILE = \"test.csv\"      # set to None if no test\n",
    "\n",
    "TARGET_COL = \"target\"       # classification label\n",
    "ID_COL = \"id\"               # optional ID\n",
    "\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8f8688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 2. Load Data & Typing Helpers ==========\n",
    "\n",
    "def load_data(\n",
    "    data_dir: Path = DATA_DIR,\n",
    "    train_file: str = TRAIN_FILE,\n",
    "    test_file: Optional[str] = TEST_FILE,\n",
    "):\n",
    "    train_path = data_dir / train_file\n",
    "    if not train_path.exists():\n",
    "        raise FileNotFoundError(f\"Train file not found: {train_path}\")\n",
    "    train_df = pd.read_csv(train_path)\n",
    "\n",
    "    test_df = None\n",
    "    if test_file is not None:\n",
    "        test_path = data_dir / test_file\n",
    "        if test_path.exists():\n",
    "            test_df = pd.read_csv(test_path)\n",
    "        else:\n",
    "            print(f\"Test file not found: {test_path} (continuing without test_df)\")\n",
    "\n",
    "    print(\"Train shape:\", train_df.shape)\n",
    "    if test_df is not None:\n",
    "        print(\"Test shape :\", test_df.shape)\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def get_numeric_features(df: pd.DataFrame, exclude: Optional[List[str]] = None) -> List[str]:\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if exclude:\n",
    "        num_cols = [c for c in num_cols if c not in exclude]\n",
    "    return num_cols\n",
    "\n",
    "\n",
    "def get_categorical_features(df: pd.DataFrame) -> List[str]:\n",
    "    return df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "\n",
    "def get_boolean_features(df: pd.DataFrame) -> List[str]:\n",
    "    bool_cols = df.select_dtypes(include=[\"bool\"]).columns.tolist()\n",
    "    for col in df.select_dtypes(include=[\"int64\", \"int32\", \"int16\"]).columns:\n",
    "        unique_vals = df[col].dropna().unique()\n",
    "        if len(unique_vals) <= 2 and set(unique_vals).issubset({0, 1}):\n",
    "            bool_cols.append(col)\n",
    "    return list(dict.fromkeys(bool_cols))\n",
    "\n",
    "\n",
    "def summarize_dataframe(df: pd.DataFrame, name: str = \"df\"):\n",
    "    print(f\"===== {name} summary =====\")\n",
    "    print(\"Shape:\", df.shape)\n",
    "    display(df.head())\n",
    "    print(\"\\nDtypes:\")\n",
    "    display(df.dtypes)\n",
    "    print(\"\\nMissing (%):\")\n",
    "    display((df.isna().mean() * 100).sort_values(ascending=False))\n",
    "\n",
    "\n",
    "train_df, test_df = load_data()\n",
    "summarize_dataframe(train_df, \"train_df\")\n",
    "\n",
    "num_cols = get_numeric_features(train_df, exclude=[TARGET_COL, ID_COL] if ID_COL in train_df.columns else [TARGET_COL])\n",
    "cat_cols = get_categorical_features(train_df)\n",
    "bool_cols = get_boolean_features(train_df)\n",
    "\n",
    "print(\"Numeric cols:\", num_cols[:10], \"...\" if len(num_cols) > 10 else \"\")\n",
    "print(\"Categorical cols:\", cat_cols)\n",
    "print(\"Boolean cols:\", bool_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb7aa2f",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Target / Class Distribution\n",
    "\n",
    "Key classification-specific checks:\n",
    "\n",
    "- Is the problem **binary** or **multiclass**?\n",
    "- Are the classes **balanced** or highly skewed?\n",
    "- Is there any suspicious pattern suggesting **label leakage** (e.g. IDs perfectly predicting target)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323223d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution\n",
    "print(\"Target value counts:\")\n",
    "display(train_df[TARGET_COL].value_counts(dropna=False))\n",
    "print(\"\\nTarget value proportions:\")\n",
    "display(train_df[TARGET_COL].value_counts(normalize=True))\n",
    "\n",
    "sns.countplot(x=TARGET_COL, data=train_df)\n",
    "plt.title(\"Target class distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a753815",
   "metadata": {},
   "source": [
    "### 4Ô∏è‚É£ Simple Preprocessing & Baseline Classification Models\n",
    "\n",
    "For classification, we can start with:\n",
    "\n",
    "- Simple imputation (median for numerics, most frequent for categoricals)\n",
    "- One-hot encoding for categoricals\n",
    "- No scaling required for tree models; scaling useful for logistic regression / neural nets.\n",
    "\n",
    "We'll define a small **model factory** and a single function to train/evaluate a few baselines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0876df07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple imputers & encoders\n",
    "numeric_imputer = SimpleImputer(strategy=\"median\")\n",
    "categorical_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "def build_preprocessor(df: pd.DataFrame):\n",
    "    num_cols = get_numeric_features(df, exclude=[TARGET_COL, ID_COL] if ID_COL in df.columns else [TARGET_COL])\n",
    "    cat_cols = get_categorical_features(df)\n",
    "\n",
    "    numeric_pipeline = Pipeline(steps=[\n",
    "        (\"imputer\", numeric_imputer),\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),  # with_mean=False for sparse safety\n",
    "    ])\n",
    "\n",
    "    categorical_pipeline = Pipeline(steps=[\n",
    "        (\"imputer\", categorical_imputer),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ])\n",
    "\n",
    "    transformers = []\n",
    "    if num_cols:\n",
    "        transformers.append((\"num\", numeric_pipeline, num_cols))\n",
    "    if cat_cols:\n",
    "        transformers.append((\"cat\", categorical_pipeline, cat_cols))\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=transformers,\n",
    "        remainder=\"drop\",\n",
    "    )\n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "def build_classifier(model_type: str):\n",
    "    if model_type == \"logreg\":\n",
    "        return LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            n_jobs=-1,\n",
    "            class_weight=\"balanced\",  # helps with imbalance\n",
    "        )\n",
    "    if model_type == \"rf\":\n",
    "        return RandomForestClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=None,\n",
    "            n_jobs=-1,\n",
    "            random_state=RANDOM_STATE,\n",
    "            class_weight=\"balanced_subsample\",\n",
    "        )\n",
    "    if model_type == \"xgb\":\n",
    "        if XGBClassifier is None:\n",
    "            raise ImportError(\"xgboost not installed\")\n",
    "        return XGBClassifier(\n",
    "            n_estimators=500,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=6,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            tree_method=\"hist\",\n",
    "            random_state=RANDOM_STATE,\n",
    "        )\n",
    "    if model_type == \"lgbm\":\n",
    "        if LGBMClassifier is None:\n",
    "            raise ImportError(\"lightgbm not installed\")\n",
    "        return LGBMClassifier(\n",
    "            n_estimators=500,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=-1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=RANDOM_STATE,\n",
    "        )\n",
    "    # Keras MLP option could be added later similarly\n",
    "    raise ValueError(f\"Unknown model_type: {model_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f10b3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifiers(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str = TARGET_COL,\n",
    "    id_col: Optional[str] = ID_COL,\n",
    "    model_types: Optional[List[str]] = None,\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = RANDOM_STATE,\n",
    "):\n",
    "    if model_types is None:\n",
    "        model_types = [\"logreg\", \"rf\", \"xgb\", \"lgbm\"]\n",
    "\n",
    "    df = df.copy()\n",
    "    drop_cols = [target_col]\n",
    "    if id_col is not None and id_col in df.columns:\n",
    "        drop_cols.append(id_col)\n",
    "\n",
    "    X = df.drop(columns=drop_cols)\n",
    "    y = df[target_col]\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    preprocessor = build_preprocessor(df)\n",
    "    results = []\n",
    "\n",
    "    for mt in model_types:\n",
    "        print(f\"\\n=== Model: {mt} ===\")\n",
    "        try:\n",
    "            clf = build_classifier(mt)\n",
    "            pipe = Pipeline(\n",
    "                steps=[\n",
    "                    (\"preprocessor\", preprocessor),\n",
    "                    (\"model\", clf),\n",
    "                ]\n",
    "            )\n",
    "            pipe.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = pipe.predict(X_valid)\n",
    "            acc = accuracy_score(y_valid, y_pred)\n",
    "            f1 = f1_score(y_valid, y_pred, average=\"weighted\")\n",
    "\n",
    "            # ROC-AUC for binary only\n",
    "            auc = np.nan\n",
    "            if len(np.unique(y_valid)) == 2 and hasattr(pipe, \"predict_proba\"):\n",
    "                y_proba = pipe.predict_proba(X_valid)[:, 1]\n",
    "                auc = roc_auc_score(y_valid, y_proba)\n",
    "\n",
    "            print(f\"Accuracy: {acc:.4f} | F1 (weighted): {f1:.4f} | ROC-AUC: {auc if not np.isnan(auc) else 'N/A'}\")\n",
    "\n",
    "            cm = confusion_matrix(y_valid, y_pred)\n",
    "            ConfusionMatrixDisplay(cm).plot()\n",
    "            plt.title(f\"Confusion Matrix ‚Äì {mt}\")\n",
    "            plt.show()\n",
    "\n",
    "            results.append({\"model_type\": mt, \"accuracy\": acc, \"f1_weighted\": f1, \"roc_auc\": auc})\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "\n",
    "    results_df = pd.DataFrame(results).sort_values(\"f1_weighted\", ascending=False)\n",
    "    display(results_df)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Run baseline comparison (you can comment out models you don't have installed)\n",
    "classification_results = evaluate_classifiers(train_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
