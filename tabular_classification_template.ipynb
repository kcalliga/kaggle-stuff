{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b767b091",
   "metadata": {},
   "source": [
    "# Tabular Classification Template â€“ EDA â†’ Imbalance â†’ Modeling\n",
    "\n",
    "This notebook is a reusable template for **tabular classification problems** (binary or multiclass).  \n",
    "It mirrors the structure of the regression workflow, but with **classification-specific twists**:\n",
    "\n",
    "- Class balance & target leakage checks\n",
    "- Metrics and evaluation suited to classification (accuracy, ROC-AUC, F1, etc.)\n",
    "- Handling label imbalance (class weights, resampling)\n",
    "- Same modular model factory (trees, linear, neural nets)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” High-Level Workflow (Classification)\n",
    "\n",
    "1. **Imports & config**\n",
    "2. **Load data**\n",
    "3. **Structure & column typing**\n",
    "4. **EDA**\n",
    "   - target distribution (class balance)\n",
    "   - numeric & categorical feature analysis\n",
    "5. **Skewness & outliers (on numeric predictors)**\n",
    "6. **Feature engineering** (date parts, ratios, bins, etc.)\n",
    "7. **Missingness & imputation**\n",
    "8. **Class imbalance strategy**\n",
    "9. **Model family selection & baseline training**\n",
    "10. **Evaluation & error analysis (confusion matrix, ROC, PR curves)**\n",
    "\n",
    "You can duplicate this notebook for any new **classification** competition and adjust only the config.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89986b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1. Imports & Config (Classification) ==========\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Optional: gradient boosting libs\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "except ImportError:\n",
    "    XGBClassifier = None\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "except ImportError:\n",
    "    LGBMClassifier = None\n",
    "\n",
    "# Optional: Keras/TensorFlow\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "except ImportError:\n",
    "    tf = None\n",
    "    keras = None\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "# ---- Config (edit per dataset) ----\n",
    "DATA_DIR = Path(\"../input\")\n",
    "TRAIN_FILE = \"train.csv\"\n",
    "TEST_FILE = \"test.csv\"      # set to None if no test\n",
    "\n",
    "TARGET_COL = \"target\"       # classification label\n",
    "ID_COL = \"id\"               # optional ID\n",
    "\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8f8688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 2. Load Data & Typing Helpers ==========\n",
    "\n",
    "def load_data(\n",
    "    data_dir: Path = DATA_DIR,\n",
    "    train_file: str = TRAIN_FILE,\n",
    "    test_file: Optional[str] = TEST_FILE,\n",
    "):\n",
    "    train_path = data_dir / train_file\n",
    "    if not train_path.exists():\n",
    "        raise FileNotFoundError(f\"Train file not found: {train_path}\")\n",
    "    train_df = pd.read_csv(train_path)\n",
    "\n",
    "    test_df = None\n",
    "    if test_file is not None:\n",
    "        test_path = data_dir / test_file\n",
    "        if test_path.exists():\n",
    "            test_df = pd.read_csv(test_path)\n",
    "        else:\n",
    "            print(f\"Test file not found: {test_path} (continuing without test_df)\")\n",
    "\n",
    "    print(\"Train shape:\", train_df.shape)\n",
    "    if test_df is not None:\n",
    "        print(\"Test shape :\", test_df.shape)\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def get_numeric_features(df: pd.DataFrame, exclude: Optional[List[str]] = None) -> List[str]:\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if exclude:\n",
    "        num_cols = [c for c in num_cols if c not in exclude]\n",
    "    return num_cols\n",
    "\n",
    "\n",
    "def get_categorical_features(df: pd.DataFrame) -> List[str]:\n",
    "    return df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "\n",
    "def get_boolean_features(df: pd.DataFrame) -> List[str]:\n",
    "    bool_cols = df.select_dtypes(include=[\"bool\"]).columns.tolist()\n",
    "    for col in df.select_dtypes(include=[\"int64\", \"int32\", \"int16\"]).columns:\n",
    "        unique_vals = df[col].dropna().unique()\n",
    "        if len(unique_vals) <= 2 and set(unique_vals).issubset({0, 1}):\n",
    "            bool_cols.append(col)\n",
    "    return list(dict.fromkeys(bool_cols))\n",
    "\n",
    "\n",
    "def summarize_dataframe(df: pd.DataFrame, name: str = \"df\"):\n",
    "    print(f\"===== {name} summary =====\")\n",
    "    print(\"Shape:\", df.shape)\n",
    "    display(df.head())\n",
    "    print(\"\\nDtypes:\")\n",
    "    display(df.dtypes)\n",
    "    print(\"\\nMissing (%):\")\n",
    "    display((df.isna().mean() * 100).sort_values(ascending=False))\n",
    "\n",
    "\n",
    "train_df, test_df = load_data()\n",
    "summarize_dataframe(train_df, \"train_df\")\n",
    "\n",
    "num_cols = get_numeric_features(train_df, exclude=[TARGET_COL, ID_COL] if ID_COL in train_df.columns else [TARGET_COL])\n",
    "cat_cols = get_categorical_features(train_df)\n",
    "bool_cols = get_boolean_features(train_df)\n",
    "\n",
    "print(\"Numeric cols:\", num_cols[:10], \"...\" if len(num_cols) > 10 else \"\")\n",
    "print(\"Categorical cols:\", cat_cols)\n",
    "print(\"Boolean cols:\", bool_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb7aa2f",
   "metadata": {},
   "source": [
    "### 3ï¸âƒ£ Target / Class Distribution\n",
    "\n",
    "Key classification-specific checks:\n",
    "\n",
    "- Is the problem **binary** or **multiclass**?\n",
    "- Are the classes **balanced** or highly skewed?\n",
    "- Is there any suspicious pattern suggesting **label leakage** (e.g. IDs perfectly predicting target)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323223d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution\n",
    "print(\"Target value counts:\")\n",
    "display(train_df[TARGET_COL].value_counts(dropna=False))\n",
    "print(\"\\nTarget value proportions:\")\n",
    "display(train_df[TARGET_COL].value_counts(normalize=True))\n",
    "\n",
    "sns.countplot(x=TARGET_COL, data=train_df)\n",
    "plt.title(\"Target class distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a753815",
   "metadata": {},
   "source": [
    "### 4ï¸âƒ£ Simple Preprocessing & Baseline Classification Models\n",
    "\n",
    "For classification, we can start with:\n",
    "\n",
    "- Simple imputation (median for numerics, most frequent for categoricals)\n",
    "- One-hot encoding for categoricals\n",
    "- No scaling required for tree models; scaling useful for logistic regression / neural nets.\n",
    "\n",
    "We'll define a small **model factory** and a single function to train/evaluate a few baselines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0876df07",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m numeric_imputer = SimpleImputer(strategy=\u001b[33m\"\u001b[39m\u001b[33mmedian\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m categorical_imputer = SimpleImputer(strategy=\u001b[33m\"\u001b[39m\u001b[33mmost_frequent\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild_preprocessor\u001b[39m(df: \u001b[43mpd\u001b[49m.DataFrame, log_cols=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# figure out numeric / categorical\u001b[39;00m\n\u001b[32m     12\u001b[39m     num_cols = get_numeric_features(\n\u001b[32m     13\u001b[39m         df,\n\u001b[32m     14\u001b[39m         exclude=[TARGET_COL, ID_COL] \u001b[38;5;28;01mif\u001b[39;00m ID_COL \u001b[38;5;129;01min\u001b[39;00m df.columns \u001b[38;5;28;01melse\u001b[39;00m [TARGET_COL]\n\u001b[32m     15\u001b[39m     )\n\u001b[32m     16\u001b[39m     cat_cols = get_categorical_features(df)\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# simple imputers (as you had)\n",
    "numeric_imputer = SimpleImputer(strategy=\"median\")\n",
    "categorical_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "def build_preprocessor(df: pd.DataFrame, log_cols=None):\n",
    "    # figure out numeric / categorical\n",
    "    num_cols = get_numeric_features(\n",
    "        df,\n",
    "        exclude=[TARGET_COL, ID_COL] if ID_COL in df.columns else [TARGET_COL]\n",
    "    )\n",
    "    cat_cols = get_categorical_features(df)\n",
    "\n",
    "    # make sure log_cols is a subset of num_cols\n",
    "    log_cols = log_cols or []\n",
    "    log_cols = [c for c in log_cols if c in num_cols]\n",
    "\n",
    "    # numeric columns that are NOT log-transformed\n",
    "    plain_num_cols = [c for c in num_cols if c not in log_cols]\n",
    "\n",
    "    # pipeline for plain numeric features\n",
    "    numeric_pipeline = Pipeline(steps=[\n",
    "        (\"imputer\", numeric_imputer),\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),  # sparse-safe\n",
    "    ])\n",
    "\n",
    "    # pipeline for log-transformed numeric features\n",
    "    log_numeric_pipeline = Pipeline(steps=[\n",
    "        (\"imputer\", numeric_imputer),\n",
    "        (\"log\", FunctionTransformer(np.log1p, feature_names_out=\"one-to-one\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    ])\n",
    "\n",
    "    # categorical pipeline (unchanged)\n",
    "    categorical_pipeline = Pipeline(steps=[\n",
    "        (\"imputer\", categorical_imputer),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ])\n",
    "\n",
    "    transformers = []\n",
    "    if plain_num_cols:\n",
    "        transformers.append((\"num\", numeric_pipeline, plain_num_cols))\n",
    "    if log_cols:\n",
    "        transformers.append((\"num_log\", log_numeric_pipeline, log_cols))\n",
    "    if cat_cols:\n",
    "        transformers.append((\"cat\", categorical_pipeline, cat_cols))\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=transformers,\n",
    "        remainder=\"drop\",\n",
    "    )\n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "def build_classifier(model_type: str):\n",
    "    if model_type == \"logreg\":\n",
    "        return LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            n_jobs=-1,\n",
    "            class_weight=\"balanced\",  # helps with imbalance\n",
    "        )\n",
    "    if model_type == \"rf\":\n",
    "        return RandomForestClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=None,\n",
    "            n_jobs=-1,\n",
    "            random_state=RANDOM_STATE,\n",
    "            class_weight=\"balanced_subsample\",\n",
    "        )\n",
    "    if model_type == \"xgb\":\n",
    "        if XGBClassifier is None:\n",
    "            raise ImportError(\"xgboost not installed\")\n",
    "        return XGBClassifier(\n",
    "            n_estimators=500,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=6,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            tree_method=\"hist\",\n",
    "            random_state=RANDOM_STATE,\n",
    "        )\n",
    "    if model_type == \"lgbm\":\n",
    "        if LGBMClassifier is None:\n",
    "            raise ImportError(\"lightgbm not installed\")\n",
    "        return LGBMClassifier(\n",
    "            n_estimators=500,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=-1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=RANDOM_STATE,\n",
    "        )\n",
    "    # Keras MLP option could be added later similarly\n",
    "    raise ValueError(f\"Unknown model_type: {model_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f10b3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifiers(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str = TARGET_COL,\n",
    "    id_col: Optional[str] = ID_COL,\n",
    "    model_types: Optional[List[str]] = None,\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = RANDOM_STATE,\n",
    "):\n",
    "    if model_types is None:\n",
    "        model_types = [\"logreg\", \"rf\", \"xgb\", \"lgbm\"]\n",
    "\n",
    "    df = df.copy()\n",
    "    drop_cols = [target_col]\n",
    "    if id_col is not None and id_col in df.columns:\n",
    "        drop_cols.append(id_col)\n",
    "\n",
    "    X = df.drop(columns=drop_cols)\n",
    "    y = df[target_col]\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    preprocessor = build_preprocessor(df)\n",
    "    results = []\n",
    "\n",
    "    for mt in model_types:\n",
    "        print(f\"\\n=== Model: {mt} ===\")\n",
    "        try:\n",
    "            clf = build_classifier(mt)\n",
    "            pipe = Pipeline(\n",
    "                steps=[\n",
    "                    (\"preprocessor\", preprocessor),\n",
    "                    (\"model\", clf),\n",
    "                ]\n",
    "            )\n",
    "            pipe.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = pipe.predict(X_valid)\n",
    "            acc = accuracy_score(y_valid, y_pred)\n",
    "            f1 = f1_score(y_valid, y_pred, average=\"weighted\")\n",
    "\n",
    "            # ROC-AUC for binary only\n",
    "            auc = np.nan\n",
    "            if len(np.unique(y_valid)) == 2 and hasattr(pipe, \"predict_proba\"):\n",
    "                y_proba = pipe.predict_proba(X_valid)[:, 1]\n",
    "                auc = roc_auc_score(y_valid, y_proba)\n",
    "\n",
    "            print(f\"Accuracy: {acc:.4f} | F1 (weighted): {f1:.4f} | ROC-AUC: {auc if not np.isnan(auc) else 'N/A'}\")\n",
    "\n",
    "            cm = confusion_matrix(y_valid, y_pred)\n",
    "            ConfusionMatrixDisplay(cm).plot()\n",
    "            plt.title(f\"Confusion Matrix â€“ {mt}\")\n",
    "            plt.show()\n",
    "\n",
    "            results.append({\"model_type\": mt, \"accuracy\": acc, \"f1_weighted\": f1, \"roc_auc\": auc})\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "\n",
    "    results_df = pd.DataFrame(results).sort_values(\"f1_weighted\", ascending=False)\n",
    "    display(results_df)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Run baseline comparison (you can comment out models you don't have installed)\n",
    "classification_results = evaluate_classifiers(train_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
