{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1074a49",
   "metadata": {},
   "source": [
    "# Geospatial Tabular Template ‚Äì Weather Prediction from ZIP Codes\n",
    "\n",
    "This template is for **tabular ML with simple geospatial features**, e.g.:\n",
    "\n",
    "- Predicting daily high temperature for a **US ZIP code**\n",
    "- Using **latitude/longitude**, distances, and regional clusters as features\n",
    "- Training a **standard regression model** (RandomForest, Gradient Boosting, etc.)\n",
    "\n",
    "We stay in the **tabular world**, but add a light layer of geospatial thinking:\n",
    "\n",
    "- ZIP ‚Üí latitude/longitude\n",
    "- Distances to reference points (e.g., weather stations, coastline, city center)\n",
    "- Geo-clusters (KMeans on lat/lon)\n",
    "- Optional interaction with time (date features)\n",
    "\n",
    "This is the ‚Äúbridge‚Äù between pure tabular and full GIS / geospatial ML.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cca937",
   "metadata": {},
   "source": [
    "## üîÅ High-Level Workflow (Template A ‚Äì Tabular Geospatial)\n",
    "\n",
    "1. Imports & config\n",
    "2. Load data (ZIP, date, target, extra covariates)\n",
    "3. ZIP ‚Üí latitude/longitude (via lookup or pre-joined file)\n",
    "4. Geospatial feature engineering\n",
    "   - Distances (to weather stations or city centers)\n",
    "   - Simple geo-clusters on lat/lon\n",
    "5. Time features (month, day-of-year, etc.)\n",
    "6. Train/validation split (random vs time-based)\n",
    "7. Baseline regression models\n",
    "8. Evaluation & feature importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba56ceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1. Imports & Config (Geo Tabular Weather) ==========\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "# If you want to use pgeocode for ZIP -> lat/lon, you can install it via:\n",
    "#   pip install pgeocode\n",
    "try:\n",
    "    import pgeocode\n",
    "    PGEOCODE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PGEOCODE_AVAILABLE = False\n",
    "    print(\"pgeocode not installed; ZIP -> lat/lon via pgeocode will be skipped unless you install it.\")\n",
    "\n",
    "# ---- Config ----\n",
    "DATA_DIR = Path(\"../input\")\n",
    "TRAIN_FILE = \"weather_train.csv\"   # edit to your file name\n",
    "\n",
    "ZIP_COL = \"zip\"\n",
    "DATE_COL = \"date\"\n",
    "TARGET_COL = \"temp_high\"           # numeric regression target\n",
    "\n",
    "ID_COL = \"id\"                      # optional\n",
    "\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176f53f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 2. Load Data & Basic Checks ==========\n",
    "\n",
    "def load_data(data_dir: Path = DATA_DIR, train_file: str = TRAIN_FILE) -> pd.DataFrame:\n",
    "    path = data_dir / train_file\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Train file not found: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    print(\"Data shape:\", df.shape)\n",
    "    display(df.head())\n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_data()\n",
    "\n",
    "# Basic sanity checks\n",
    "if ZIP_COL not in df.columns:\n",
    "    raise ValueError(f\"Expected ZIP column '{ZIP_COL}' not in dataframe\")\n",
    "if TARGET_COL not in df.columns:\n",
    "    raise ValueError(f\"Expected target column '{TARGET_COL}' not in dataframe\")\n",
    "\n",
    "print(\"\\nDtypes:\")\n",
    "display(df.dtypes)\n",
    "\n",
    "print(\"\\nMissing (%):\")\n",
    "display((df.isna().mean() * 100).sort_values(ascending=False))\n",
    "\n",
    "# Simple target distribution\n",
    "sns.histplot(df[TARGET_COL], bins=40)\n",
    "plt.title(\"Target distribution\")\n",
    "plt.xlabel(TARGET_COL)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0e4649",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ ZIP ‚Üí Latitude/Longitude\n",
    "\n",
    "You have two main options:\n",
    "\n",
    "1. **Pre-joined coordinates**: your CSV already has `lat` / `lon` columns.\n",
    "2. **Lookup with pgeocode**: derive lat/lon from ZIP on the fly.\n",
    "\n",
    "For performance and reproducibility, pre-joining coordinates into your dataset\n",
    "(often by merging with a ZIP‚Üílat/lon lookup table) is recommended.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86535b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 3. ZIP -> Lat/Lon (if not already present) ==========\n",
    "\n",
    "if \"lat\" in df.columns and \"lon\" in df.columns:\n",
    "    print(\"Using existing lat/lon columns in dataframe.\")\n",
    "else:\n",
    "    if not PGEOCODE_AVAILABLE:\n",
    "        raise ImportError(\n",
    "            \"lat/lon not in dataframe and pgeocode is not installed. \"\n",
    "            \"Either add lat/lon to your CSV or install pgeocode.\"\n",
    "        )\n",
    "    geo = pgeocode.Nominatim(\"us\")\n",
    "    # pgeocode expects strings\n",
    "    df[ZIP_COL] = df[ZIP_COL].astype(str)\n",
    "    loc = df[ZIP_COL].apply(lambda z: geo.query_postal_code(z))\n",
    "    df[\"lat\"] = loc.apply(lambda r: r.latitude)\n",
    "    df[\"lon\"] = loc.apply(lambda r: r.longitude)\n",
    "\n",
    "print(\"Lat/Lon summary:\")\n",
    "display(df[[\"lat\", \"lon\"]].describe(include=\"all\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489b8959",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Geospatial Feature Engineering (Tabular)\n",
    "\n",
    "We will add:\n",
    "\n",
    "- **Distance to a reference point** (e.g., a ‚Äúcentral‚Äù location or known station)\n",
    "- **Regional geo-cluster** using KMeans on (lat, lon)\n",
    "\n",
    "You can adapt this to:\n",
    "\n",
    "- Distance to nearest coastline / city\n",
    "- Distance to nearest station with real measurements\n",
    "- Clusters based on your specific region of interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5e3f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 4. Geospatial Feature Engineering ==========\n",
    "\n",
    "# Simple haversine distance (in kilometers)\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0  # Earth radius in km\n",
    "    lat1_rad = np.radians(lat1)\n",
    "    lon1_rad = np.radians(lon1)\n",
    "    lat2_rad = np.radians(lat2)\n",
    "    lon2_rad = np.radians(lon2)\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    a = np.sin(dlat / 2) ** 2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2) ** 2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "\n",
    "# Example: distance to a reference point (e.g., Kansas center ~ US centroid)\n",
    "REF_LAT, REF_LON = 39.5, -98.35  # approximate geographic center of contiguous US\n",
    "\n",
    "df[\"dist_ref_km\"] = haversine_km(df[\"lat\"], df[\"lon\"], REF_LAT, REF_LON)\n",
    "\n",
    "# Geo-clusters on lat/lon\n",
    "geo_coords = df[[\"lat\", \"lon\"]].dropna().values\n",
    "\n",
    "N_CLUSTERS = 8\n",
    "kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=RANDOM_STATE)\n",
    "cluster_labels = kmeans.fit_predict(geo_coords)\n",
    "\n",
    "df.loc[df[[\"lat\", \"lon\"]].notna().all(axis=1), \"geo_cluster\"] = cluster_labels\n",
    "df[\"geo_cluster\"] = df[\"geo_cluster\"].astype(\"Int64\")  # nullable integer\n",
    "\n",
    "print(\"Geo features created: dist_ref_km, geo_cluster\")\n",
    "display(df[[\"lat\", \"lon\", \"dist_ref_km\", \"geo_cluster\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6a953a",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Time Features (Optional but Recommended)\n",
    "\n",
    "If you have a date column, you can add:\n",
    "\n",
    "- Year, month, day, day-of-week\n",
    "- Day-of-year (captures seasonality)\n",
    "- Simple cyclical encodings (sin/cos of day-of-year)\n",
    "\n",
    "For **proper forecasting**, your train/valid split should be **time-based**, not random.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776719e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 5. Time Features from DATE_COL (if present) ==========\n",
    "\n",
    "if DATE_COL in df.columns:\n",
    "    df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n",
    "    df[\"year\"] = df[DATE_COL].dt.year\n",
    "    df[\"month\"] = df[DATE_COL].dt.month\n",
    "    df[\"day\"] = df[DATE_COL].dt.day\n",
    "    df[\"dayofyear\"] = df[DATE_COL].dt.dayofyear\n",
    "    df[\"dayofweek\"] = df[DATE_COL].dt.dayofweek\n",
    "\n",
    "    # Simple cyclical encoding for seasonality\n",
    "    df[\"doy_sin\"] = np.sin(2 * np.pi * df[\"dayofyear\"] / 365.25)\n",
    "    df[\"doy_cos\"] = np.cos(2 * np.pi * df[\"dayofyear\"] / 365.25)\n",
    "\n",
    "    print(\"Added time features from DATE_COL.\")\n",
    "else:\n",
    "    print(f\"DATE_COL '{DATE_COL}' not in dataframe; skipping time features.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f26d48c",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Train/Validation Split ‚Äì Random vs Time-Based\n",
    "\n",
    "Two main options:\n",
    "\n",
    "1. **Random split** (standard): fine if you treat this as generic regression.\n",
    "2. **Time-based split**: required if you want honest forecasting evaluation.\n",
    "\n",
    "For a **time-based split**, you can:\n",
    "\n",
    "- Sort by date\n",
    "- Use the earliest part for training, later part for validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7a45e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 6. Train/Validation Split ==========\n",
    "\n",
    "# Choose strat or time-based behavior here:\n",
    "USE_TIME_BASED_SPLIT = DATE_COL in df.columns\n",
    "\n",
    "drop_cols = [TARGET_COL]\n",
    "for c in [ID_COL, DATE_COL]:\n",
    "    if c in df.columns:\n",
    "        drop_cols.append(c)\n",
    "\n",
    "# Example: treat geo_cluster as categorical and one-hot encode later (or let a tree handle it)\n",
    "X = df.drop(columns=drop_cols)\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "if USE_TIME_BASED_SPLIT:\n",
    "    df_sorted = df.sort_values(DATE_COL)\n",
    "    split_idx = int(len(df_sorted) * 0.8)\n",
    "    train_idx = df_sorted.index[:split_idx]\n",
    "    valid_idx = df_sorted.index[split_idx:]\n",
    "    X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n",
    "    y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n",
    "    print(\"Using time-based split (80% earliest dates for train, 20% latest for valid).\")\n",
    "else:\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_STATE\n",
    "    )\n",
    "    print(\"Using random train/validation split.\")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \"Valid shape:\", X_valid.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6bb3c6",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Baseline Regression Model\n",
    "\n",
    "We will start with **RandomForestRegressor** because:\n",
    "\n",
    "- Handles mixed feature types reasonably well (numeric + integer clusters)\n",
    "- Captures nonlinear relationships without heavy tuning\n",
    "\n",
    "You can later plug in:\n",
    "\n",
    "- GradientBoosting, XGBoost, LightGBM, CatBoost\n",
    "- Linear models on standardized features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fc7898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 7. Baseline Model: RandomForestRegressor ==========\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=None,\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_valid)\n",
    "\n",
    "rmse = mean_squared_error(y_valid, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_valid, y_pred)\n",
    "r2 = r2_score(y_valid, y_pred)\n",
    "\n",
    "print(f\"RandomForest - RMSE: {rmse:.3f}, MAE: {mae:.3f}, R2: {r2:.3f}\")\n",
    "\n",
    "plt.scatter(y_valid, y_pred, alpha=0.3)\n",
    "plt.xlabel(\"True\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"RandomForest predictions vs true\")\n",
    "plt.axline((0, 0), slope=1, color=\"red\", linestyle=\"--\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526a60bc",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Feature Importance & Next Steps\n",
    "\n",
    "We can inspect feature importances to understand what the model is using:\n",
    "\n",
    "- Are `lat`, `lon`, `dist_ref_km`, `geo_cluster` important?\n",
    "- Are time features (month, doy_sin/cos) important?\n",
    "\n",
    "Then, iterate:\n",
    "\n",
    "- Add better distance-based features (e.g., distance to nearest station)\n",
    "- Add richer time features (e.g., lagged temps if you have history)\n",
    "- Try gradient boosting models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4134e0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 8. Feature Importance Plot ==========\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "feat_names = X_train.columns\n",
    "\n",
    "fi = pd.DataFrame({\"feature\": feat_names, \"importance\": importances})\n",
    "fi = fi.sort_values(\"importance\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8, 0.3 * len(fi)))\n",
    "sns.barplot(data=fi.head(30), x=\"importance\", y=\"feature\")\n",
    "plt.title(\"RandomForest Feature Importances (top 30)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fi.head(30)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
