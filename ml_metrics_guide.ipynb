{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d069b809",
   "metadata": {},
   "source": [
    "# ML Metrics Guide – Choosing the Right Evaluation\n",
    "\n",
    "This notebook is a **learning + reference** guide for ML evaluation metrics.\n",
    "\n",
    "It focuses on:\n",
    "\n",
    "- **Regression** metrics: MSE, RMSE, MAE, R², MAPE\n",
    "- **Classification** metrics: accuracy, precision, recall, F1, ROC-AUC, PR-AUC\n",
    "- **Confusion matrix** and how to interpret it\n",
    "- How to choose **which metric** based on the problem (imbalanced data, ranking vs calibration, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4089b565",
   "metadata": {},
   "source": [
    "## 1. Regression Metrics\n",
    "\n",
    "When your target is **continuous** (price, points, temperature), you typically use:\n",
    "\n",
    "- **MSE (Mean Squared Error)**:\n",
    "  - Penalizes large errors more (squared).\n",
    "  - Often used in training objectives.\n",
    "- **RMSE (Root Mean Squared Error)**:\n",
    "  - Square root of MSE.\n",
    "  - Same units as the target (more interpretable).\n",
    "- **MAE (Mean Absolute Error)**:\n",
    "  - Measures average absolute error.\n",
    "  - More robust to outliers than MSE.\n",
    "- **R² (Coefficient of Determination)**:\n",
    "  - Proportion of variance explained by the model.\n",
    "  - 1.0 is perfect, 0.0 means no better than predicting the mean.\n",
    "- **MAPE (Mean Absolute Percentage Error)** (optional):\n",
    "  - Expresses error as percentage.\n",
    "  - Sensitive when true values are near zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e05b6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1.1 Regression metrics on a toy example ==========\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "y_true = np.array([10, 12, 15, 20, 18])\n",
    "y_pred_good = np.array([11, 11, 14, 19, 17])\n",
    "y_pred_bad = np.array([5, 25, 5, 30, 10])\n",
    "\n",
    "def regression_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    return mse, rmse, mae, r2, mape\n",
    "\n",
    "for name, y_hat in [('Good', y_pred_good), ('Bad', y_pred_bad)]:\n",
    "    mse, rmse, mae, r2, mape = regression_metrics(y_true, y_hat)\n",
    "    print(f'=== {name} model ===')\n",
    "    print(f'MSE:  {mse:.3f}')\n",
    "    print(f'RMSE: {rmse:.3f}')\n",
    "    print(f'MAE:  {mae:.3f}')\n",
    "    print(f'R2:   {r2:.3f}')\n",
    "    print(f'MAPE: {mape:.2f}%')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f90c54b",
   "metadata": {},
   "source": [
    "### When to use which regression metric?\n",
    "\n",
    "- **MSE / RMSE**:\n",
    "  - When large errors are especially bad.\n",
    "  - Common for many competitions and default in many libraries.\n",
    "- **MAE**:\n",
    "  - When you care about **median-like** behavior and robustness to outliers.\n",
    "- **R²**:\n",
    "  - When you want a normalized sense of how much variance is explained.\n",
    "  - Good for model comparison on the same dataset.\n",
    "- **MAPE**:\n",
    "  - When percentage error is more meaningful, and target values are not near zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee5c91f",
   "metadata": {},
   "source": [
    "## 2. Classification Metrics & Confusion Matrix\n",
    "\n",
    "When your target is a **class label** (binary or multiclass), evaluation revolves around\n",
    "the **confusion matrix**.\n",
    "\n",
    "For binary classification (positive vs negative):\n",
    "\n",
    "- **True Positive (TP)**: predicted positive and actually positive\n",
    "- **True Negative (TN)**: predicted negative and actually negative\n",
    "- **False Positive (FP)**: predicted positive but actually negative\n",
    "- **False Negative (FN)**: predicted negative but actually positive\n",
    "\n",
    "From these, we derive metrics:\n",
    "\n",
    "- **Accuracy** = (TP + TN) / (TP + TN + FP + FN)\n",
    "- **Precision** = TP / (TP + FP) – \"When I predict positive, how often am I right?\"\n",
    "- **Recall (Sensitivity)** = TP / (TP + FN) – \"Of all actual positives, how many did I catch?\"\n",
    "- **F1 Score** = harmonic mean of precision and recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ae995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 2.1 Confusion matrix example ==========\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "y_true = np.array([0, 0, 1, 1, 1, 0, 1, 0, 1, 0])\n",
    "y_pred = np.array([0, 0, 1, 0, 1, 0, 1, 1, 0, 0])\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print('Confusion matrix (rows=true, cols=pred):')\n",
    "print(cm)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.title('Confusion matrix')\n",
    "plt.show()\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "print(f'Accuracy:  {accuracy:.3f}')\n",
    "print(f'Precision: {precision:.3f}')\n",
    "print(f'Recall:    {recall:.3f}')\n",
    "print(f'F1 score:  {f1:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a835fe49",
   "metadata": {},
   "source": [
    "### When to use which classification metric?\n",
    "\n",
    "- **Accuracy**:\n",
    "  - Works when classes are balanced and all errors are equally bad.\n",
    "  - Can be misleading for imbalanced datasets (e.g., 99% negatives).\n",
    "\n",
    "- **Precision & Recall**:\n",
    "  - Precision: \"If I flag something as positive, how often am I correct?\"\n",
    "  - Recall: \"Of all true positives, how many did I catch?\"\n",
    "  - Useful when FP vs FN costs are asymmetric (fraud, disease detection).\n",
    "\n",
    "- **F1 Score**:\n",
    "  - Balanced tradeoff between precision and recall.\n",
    "  - Common for imbalanced classification when you want a single summary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964e9770",
   "metadata": {},
   "source": [
    "## 3. ROC-AUC and PR-AUC\n",
    "\n",
    "Many classifiers output **scores or probabilities**, not just hard labels.\n",
    "By sweeping a threshold across these scores, you get different (TPR, FPR) tradeoffs.\n",
    "\n",
    "- **ROC curve**: plots True Positive Rate (TPR) vs False Positive Rate (FPR).\n",
    "- **ROC-AUC**: area under ROC curve; 0.5 is random, 1.0 is perfect.\n",
    "- **PR curve**: plots Precision vs Recall.\n",
    "- **PR-AUC**: area under PR curve.\n",
    "\n",
    "Guidance:\n",
    "- **ROC-AUC** is good when classes are reasonably balanced.\n",
    "- **PR-AUC** is often more informative for **heavily imbalanced** problems –\n",
    "  it focuses on the positive class performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885daa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 3.1 ROC-AUC and PR-AUC on a toy example ==========\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, auc\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "n = 500\n",
    "y_true = rng.integers(0, 2, size=n)\n",
    "scores = rng.normal(loc=y_true, scale=0.8, size=n)\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, scores)\n",
    "fpr, tpr, roc_thresh = roc_curve(y_true, scores)\n",
    "\n",
    "prec, rec, pr_thresh = precision_recall_curve(y_true, scores)\n",
    "pr_auc = auc(rec, prec)\n",
    "\n",
    "print(f'ROC-AUC: {roc_auc:.3f}')\n",
    "print(f'PR-AUC:  {pr_auc:.3f}')\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(rec, prec)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision–Recall curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9abc15",
   "metadata": {},
   "source": [
    "## 4. Metric Selection Cheat Sheet\n",
    "\n",
    "### 4.1 Regression\n",
    "\n",
    "- Use **RMSE** when:\n",
    "  - Larger errors are especially harmful.\n",
    "  - The competition or stakeholder optimizes squared error.\n",
    "\n",
    "- Use **MAE** when:\n",
    "  - You want a robust metric against outliers.\n",
    "  - You care about \"typical\" absolute error.\n",
    "\n",
    "- Use **R²** when:\n",
    "  - You want to explain how much variance is captured.\n",
    "  - Comparing models on the same dataset.\n",
    "\n",
    "### 4.2 Classification\n",
    "\n",
    "- Use **accuracy** when:\n",
    "  - Classes are roughly balanced.\n",
    "  - All errors have similar cost.\n",
    "\n",
    "- Use **precision / recall / F1** when:\n",
    "  - Data is imbalanced.\n",
    "  - You care about a specific class (e.g., positive class).\n",
    "  - You can explain FP vs FN cost clearly.\n",
    "\n",
    "- Use **ROC-AUC** when:\n",
    "  - You care about ranking quality across thresholds.\n",
    "  - Class balance is not extremely skewed.\n",
    "\n",
    "- Use **PR-AUC** when:\n",
    "  - Positive class is rare and critical (fraud, disease).\n",
    "  - You care about performance at the top of the score distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149dea6f",
   "metadata": {},
   "source": [
    "## 5. Multi-Class & Other Settings (Brief)\n",
    "\n",
    "For **multi-class** problems:\n",
    "\n",
    "- Accuracy is still straightforward.\n",
    "- Precision/recall/F1 can be averaged:\n",
    "  - **Macro**: average of per-class metrics (treats each class equally).\n",
    "  - **Weighted**: weighted by class frequency.\n",
    "- ROC and PR can be extended using one-vs-rest and averaged.\n",
    "\n",
    "For **ranking / recommendation**:\n",
    "\n",
    "- You may use metrics like:\n",
    "  - Top-k accuracy\n",
    "  - Mean Average Precision at k (MAP@k)\n",
    "  - NDCG\n",
    "\n",
    "Key idea: **match the metric to the real-world objective**.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
